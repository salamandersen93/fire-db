{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232d14f3-0a06-4782-91a7-f750e9eb1a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# notebook setup...\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "me = w.current_user.me()\n",
    "display('current user:', me.display_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64151c2-66a6-4f4f-9174-3d228cec2530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install biopython\n",
    "%pip install dateparser\n",
    "\n",
    "# imports\n",
    "from Bio import Entrez, Medline\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, to_date, when, length, udf, regexp_replace\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def fallback_dateparser(date_str):\n",
    "    if not date_str:\n",
    "        return None\n",
    "    parsed = dateparser.parse(date_str)\n",
    "    return parsed.strftime(\"%Y-%m-%d\") if parsed else None\n",
    "\n",
    "def get_pubmed_date_range_past_months():\n",
    "    today = datetime.today()\n",
    "    three_months_ago = today - relativedelta(months=1)\n",
    "    \n",
    "    # Format dates as YYYY/MM/DD (PubMed compatible)\n",
    "    today_str = today.strftime(\"%Y/%m/%d\")\n",
    "    three_months_ago_str = three_months_ago.strftime(\"%Y/%m/%d\")\n",
    "    \n",
    "    return three_months_ago_str, today_str\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "Entrez.email = \"mikeandersen622@gmail.com\"\n",
    "\n",
    "# global udfs \n",
    "parse_pubmed_date_udf = udf(fallback_dateparser, StringType())\n",
    "\n",
    "# pubmed search conditions\n",
    "# pulls documents tagged with MeSH terms for physical conditioning\n",
    "# TODO: expand to relevant sets of mesh terms, including cardio\n",
    "start_date, end_date = get_pubmed_date_range_past_3_months()\n",
    "print(f\"Search range: {start_date} to {end_date}\")\n",
    "\n",
    "search_term = f'\"Physical Conditioning, Human\"[MeSH Terms] AND (\"{start_date}\"[Date - Publication] : \"{end_date}\"[Date - Publication])'\n",
    "\n",
    "# batch in sets of 100 (temporary max of 1000)\n",
    "batch_size = 100\n",
    "target_count = 200\n",
    "pmid_list = []\n",
    "\n",
    "# get total counts of pubs\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=0)\n",
    "total_count = int(Entrez.read(handle)[\"Count\"])\n",
    "print(f\"Total available: {total_count}\")\n",
    "displayHTML(f\"<b>total available {total_count}</b>\")\n",
    "\n",
    "# fetch results in batches\n",
    "for start in range(0, min(target_count, total_count), batch_size):\n",
    "    handle = Entrez.esearch(\n",
    "        db=\"pubmed\",\n",
    "        term=search_term,\n",
    "        retmax=batch_size,\n",
    "        retstart=start)\n",
    "    batch_result = Entrez.read(handle)\n",
    "    pmid_list.extend(batch_result[\"IdList\"])\n",
    "\n",
    "    displayHTML(f\"<b>Fetched {len(pmid_list)} PMIDs so far...</b>\")\n",
    "\n",
    "# fetch details in batches...\n",
    "all_records = []\n",
    "for i in range(0, len(pmid_list), batch_size):\n",
    "    batch_pmids = pmid_list[i:i+batch_size]\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=batch_pmids, rettype=\"medline\", retmode=\"text\")\n",
    "    records = list(Medline.parse(handle))\n",
    "\n",
    "    for rec in records:\n",
    "        all_records.append({\n",
    "            \"pmid\": rec.get(\"PMID\"),\n",
    "            \"title\": rec.get(\"TI\"),\n",
    "            \"abstract\": rec.get(\"AB\"),\n",
    "            \"authors\": rec.get(\"AU\"),\n",
    "            \"journal\": rec.get(\"JT\"),\n",
    "            \"date\": rec.get(\"DP\")\n",
    "        })\n",
    "\n",
    "# convert to a pyspark df then store\n",
    "pdf = pd.DataFrame(all_records)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pdf)\n",
    "display(df.show(10, truncate=False))\n",
    "\n",
    "normalized_df = (\n",
    "    df.withColumn(\"abstract\", regexp_replace(col(\"abstract\"), \"\\n\", \" \"))\n",
    "      .withColumn(\"title\", regexp_replace(trim(col(\"title\")), \"\\n\", \"\"))\n",
    "      .withColumn(\"pub_date\", parse_pubmed_date_udf(col(\"date\")))\n",
    "      .drop(\"authors\")\n",
    "      .drop(\"date\"))\n",
    "\n",
    "normalized_df = normalized_df.limit(3)\n",
    "display(normalized_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pubmed_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
