{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04ffa21-dccf-4cea-ae1c-f34fa5c13e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from langdetect import detect\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import pandas as pd\n",
    "import ast\n",
    "from json_repair import repair_json\n",
    "\n",
    "def get_exercises(api_url):\n",
    "    '''Retrieves complete list of exercises from the wger public API'''\n",
    "    all_exercises = []\n",
    "    current_url = api_url\n",
    "\n",
    "    try:\n",
    "        while current_url:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            all_exercises.extend(data['results'])\n",
    "            current_url = data.get('next')\n",
    "        return all_exercises\n",
    "    except Exception as e:\n",
    "        print('error fetching exercise data from wger API:', e)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def is_english_content(description):\n",
    "    \"\"\"check if description is actually in English\"\"\"\n",
    "    try:\n",
    "        # check description language, sampling first 100 chars \n",
    "        if description and len(description.strip()) > 0:\n",
    "            desc_sample = description[:100]\n",
    "            desc_lang = detect(desc_sample)\n",
    "            if desc_lang != 'en':\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # if detection fails, assume not english\n",
    "        return False\n",
    "    \n",
    "def normalize_record(record):\n",
    "    \"\"\"Normalize a record dictionary by extracting and formatting specific fields.\"\"\"\n",
    "    # extract name and description from translations with language == 2 (english)\n",
    "    translations = record.get('translations', [])\n",
    "    name = None\n",
    "    description = None\n",
    "    \n",
    "    for translation in translations:\n",
    "        if translation.get(\"language\") == 2:\n",
    "            candidate_name = translation.get(\"name\")\n",
    "            candidate_description = translation.get(\"description\")\n",
    "            \n",
    "            # verify the content is english\n",
    "            if is_english_content(candidate_description):\n",
    "                name = candidate_name\n",
    "                description = candidate_description\n",
    "                break\n",
    "    \n",
    "    # return None if no English content found\n",
    "    # this is a data quality fix;\n",
    "    # there is a known issue in wger API that results in non-english records despite language = 2 filtering\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    # data cleaning step: remove HTML tags from name and description\n",
    "    if name:\n",
    "        name = re.sub(r'<[^>]*>', '', name).strip()\n",
    "    if description:\n",
    "        description = re.sub(r'<[^>]*>', '', description).strip()\n",
    "    \n",
    "    # parse datetime fields with proper error handling\n",
    "    created = None\n",
    "    if record.get(\"created\"):\n",
    "        created = datetime.fromisoformat(record[\"created\"].replace('Z', '+00:00'))\n",
    "    \n",
    "    last_update = None\n",
    "    if record.get(\"last_update\"):\n",
    "        last_update = datetime.fromisoformat(\n",
    "            record[\"last_update\"].replace('Z', '+00:00'))\n",
    "    \n",
    "    # generate normalized dict structure\n",
    "    normalized = {\n",
    "        \"id\": record.get(\"id\"),\n",
    "        \"uuid\": record.get(\"uuid\"),\n",
    "        \"name\": name.upper(),\n",
    "        \"description\": description,\n",
    "        \"created\": created,\n",
    "        \"last_update\": last_update,\n",
    "        \"category\": record.get(\"category\")[\"name\"],\n",
    "        \"muscles\": [muscle[\"name\"] for muscle in record.get(\"muscles\", [])],\n",
    "        \"muscles_secondary\": [muscle[\"name\"] for muscle in record.get(\"muscles_secondary\", [])],\n",
    "        \"equipment\": [equip[\"name\"] for equip in record.get(\"equipment\", [])],\n",
    "        \"variations\": record.get(\"variations\") or [],\n",
    "        \"license_author\": record.get(\"license_author\")}\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def query_databricks_foundation_model(prompt, model_name=\"databricks-llama-4-maverick\", max_tokens=512, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Query a Databricks foundation model through the serving endpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    # construct the API endpoint URL\n",
    "    api_url = f\"https://{DATABRICKS_INSTANCE}/serving-endpoints/{model_name}/invocations\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # chat payload\n",
    "    payload_messages = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # prompt payload\n",
    "    payload_prompt = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # input payload\n",
    "    payload_input = {\n",
    "        \"input\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # attempt each format above\n",
    "    for payload_name, payload in [(\"messages\", payload_messages), (\"prompt\", payload_prompt), (\"input\", payload_input)]:\n",
    "        try:\n",
    "            print(f\"Trying payload format: {payload_name}\")\n",
    "            response = requests.post(api_url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Success with {payload_name} format!\")\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed with {payload_name} format: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"all payload formats failed\")\n",
    "    return None\n",
    "\n",
    "# Alternative approach using the newer Databricks SDK\n",
    "def query_with_databricks_sdk(prompt, model_name=\"databricks-llama-4-maverick\", max_tokens=5000, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Alternative approach using Databricks SDK (if available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        \n",
    "        w = WorkspaceClient()\n",
    "        \n",
    "        # Format the request properly for the SDK\n",
    "        request_data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        response = w.serving_endpoints.query(\n",
    "            name=model_name,\n",
    "            **request_data\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except ImportError:\n",
    "        print(\"Databricks SDK not available, use the API approach instead\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error with SDK approach: {e}\")\n",
    "        return None\n",
    "\n",
    "def format_prompt(instruction, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Format prompt similar to your original template\n",
    "    Based on documentation here:\n",
    "    https://github.com/databricks/databricks-ml-examples/blob/master/llm-models/mistral/mistral-7b/01_load_inference.py\n",
    "    \"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"You are a JSON-only exercise standardization assistant. \n",
    "        ALWAYS respond with valid JSON in this exact format:\n",
    "        {\"groups\":[{\"standardized_name\":\"NAME\",\"variations\":[\"var1\",\"var2\"]}]}\n",
    "        Never include explanations, markdown, misspellings, or extra text.\"\"\"\n",
    "    \n",
    "    formatted_prompt = f\"\"\"<s>[INST]<<SYS>>{system_prompt}<</SYS>>{instruction}[/INST]\"\"\"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "# Example usage\n",
    "def gen_text_databricks(prompts, use_template=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate text using Databricks foundation model\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        if use_template:\n",
    "            formatted_prompt = format_prompt(prompt)\n",
    "        else:\n",
    "            formatted_prompt = prompt\n",
    "            \n",
    "        # First try the API approach\n",
    "        response = query_databricks_foundation_model(\n",
    "            formatted_prompt, \n",
    "            max_tokens=kwargs.get('max_new_tokens', 512),\n",
    "            temperature=kwargs.get('temperature', 0.5)\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            # Handle different response formats\n",
    "            if 'choices' in response and response['choices']:\n",
    "                # OpenAI-style response\n",
    "                content = response['choices'][0].get('message', {}).get('content', '') or response['choices'][0].get('text', '')\n",
    "                results.append(content)\n",
    "            elif 'predictions' in response:\n",
    "                # MLflow-style response\n",
    "                results.append(response['predictions'][0].get('generated_text', ''))\n",
    "            elif 'candidates' in response:\n",
    "                # Gemini-style response\n",
    "                results.append(response['candidates'][0].get('content', {}).get('parts', [{}])[0].get('text', ''))\n",
    "            else:\n",
    "                # Try to extract any text from the response\n",
    "                results.append(str(response))\n",
    "        else:\n",
    "            # Try SDK approach as fallback\n",
    "            sdk_response = query_with_databricks_sdk(\n",
    "                formatted_prompt,\n",
    "                max_tokens=kwargs.get('max_new_tokens', 512),\n",
    "                temperature=kwargs.get('temperature', 0.5)\n",
    "            )\n",
    "            if sdk_response:\n",
    "                results.append(str(sdk_response))\n",
    "            else:\n",
    "                results.append(\"Error: Could not get response from model\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "DATABRICKS_INSTANCE = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "# documentation link: https://exercise.hellogym.io/nl/software/api\n",
    "# the exerciseinfo contains denormalized data with nested\n",
    "wger_api_url = \"https://wger.de/api/v2/exerciseinfo/?status=2&language=2\"\n",
    "exercises = get_exercises(wger_api_url)\n",
    "\n",
    "ex_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"uuid\", StringType()), \n",
    "    StructField(\"name\", StringType()), \n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"created\", TimestampType()),\n",
    "    StructField(\"last_update\", TimestampType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"muscles\",  StringType()),\n",
    "    StructField(\"muscles_secondary\",  StringType()),\n",
    "    StructField(\"equipment\",  StringType()),\n",
    "    StructField(\"variations\", StringType()),\n",
    "    StructField(\"license_author\", StringType())])\n",
    "    \n",
    "# filter out None values for data quality\n",
    "exercise_abbv = [normalize_record(record) for record in exercises]\n",
    "exercise_abbv = [record for record in exercise_abbv if record is not None]  # Remove non-English records\n",
    "\n",
    "try:\n",
    "    exercise_df = spark.createDataFrame(exercise_abbv, schema=ex_schema)\n",
    "    exercise_df = exercise_df.drop(\"uuid\", \"created\", \"last_update\", \"license_author\")\n",
    "    display(exercise_df.sort(\"name\"))\n",
    "except Exception as e:\n",
    "    print(\"error creating dataframe:\", e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867f74dc-c57e-44f7-b5bc-a90aa1c199e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def validate_json_structure(result):\n",
    "    \"\"\"Validate that the JSON has the required structure\"\"\"\n",
    "\n",
    "    if not isinstance(result, dict):\n",
    "        raise ValueError(\"Result is not a dictionary\")\n",
    "    \n",
    "    if 'groups' not in result:\n",
    "        raise ValueError(\"Missing 'groups' key\")\n",
    "    \n",
    "    if not isinstance(result['groups'], list):\n",
    "        raise ValueError(\"'groups' is not a list\")\n",
    "    \n",
    "    for i, group in enumerate(result['groups']):\n",
    "        if not isinstance(group, dict):\n",
    "            raise ValueError(f\"Group {i} is not a dictionary\")\n",
    "        \n",
    "        # Check for standardized name with multiple possible keys\n",
    "        if not any(key in group for key in ['standardized_name', 'standard_name']):\n",
    "            raise ValueError(f\"Group {i} missing standardized name key\")\n",
    "        \n",
    "        if 'variations' not in group:\n",
    "            raise ValueError(f\"Group {i} missing 'variations' key\")\n",
    "        \n",
    "        if not isinstance(group['variations'], list):\n",
    "            raise ValueError(f\"Group {i} 'variations' is not a list\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# dataset augmentation module\n",
    "class AdvancedExerciseStandardizer:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with a sentence transformer model optimized for semantic similarity\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # equipment terms that should NOT be ignored as they define different exercises\n",
    "        # for example, resistance band curl is very different than ez bar curl\n",
    "        # thus, they should never be clustered/harmonized into the same representation\n",
    "        self.critical_equipment = {\n",
    "            'BARBELL', 'DUMBBELL', 'DUMBBELLS', 'EZ BAR', 'EZ-BAR', 'EZBAR',\n",
    "            'CABLE', 'MACHINE', 'KETTLEBELL', 'RESISTANCE BAND', 'TRX',\n",
    "            'SMITH MACHINE', 'PREACHER', 'HAMMER', 'ROPE', 'T-BAR'}\n",
    "        \n",
    "        # equipment terms that can be normalized/ignored\n",
    "        # for example, curl on machine or curl using machine\n",
    "        self.non_critical_equipment = {'ON MACHINE', 'USING MACHINE', 'WITH MACHINE'}\n",
    "        \n",
    "        # movement patterns that define exercise families\n",
    "        # the list is not exhaustive, but helps direct the LLM\n",
    "        self.movement_patterns = {\n",
    "            'CURL', 'PRESS', 'ROW', 'RAISE', 'EXTENSION', 'SQUAT', 'DEADLIFT',\n",
    "            'PULL', 'PUSH', 'FLY', 'DIP', 'LUNGE', 'PLANK', 'CRUNCH'}\n",
    "\n",
    "    def extract_exercise_features(self, exercise_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract key features that distinguish exercises\n",
    "        \"\"\"\n",
    "        name_upper = exercise_name.upper()\n",
    "        \n",
    "        # extract equipment\n",
    "        equipment = []\n",
    "        for equip in self.critical_equipment:\n",
    "            if equip in name_upper:\n",
    "                equipment.append(equip)\n",
    "        \n",
    "        # extract movement pattern\n",
    "        movement = None\n",
    "        for pattern in self.movement_patterns:\n",
    "            if pattern in name_upper:\n",
    "                movement = pattern\n",
    "                break\n",
    "        \n",
    "        # extract body part/muscle groups\n",
    "        body_parts = []\n",
    "        body_part_terms = ['BICEP', 'TRICEP', 'CHEST', 'BACK', 'SHOULDER', 'LEG', 'CALF', 'ABS']\n",
    "        for part in body_part_terms:\n",
    "            if part in name_upper:\n",
    "                body_parts.append(part)\n",
    "        \n",
    "        # extract modifiers (grip, stance, etc.)\n",
    "        modifiers = []\n",
    "        modifier_terms = ['NARROW', 'WIDE', 'CLOSE', 'HAMMER', 'REVERSE', 'OVERHEAD']\n",
    "        for mod in modifier_terms:\n",
    "            if mod in name_upper:\n",
    "                modifiers.append(mod)\n",
    "        \n",
    "        return {\n",
    "            'equipment': equipment,\n",
    "            'movement': movement,\n",
    "            'body_parts': body_parts,\n",
    "            'modifiers': modifiers,\n",
    "            'raw_name': exercise_name\n",
    "        }\n",
    "\n",
    "    def create_feature_aware_embeddings(self, exercise_names: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embeddings that emphasize critical distinguishing features\n",
    "        \"\"\"\n",
    "        enhanced_texts = []\n",
    "        \n",
    "        for name in exercise_names:\n",
    "            features = self.extract_exercise_features(name)\n",
    "            \n",
    "            # create enhanced text that emphasizes critical features\n",
    "            enhanced_parts = [name]\n",
    "            # emphasize equipment (doubled weight for critical equipment)\n",
    "            for equip in features['equipment']:\n",
    "                enhanced_parts.extend([equip, equip])\n",
    "            # add movement pattern\n",
    "            if features['movement']:\n",
    "                enhanced_parts.append(features['movement'])\n",
    "            # modifiers...\n",
    "            enhanced_parts.extend(features['modifiers'])\n",
    "            enhanced_text = ' '.join(enhanced_parts)\n",
    "            enhanced_texts.append(enhanced_text)\n",
    "        \n",
    "        return self.model.encode(enhanced_texts)\n",
    "\n",
    "    def are_exercises_equivalent(self, name1: str, name2: str, threshold: float = 0.85) -> bool:\n",
    "        \"\"\"\n",
    "        Use LLM to validate if two exercises should be considered equivalent\n",
    "        \"\"\"\n",
    "        # Quick feature check first\n",
    "        features1 = self.extract_exercise_features(name1)\n",
    "        features2 = self.extract_exercise_features(name2)\n",
    "        \n",
    "        # if equipment differs, they're likely different exercises\n",
    "        if set(features1['equipment']) != set(features2['equipment']):\n",
    "            # exception: if one has no equipment specified, might be same\n",
    "            if features1['equipment'] and features2['equipment']:\n",
    "                return False\n",
    "        \n",
    "        # if movement patterns differ significantly, likely different\n",
    "        if features1['movement'] != features2['movement'] and \\\n",
    "           features1['movement'] and features2['movement']:\n",
    "            return False\n",
    "        \n",
    "        # semantic similarity as final check\n",
    "        embeddings = self.model.encode([name1, name2])\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        \n",
    "        return similarity >= threshold\n",
    "\n",
    "    def llm_validate_cluster(self, exercise_names: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        use LLM to validate and standardize a cluster of exercise names\n",
    "        \"\"\"\n",
    "        if len(exercise_names) == 1:\n",
    "            return {exercise_names[0]: self.clean_exercise_name(exercise_names[0])}\n",
    "        \n",
    "        # custom prompt for LLM validation\n",
    "        prompt = f\"\"\"\n",
    "        Analyze these exercise names and determine if they represent the same exercise or different exercises.\n",
    "        If they are the same exercise, provide one standardized name.\n",
    "        If they are different exercises, group them appropriately and provide standardized names for each group.\n",
    "\n",
    "        IMPORTANT RULES:\n",
    "        1. Different equipment (Barbell vs Dumbbell vs EZ Bar) = DIFFERENT exercises\n",
    "        2. Different grips (Narrow vs Wide) = DIFFERENT exercises  \n",
    "        3. Different positions (Incline vs Decline vs Flat) = DIFFERENT exercises\n",
    "        4. Only spelling/formatting differences = SAME exercise\n",
    "        5. Left and right variations = SAME exercise\n",
    "\n",
    "        Exercise names: {exercise_names}\n",
    "\n",
    "        CRITICAL: Return ONLY valid JSON. No explanations, no markdown, no extra text.\n",
    "        Start your response with {{ and end with }}.\n",
    "\n",
    "        Format:\n",
    "        {{\"groups\":[{{\"standardized_name\":\"EXERCISE NAME\",\"variations\":[\"var1\",\"var2\"]}}]}}\n",
    "\n",
    "        Example:\n",
    "        {{\"groups\":[{{\"standardized_name\":\"BARBELL BICEP CURL\",\"variations\":[\"Barbell Biceps Curl\",\"Bar Bell Bicep Curls\"]}},{{\"standardized_name\":\"EZ BAR BICEP CURL\",\"variations\":[\"EZ Bar Biceps Curl\",\"Ez-Bar Bicep Curl\"]}}]}}\n",
    "        \"\"\"\n",
    "                \n",
    "        prompt = prompt.replace(\"'\", \"''\")\n",
    "        \n",
    "        try:\n",
    "            # leveraging Databricks foundation model (free tier...)\n",
    "            response = str(gen_text_databricks([prompt], temperature=0.1, max_new_tokens=5000, use_template=True))\n",
    "            response = response.replace('```json', \"\").replace(\"```\", \"\").strip()\n",
    "            bad_response = True\n",
    "            while bad_response:\n",
    "                try:\n",
    "                    repaired = repair_json(response)\n",
    "                    result = ast.literal_eval(json.loads(repaired)[0])\n",
    "                    \n",
    "                    # Validate structure\n",
    "                    validate_json_structure(result)\n",
    "                    \n",
    "                    bad_response = False\n",
    "                    print('Success:')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print('failure:', response)\n",
    "                    bad_response = True\n",
    "\n",
    "            # then create mapping from variations to standardized names\n",
    "            mapping = {}\n",
    "            for group in result['groups']:\n",
    "                try:\n",
    "                    standardized = group['standardized_name']\n",
    "                except: # plan for the LLM to hallucinate standardized name \n",
    "                    standardized = group['standard_name']\n",
    "                for variation in group['variations']:\n",
    "                    mapping[variation] = standardized\n",
    "            \n",
    "            return mapping\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLM validation failed: {e}\")\n",
    "            raise\n",
    "            # fallback: treat as one group with cleaned name\n",
    "            #cleaned_name = self.clean_exercise_name(exercise_names[0])\n",
    "            #return {name: cleaned_name for name in exercise_names}\n",
    "\n",
    "    def clean_exercise_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean exercise name while preserving critical distinctions\n",
    "        \"\"\"\n",
    "        # remove parenthetical information \n",
    "        name = re.sub(r'\\s*\\([^)]*\\)', '', name)\n",
    "        \n",
    "        # standardize equipment terminology\n",
    "        equipment_replacements = {\n",
    "            r'DUMBBELLS?': 'DUMBBELL',\n",
    "            r'BARBELLS?': 'BARBELL', \n",
    "            r'EZ[-\\s]?BAR': 'EZ BAR',\n",
    "            r'T[-\\s]?BAR': 'T-BAR'}\n",
    "        \n",
    "        for pattern, replacement in equipment_replacements.items():\n",
    "            name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
    "        \n",
    "        # standardize exercise terms\n",
    "        exercise_replacements = {\n",
    "            r'BICEPS?': 'BICEP',\n",
    "            r'TRICEPS?': 'TRICEP',\n",
    "            r'PULL[-\\s]?UPS?': 'PULL UP',\n",
    "            r'PUSH[-\\s]?UPS?': 'PUSH UP',\n",
    "            r'SIT[-\\s]?UPS?': 'SIT UP'}\n",
    "        \n",
    "        for pattern, replacement in exercise_replacements.items():\n",
    "            name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
    "        \n",
    "        # whitespace/formatting adjustment\n",
    "        name = re.sub(r'[-_]+', ' ', name)\n",
    "        name = ' '.join(name.split())\n",
    "        \n",
    "        return name.strip().upper()\n",
    "\n",
    "    def smart_clustering(self, exercise_names: List[str], eps: float = 0.25, min_samples: int = 2) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        perform intelligent clustering with LLM validation\n",
    "        \"\"\"\n",
    "        print(f\"starting smart clustering for {len(exercise_names)} exercises...\")\n",
    "        \n",
    "        # create feature-aware embeddings\n",
    "        embeddings = self.create_feature_aware_embeddings(exercise_names)\n",
    "        # initial clustering\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "        clusters = clustering.fit_predict(embeddings)\n",
    "        \n",
    "        print(f\"Initial clustering found {len(set(clusters))} clusters\")\n",
    "        \n",
    "        final_mapping = {}\n",
    "        \n",
    "        # process each cluster individually\n",
    "        for cluster_id in set(clusters):\n",
    "            if cluster_id == -1:  # noise points (unique exercises)\n",
    "                continue\n",
    "            \n",
    "            # get exercises in this cluster\n",
    "            cluster_exercises = [exercise_names[i] for i in range(len(exercise_names)) \n",
    "                               if clusters[i] == cluster_id]\n",
    "            \n",
    "            print(f\"processing cluster {cluster_id} with {len(cluster_exercises)} exercises\")\n",
    "            \n",
    "            # validate cluster with LLM\n",
    "            cluster_mapping = self.llm_validate_cluster(cluster_exercises)\n",
    "            final_mapping.update(cluster_mapping)\n",
    "        \n",
    "        # handle noise points (unique exercises)\n",
    "        for i, cluster_id in enumerate(clusters):\n",
    "            if cluster_id == -1:\n",
    "                exercise = exercise_names[i]\n",
    "                final_mapping[exercise] = self.clean_exercise_name(exercise)\n",
    "        \n",
    "        return final_mapping\n",
    "\n",
    "    def standardize_exercise_dataset(self, exercise_names: List[str]) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Complete standardization pipeline\n",
    "        \"\"\"\n",
    "        print(\"starting exercise standardization pipeline...\")\n",
    "        \n",
    "        # remove obvious invalid exercises\n",
    "        valid_exercises = [name for name in exercise_names \n",
    "                          if not any(invalid in name.upper() \n",
    "                                   for invalid in ['REST', 'JOGGING', 'WALKING', 'CYCLING'])]\n",
    "        \n",
    "        print(f\"filtered out {len(exercise_names) - len(valid_exercises)} invalid exercises\")\n",
    "        \n",
    "        # perform smart clustering\n",
    "        mapping = self.smart_clustering(valid_exercises)\n",
    "        # generate summary to print...\n",
    "        standardized_names = list(set(mapping.values()))\n",
    "        \n",
    "        # group by standardized name for review\n",
    "        groups = {}\n",
    "        for original, standardized in mapping.items():\n",
    "            if standardized not in groups:\n",
    "                groups[standardized] = []\n",
    "            groups[standardized].append(original)\n",
    "        \n",
    "        # Create DataFrame for review\n",
    "        review_data = []\n",
    "        for standardized, originals in groups.items():\n",
    "            review_data.append({\n",
    "                'standardized_name': standardized,\n",
    "                'original_count': len(originals),\n",
    "                'original_names': ' | '.join(originals)})\n",
    "        \n",
    "        review_df = pd.DataFrame(review_data).sort_values('original_count', ascending=False)\n",
    "        \n",
    "        print(f\"Standardization complete:\")\n",
    "        print(f\"  Original exercises: {len(exercise_names)}\")\n",
    "        print(f\"  Valid exercises: {len(valid_exercises)}\")\n",
    "        print(f\"  Standardized exercises: {len(standardized_names)}\")\n",
    "        print(f\"  Reduction: {((len(valid_exercises) - len(standardized_names)) / len(valid_exercises) * 100):.1f}%\")\n",
    "        \n",
    "        return mapping, review_df\n",
    "\n",
    "# implementation\n",
    "def apply_standardization(df, exercise_column='name'):\n",
    "    \"\"\"\n",
    "    Apply standardization to your exercise DataFrame using broadcast join\n",
    "    \"\"\"\n",
    "    \n",
    "    standardizer = AdvancedExerciseStandardizer()\n",
    "    unique_exercises = [row.name for row in df.select(\"name\").distinct().collect() if row.name is not None]\n",
    "    mapping, review_df = standardizer.standardize_exercise_dataset(unique_exercises)\n",
    "    \n",
    "    print(\"Type of mapping:\", type(mapping))\n",
    "    print(\"First few items:\", list(mapping.items())[:5] if hasattr(mapping, 'items') else mapping.head())\n",
    "    \n",
    "    # mapping df containing original name and standardized name\n",
    "    if isinstance(mapping, dict):\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        mapping_data = [(k, v) for k, v in mapping.items()]\n",
    "        mapping_df = spark.createDataFrame(mapping_data, [\"name\", \"standardized_name\"])\n",
    "        \n",
    "        # can safely use a broadcast join here, mapping_df is quite small (< 500 records)\n",
    "        df = df.join(broadcast(mapping_df), on=\"name\", how=\"left\")\n",
    "    else:\n",
    "        df = df.join(broadcast(mapping), on=\"name\", how=\"left\")\n",
    "    \n",
    "    # drop records where a standardized name was not successfully generated\n",
    "    df = df.dropna(subset=['standardized_name'])\n",
    "    \n",
    "    return df, mapping, review_df\n",
    "\n",
    "# apply gen AI standardization of exercise names\n",
    "standardized_df, name_mapping, review_report = apply_standardization(exercise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475fcdbb-0651-4ee4-87fa-209ce1655b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standardized_df = standardized_df.select('name', 'standardized_name', 'category', 'equipment',  'muscles', 'muscles_secondary', 'description')\n",
    "display(standardized_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wger_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
