{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04ffa21-dccf-4cea-ae1c-f34fa5c13e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from langdetect import detect\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import pandas as pd\n",
    "import ast\n",
    "from json_repair import repair_json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize, punkt\n",
    "\n",
    "def get_exercises(api_url):\n",
    "    \"\"\"Retrieves complete list of exercises from the wger public API\"\"\"\n",
    "    all_exercises = []\n",
    "    current_url = api_url\n",
    "\n",
    "    try:\n",
    "        while current_url:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            all_exercises.extend(data[\"results\"])\n",
    "            current_url = data.get(\"next\")\n",
    "        return all_exercises\n",
    "    except Exception as e:\n",
    "        print(\"error fetching exercise data from wger API:\", e)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def is_english_content(description):\n",
    "    \"\"\"check if description is actually in English\"\"\"\n",
    "    try:\n",
    "        # check description language, sampling first 100 chars \n",
    "        if description and len(description.strip()) > 0:\n",
    "            desc_sample = description[:100]\n",
    "            desc_lang = detect(desc_sample)\n",
    "            if desc_lang != \"en\":\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # if detection fails, assume not english\n",
    "        return False\n",
    "    \n",
    "def normalize_record(record):\n",
    "    \"\"\"Normalize a record dictionary by extracting and formatting specific fields.\"\"\"\n",
    "    # extract name and description from translations with language == 2 (english)\n",
    "    translations = record.get(\"translations\", [])\n",
    "    name = None\n",
    "    description = None\n",
    "    \n",
    "    for translation in translations:\n",
    "        if translation.get(\"language\") == 2:\n",
    "            candidate_name = translation.get(\"name\")\n",
    "            candidate_description = translation.get(\"description\")\n",
    "            \n",
    "            # verify the content is english\n",
    "            if is_english_content(candidate_description):\n",
    "                name = candidate_name\n",
    "                description = candidate_description\n",
    "                break\n",
    "    \n",
    "    # return None if no English content found\n",
    "    # this is a data quality fix;\n",
    "    # there is a known issue in wger API that results in non-english records despite language = 2 filtering\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    # data cleaning step: remove HTML tags from name and description\n",
    "    if name:\n",
    "        name = re.sub(r\"<[^>]*>\", \"\", name).strip()\n",
    "    if description:\n",
    "        description = re.sub(r\"<[^>]*>\", \"\", description).strip()\n",
    "    \n",
    "    # parse datetime fields with proper error handling\n",
    "    created = None\n",
    "    if record.get(\"created\"):\n",
    "        created = datetime.fromisoformat(record[\"created\"].replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "    last_update = None\n",
    "    if record.get(\"last_update\"):\n",
    "        last_update = datetime.fromisoformat(\n",
    "            record[\"last_update\"].replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "    # generate normalized dict structure\n",
    "    normalized = {\n",
    "        \"id\": record.get(\"id\"),\n",
    "        \"uuid\": record.get(\"uuid\"),\n",
    "        \"name\": name.upper(),\n",
    "        \"description\": description,\n",
    "        \"created\": created,\n",
    "        \"last_update\": last_update,\n",
    "        \"category\": record.get(\"category\")[\"name\"],\n",
    "        \"muscles\": [muscle[\"name\"] for muscle in record.get(\"muscles\", [])],\n",
    "        \"muscles_secondary\": [muscle[\"name\"] for muscle in record.get(\"muscles_secondary\", [])],\n",
    "        \"equipment\": [equip[\"name\"] for equip in record.get(\"equipment\", [])],\n",
    "        \"variations\": record.get(\"variations\") or [],\n",
    "        \"license_author\": record.get(\"license_author\")}\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def query_databricks_foundation_model(prompt, model_name=\"databricks-llama-4-maverick\", max_tokens=512, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Query a Databricks foundation model through the serving endpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    # construct the API endpoint URL\n",
    "    api_url = f\"https://{DATABRICKS_INSTANCE}/serving-endpoints/{model_name}/invocations\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # chat payload\n",
    "    payload_messages = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # prompt payload\n",
    "    payload_prompt = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # input payload\n",
    "    payload_input = {\n",
    "        \"input\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # attempt each format above\n",
    "    for payload_name, payload in [(\"messages\", payload_messages), (\"prompt\", payload_prompt), (\"input\", payload_input)]:\n",
    "        try:\n",
    "            print(f\"Trying payload format: {payload_name}\")\n",
    "            response = requests.post(api_url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Success with {payload_name} format!\")\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed with {payload_name} format: {e}\")\n",
    "            if hasattr(e, \"response\") and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"all payload formats failed\")\n",
    "    return None\n",
    "\n",
    "# Alternative approach using the newer Databricks SDK\n",
    "def query_with_databricks_sdk(prompt, model_name=\"databricks-llama-4-maverick\", max_tokens=5000, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Alternative approach using Databricks SDK (if available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        \n",
    "        w = WorkspaceClient()\n",
    "        \n",
    "        # Format the request properly for the SDK\n",
    "        request_data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        response = w.serving_endpoints.query(\n",
    "            name=model_name,\n",
    "            **request_data\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except ImportError:\n",
    "        print(\"Databricks SDK not available, use the API approach instead\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error with SDK approach: {e}\")\n",
    "        return None\n",
    "\n",
    "def format_prompt(instruction, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Format prompt similar to your original template\n",
    "    Based on documentation here:\n",
    "    https://github.com/databricks/databricks-ml-examples/blob/master/llm-models/mistral/mistral-7b/01_load_inference.py\n",
    "    \"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"You are a JSON-only exercise standardization assistant. \n",
    "        ALWAYS respond with valid JSON in this exact format:\n",
    "        {\"groups\":[{\"standardized_name\":\"NAME\",\"variations\":[\"var1\",\"var2\"]}]}\n",
    "        Never include explanations, markdown, misspellings, or extra text.\"\"\"\n",
    "    \n",
    "    formatted_prompt = f\"\"\"[INST]<>{system_prompt}<>{instruction}[/INST]\"\"\"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "# Example usage\n",
    "def gen_text_databricks(prompts, use_template=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate text using Databricks foundation model\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        if use_template:\n",
    "            formatted_prompt = format_prompt(prompt)\n",
    "        else:\n",
    "            formatted_prompt = prompt\n",
    "            \n",
    "        # First try the API approach\n",
    "        response = query_databricks_foundation_model(\n",
    "            formatted_prompt, \n",
    "            max_tokens=kwargs.get(\"max_new_tokens\", 512),\n",
    "            temperature=kwargs.get(\"temperature\", 0.5)\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            # Handle different response formats\n",
    "            if \"choices\" in response and response[\"choices\"]:\n",
    "                # OpenAI-style response\n",
    "                content = response[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\") or response[\"choices\"][0].get(\"text\", \"\")\n",
    "                results.append(content)\n",
    "            elif \"predictions\" in response:\n",
    "                # MLflow-style response\n",
    "                results.append(response[\"predictions\"][0].get(\"generated_text\", \"\"))\n",
    "            elif \"candidates\" in response:\n",
    "                # Gemini-style response\n",
    "                results.append(response[\"candidates\"][0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"\"))\n",
    "            else:\n",
    "                # Try to extract any text from the response\n",
    "                results.append(str(response))\n",
    "        else:\n",
    "            # Try SDK approach as fallback\n",
    "            sdk_response = query_with_databricks_sdk(\n",
    "                formatted_prompt,\n",
    "                max_tokens=kwargs.get(\"max_new_tokens\", 512),\n",
    "                temperature=kwargs.get(\"temperature\", 0.5)\n",
    "            )\n",
    "            if sdk_response:\n",
    "                results.append(str(sdk_response))\n",
    "            else:\n",
    "                results.append(\"Error: Could not get response from model\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "DATABRICKS_INSTANCE = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "# documentation link: https://exercise.hellogym.io/nl/software/api\n",
    "# the exerciseinfo contains denormalized data with nested\n",
    "wger_api_url = \"https://wger.de/api/v2/exerciseinfo/?status=2&language=2\"\n",
    "exercises = get_exercises(wger_api_url)\n",
    "\n",
    "ex_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"uuid\", StringType()), \n",
    "    StructField(\"name\", StringType()), \n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"created\", TimestampType()),\n",
    "    StructField(\"last_update\", TimestampType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"muscles\",  StringType()),\n",
    "    StructField(\"muscles_secondary\",  StringType()),\n",
    "    StructField(\"equipment\",  StringType()),\n",
    "    StructField(\"variations\", StringType()),\n",
    "    StructField(\"license_author\", StringType())])\n",
    "    \n",
    "# filter out None values for data quality\n",
    "exercise_abbv = [normalize_record(record) for record in exercises]\n",
    "exercise_abbv = [record for record in exercise_abbv if record is not None]  # Remove non-English records\n",
    "\n",
    "try:\n",
    "    exercise_df = spark.createDataFrame(exercise_abbv, schema=ex_schema)\n",
    "    exercise_df = exercise_df.drop(\"uuid\", \"created\", \"last_update\", \"license_author\")\n",
    "    display(exercise_df.sort(\"name\"))\n",
    "except Exception as e:\n",
    "    print(\"error creating dataframe:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6e88f4-28c9-447e-8d2b-c83478bf3c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS = {'WITH', 'AND', 'ON', 'INTO', 'IN', 'AT', 'BY', 'OF', 'FOR', 'TO',\n",
    "    'FROM', 'USING', 'THE', 'A', 'AN', 'ONTO'}\n",
    "\n",
    "def validate_json_structure(result):\n",
    "    \"\"\"Validate that the JSON has the required structure\"\"\"\n",
    "    if not isinstance(result, dict):\n",
    "        raise ValueError(\"Result is not a dictionary\")\n",
    "    \n",
    "    if \"groups\" not in result:\n",
    "        raise ValueError(\"Missing 'groups' key\")\n",
    "    \n",
    "    if not isinstance(result[\"groups\"], list):\n",
    "        raise ValueError(\"'groups' is not a list\")\n",
    "    \n",
    "    for i, group in enumerate(result[\"groups\"]):\n",
    "        if not isinstance(group, dict):\n",
    "            raise ValueError(f\"Group {i} is not a dictionary\")\n",
    "        \n",
    "        # Check for standardized name with multiple possible keys\n",
    "        if not any(key in group for key in [\"standardized_name\", \"standard_name\"]):\n",
    "            raise ValueError(f\"Group {i} missing standardized name key\")\n",
    "        \n",
    "        if \"variations\" not in group:\n",
    "            raise ValueError(f\"Group {i} missing 'variations' key\")\n",
    "        \n",
    "        if not isinstance(group[\"variations\"], list):\n",
    "            raise ValueError(f\"Group {i} 'variations' is not a list\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def escalate_prompt(base_prompt, level):\n",
    "    escalated = base_prompt\n",
    "    if level == 1:\n",
    "        escalated += \"\\nSTRICTLY enforce JSON compliance. Only output a parsable JSON object.\"\n",
    "    elif level == 2:\n",
    "        escalated += \"\\nYour previous response was invalid. Ensure the response is a clean JSON object with no extra formatting or text.\"\n",
    "    elif level >= 3:\n",
    "        escalated += \"\\nFinal warning: Return ONLY a valid JSON object. No natural language, no markdown, no surrounding explanations.\"\n",
    "    return escalated\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "# dataset augmentation module\n",
    "class AdvancedExerciseStandardizer:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize with a sentence transformer model optimized for semantic similarity\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def preprocess_exercise_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess exercise name with lemmatization and stopword removal\n",
    "        \"\"\"\n",
    "        name = name.upper()\n",
    "        name = re.sub(r'[-_/\\\\|]+', ' ', name)\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        tokens = word_tokenize(name.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        lemmatized = [\n",
    "            lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "            for word, pos in tagged\n",
    "            if word.upper() not in STOPWORDS]\n",
    "\n",
    "        return ' '.join([t.upper() for t in lemmatized])\n",
    "\n",
    "    def extract_exercise_features(self, exercise_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract key features that distinguish exercises\n",
    "        \"\"\"\n",
    "        name_upper = exercise_name.upper()\n",
    "        \n",
    "        # Find equipment in the name\n",
    "        equipment_found = set()\n",
    "        equipment_list = [\n",
    "            \"BARBELL\", \"DUMBBELL\", \"KETTLEBELL\", \n",
    "            \"EZ BAR\", \"EZ-BAR\", \"EZBAR\", \"T-BAR\", \"T BAR\", \"HEX\", \"HEX BAR\",\n",
    "            \"CABLE\",  \"ROPE\", \"STRAIGHT BAR\", \"V-BAR\", \"TRICEP BAR\",\n",
    "            \"HANDLE\", \"SINGLE HANDLE\", \"MACHINE\", \"SMITH MACHINE\", \"CABLE MACHINE\",\n",
    "            \"PREACHER\", \"LEVER\", \"HACK\",\"TRX\", \"SUSPENSION\", \"BAR\", \"BAG\",\n",
    "            \"RESISTANCE BAND\", \"BAND\", \"BANDED\", \"MINI BAND\", \"THERABAND\",\n",
    "            \"BENCH\", \"INCLINE BENCH\", \"DECLINE BENCH\", \"ADJUSTABLE BENCH\", \"STEP\", \"BOX\", \"PULL BLOCK\", \"BALL\"]\n",
    "        \n",
    "        for equip in equipment_list:\n",
    "            if equip in name_upper:\n",
    "                equipment_found.add(equip)\n",
    "\n",
    "        # Find movement patterns\n",
    "        movement_found = set()\n",
    "        movement_list = [\n",
    "            \"SQUAT\", \"DEADLIFT\", \"LUNGE\", \"STEP-UP\", \"PULL-APART\",\n",
    "            \"HINGE\", \"PUSH\", \"PULL\", \"PRESS\", \"ROW\",\n",
    "            \"DIP\", \"CURL\", \"EXTENSION\", \"RAISE\", \"FLY\", \"FLYE\",\n",
    "            \"PULLOVER\", \"SHRUG\", \"ROTATION\", \"TWIST\",\n",
    "            \"CRUNCH\", \"PLANK\", \"HOLD\", \"BRIDGE\",  \"BEND\",\n",
    "            \"LIFT\", \"JUMP\", \"HOP\", \"SPRINT\", \"KICKBACK\",\n",
    "            \"CLAP\", \"THRUST\", \"TAP\", \"THROWS\",  \"PULL UP\",\n",
    "            \"PUSH DOWN\", \"CHIN UP\", \"CLIMB\", \"CARRY\", \"WALK\", \"THROW\", \"ROLL\"]\n",
    "        \n",
    "        for movement in movement_list:\n",
    "            if movement in name_upper:\n",
    "                movement_found.add(movement)\n",
    "        \n",
    "        # Find body parts\n",
    "        body_parts_found = set()\n",
    "        body_parts_list = [\n",
    "            \"NECK\", \"TRAP\", \"SHOULDER\", \"DELTOID\", \"DELT\",\n",
    "            \"BICEP\", \"BI\", \"TRICEP\", \"TRI\",\n",
    "            \"FOREARM\", \"CHEST\", \"PECTORAL\", \"PEC\",\n",
    "            \"BACK\", \"LAT\", \"UPPER BACK\", \"LOWER BACK\",\n",
    "            \"ABS\", \"ABDOMINALS\", \"AB\", \"OBLIQUE\", \"CORE\",\n",
    "            \"GLUTE\", \"BUTT\", \"HIP\",  \"HIP FLEXOR\",\n",
    "            \"ADDUCTOR\", \"ABDUCTOR\", \"QUAD\", \"QUADRICEP\", \"HAMSTRING\", \"HAM\",\n",
    "            \"CALF\", \"LEG\", \"ARM\", \"CHIN\"]\n",
    "        \n",
    "        for body_part in body_parts_list:\n",
    "            if body_part in name_upper:\n",
    "                body_parts_found.add(body_part)\n",
    "\n",
    "        # Find modifiers\n",
    "        modifiers_found = set()\n",
    "        modifiers_list = [\n",
    "            \"WIDE\", \"WIDE-GRIP\", \"NARROW\", \"NARROW-GRIP\", \"CLOSE\", \"CLOSE-GRIP\", \"CROSSBODY\", \"CROSS-BODY\",\n",
    "            \"HAMMER\", \"NEUTRAL-GRIP\", \"OVERHAND\", \"UNDERHAND\", \"SUPINATED\", \"PRONATED\", \n",
    "            \"REVERSE\", \"MIXED-GRIP\", \"SINGLE-ARM\", \"ONE-ARM\", \"ONE-ARMED\",\"SINGLE-LEG\", \"ONE-LEG\", \"ONE-LEGGED\",\n",
    "            \"UNILATERAL\", \"ALTERNATING\", \"ALTERNATE\",\"ISOLATED\", \"ISOLATION\",\"CROSSOVER\", \"CROSS-BODY\",\"BILATERAL\", \n",
    "            \"DOUBLE-ARM\", \"DOUBLE-LEG\", \"SUMO\", \"WIDE-STANCE\", \"NARROW-STANCE\", \"CLOSE-STANCE\", \"SPLIT\", \"BULGARIAN\",\n",
    "            \"STAGGERED\",\"STEP-BACK\", \"STEP-UP\",\"LATERAL\", \"SIDE\", \"SIDE-STEP\",\"CROSSOVER\", \"REVERSE\", \"INCLINE\", \n",
    "            \"DECLINE\", \"FLAT\",\"VERTICAL\", \"HORIZONTAL\",\"DIAGONAL\", \"ROTATIONAL\", \"TWISTING\", \"STANDING\", \"SEATED\", \n",
    "            \"SITTING\", \"LYING\", \"PRONE\", \"SUPINE\", \"SIDE-LYING\", \"KNEELING\", \"HALF-KNEELING\", \"LEANING\", \n",
    "            \"FORWARD-LEANING\", \"PARTIAL\", \"FULL\", \"HALF\", \"HOLD\", \"ISOMETRIC\", \"ISO\", \"STATIC-HOLD\",\"LATERAL\"]\n",
    "        \n",
    "        for modifier in modifiers_list:\n",
    "            if modifier in name_upper:\n",
    "                modifiers_found.add(modifier)\n",
    "        \n",
    "        return {\n",
    "            \"equipment\": equipment_found,\n",
    "            \"movement\": movement_found,\n",
    "            \"body_parts\": body_parts_found,\n",
    "            \"modifiers\": modifiers_found,\n",
    "            \"raw_name\": exercise_name}\n",
    "\n",
    "    def create_feature_aware_embeddings(self, exercise_names: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embeddings that emphasize critical distinguishing features\n",
    "        \"\"\"\n",
    "        enhanced_texts = []\n",
    "        \n",
    "        for name in exercise_names:\n",
    "            # First, preprocess the name (lemmatize, remove stopwords)\n",
    "            processed_name = self.preprocess_exercise_name(name)\n",
    "            features = self.extract_exercise_features(name)\n",
    "            \n",
    "            # Create enhanced text that emphasizes critical features\n",
    "            enhanced_parts = [processed_name]  # Use processed name\n",
    "            \n",
    "            # Equipment gets highest weight (most distinguishing) - 3x\n",
    "            for equip in features[\"equipment\"]:\n",
    "                enhanced_parts.extend([equip] * 3)\n",
    "            \n",
    "            # Movement gets medium weight - 2x  \n",
    "            for movement in features[\"movement\"]:\n",
    "                enhanced_parts.extend([movement] * 2)\n",
    "            \n",
    "            # Modifiers get medium weight - 2x\n",
    "            for modifier in features[\"modifiers\"]:\n",
    "                enhanced_parts.extend([modifier] * 2)\n",
    "            \n",
    "            # Body parts get lower weight - 1x\n",
    "            for body_part in features[\"body_parts\"]:\n",
    "                enhanced_parts.append(body_part)\n",
    "            \n",
    "            enhanced_texts.append(\" \".join(enhanced_parts))\n",
    "        \n",
    "        return self.model.encode(enhanced_texts)\n",
    "\n",
    "    def feature_based_pre_clustering(self, exercise_names: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Pre-cluster exercises by equipment type to create more balanced groups\n",
    "        \"\"\"\n",
    "        equipment_groups = {}\n",
    "        \n",
    "        for name in exercise_names:\n",
    "            features = self.extract_exercise_features(name)\n",
    "            \n",
    "            # Create equipment signature (primary equipment)\n",
    "            if features[\"equipment\"]:\n",
    "                # Prioritize more specific equipment\n",
    "                equipment_priority = [\n",
    "                    \"BARBELL\", \"DUMBBELL\", \"KETTLEBELL\", \"EZ BAR\", \"EZ-BAR\", \"EZBAR\",\n",
    "                    \"T-BAR\", \"T BAR\", \"HEX BAR\", \"CABLE\", \"MACHINE\", \"SMITH MACHINE\"]\n",
    "                \n",
    "                primary_equipment = None\n",
    "                for equip in equipment_priority:\n",
    "                    if equip in features[\"equipment\"]:\n",
    "                        primary_equipment = equip\n",
    "                        break\n",
    "                \n",
    "                if not primary_equipment:\n",
    "                    primary_equipment = next(iter(features[\"equipment\"]))\n",
    "            else:\n",
    "                primary_equipment = \"BODYWEIGHT\"\n",
    "            \n",
    "            # Add movement pattern for finer grouping\n",
    "            if features[\"movement\"]:\n",
    "                primary_movement = next(iter(features[\"movement\"]))\n",
    "                group_key = f\"{primary_equipment}_{primary_movement}\"\n",
    "            else:\n",
    "                group_key = primary_equipment\n",
    "            \n",
    "            if group_key not in equipment_groups:\n",
    "                equipment_groups[group_key] = []\n",
    "            equipment_groups[group_key].append(name)\n",
    "        \n",
    "        return equipment_groups\n",
    "\n",
    "    def balanced_clustering_within_group(self, exercise_names: List[str], max_cluster_size: int = 25) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Perform balanced clustering within a pre-filtered group\n",
    "        \"\"\"\n",
    "        if len(exercise_names) <= max_cluster_size:\n",
    "            return [exercise_names]\n",
    "        \n",
    "        # Create embeddings for this group\n",
    "        embeddings = self.create_feature_aware_embeddings(exercise_names)\n",
    "        \n",
    "        # Calculate initial number of clusters\n",
    "        n_clusters = max(1, (len(exercise_names) + max_cluster_size - 1) // max_cluster_size)\n",
    "        \n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "        cluster_labels = clustering.fit_predict(embeddings)\n",
    "        \n",
    "        # Group exercises by cluster\n",
    "        clusters = []\n",
    "        for cluster_id in set(cluster_labels):\n",
    "            cluster_exercises = [exercise_names[i] for i in range(len(exercise_names)) \n",
    "                               if cluster_labels[i] == cluster_id]\n",
    "            \n",
    "            # Recursively split if still too large\n",
    "            if len(cluster_exercises) > max_cluster_size:\n",
    "                sub_clusters = self.balanced_clustering_within_group(cluster_exercises, max_cluster_size)\n",
    "                clusters.extend(sub_clusters)\n",
    "            else:\n",
    "                clusters.append(cluster_exercises)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "    def get_base_prompt(self, exercise_names):\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert exercise classification system. Your task is to analyze a list of exercise names and determine whether they represent the same or distinct exercises.\n",
    "\n",
    "        For any exercises that refer to the same underlying movement, group them together and provide a single standardized name for the group. \n",
    "        If exercises are functionally distinct, group them separately with unique standardized names.\n",
    "\n",
    "        Use the following strict disambiguation rules:\n",
    "\n",
    "        RULES FOR CLASSIFICATION:\n",
    "        1. Exercises using different equipment (e.g., Barbell vs Dumbbell vs EZ Bar) = DIFFERENT exercises.\n",
    "        2. Exercises with different grips or hand positions (e.g., Wide, Narrow, Neutral, Reverse) = DIFFERENT exercises.\n",
    "        3. Exercises performed at different body positions or angles (e.g., Incline, Decline, Flat, Overhead) = DIFFERENT exercises.\n",
    "        4. Variations due only to spelling, punctuation, pluralization, or casing = SAME exercise.\n",
    "        5. Left/right or single-side references (e.g., \"Left Arm\", \"Right Leg\") = SAME exercise.\n",
    "        6. NEVER use hyphens in your standardized names (e.g., \"Pull Up\", NOT \"Pull-Up\")\n",
    "        7. Do not use acronyms or abbreviations in your standardized names.\n",
    "        8. Be aware of common acronyms and abbreviations in exercise names (e.g., \"OHP\" for \"Overhead Press\", \"DB\" for \"Dumbbell\", \"BB\" for \"Barbell\") and do not persist acronyms or abbreviations in standardized names.\n",
    "        9. Do NOT generalize specific exercise names into broader categories (e.g., \"Spider Curl\" should not be generalized to \"Bicep Curl\", \"Bird Dog\" should not be generalized to \"Arabesque\")\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        - Return output as **valid JSON** only — do not include explanations, markdown, or extra text.\n",
    "        - Do not combine individual words into single tokens (e.g., avoid malformed outputs like \"BENTOVERROWTOEXTERNALROTATION\").\n",
    "        - Ensure standardized names are clean, readable, and semantically meaningful.\n",
    "\n",
    "        INPUT:\n",
    "        Exercise names: {exercise_names}\n",
    "\n",
    "        EXPECTED OUTPUT FORMAT:\n",
    "        {{\n",
    "        \"groups\": [\n",
    "            {{\n",
    "            \"standardized_name\": \"STANDARDIZED EXERCISE NAME\",\n",
    "            \"variations\": [\"Variation 1\", \"Variation 2\", \"...\"]\n",
    "            }},\n",
    "            {{\n",
    "            \"standardized_name\": \"ANOTHER STANDARDIZED EXERCISE NAME\",\n",
    "            \"variations\": [\"Variation A\", \"Variation B\"]\n",
    "            }}\n",
    "        ]\n",
    "        }}\n",
    "\n",
    "        EXAMPLE OUTPUT:\n",
    "        {{\n",
    "        \"groups\": [\n",
    "            {{\n",
    "            \"standardized_name\": \"BARBELL BICEP CURL\",\n",
    "            \"variations\": [\"Barbell Biceps Curl\", \"Bar Bell Bicep Curls\"]\n",
    "            }},\n",
    "            {{\n",
    "            \"standardized_name\": \"EZ BAR BICEP CURL\",\n",
    "            \"variations\": [\"EZ Bar Biceps Curl\", \"Ez-Bar Bicep Curl\"]\n",
    "            }}\n",
    "        ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def llm_validate_cluster(self, exercise_names: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        use LLM to validate and standardize a cluster of exercise names\n",
    "        \"\"\"\n",
    "        prompt = self.get_base_prompt(exercise_names).replace(\"'\", \"''\")\n",
    "        \n",
    "        bad_response = True\n",
    "        attempt = 0\n",
    "        max_attempts = 5\n",
    "\n",
    "        while bad_response and attempt < max_attempts:\n",
    "            try:\n",
    "                escalated_prompt = escalate_prompt(self.get_base_prompt(exercise_names), attempt)\n",
    "                \n",
    "                # leveraging Databricks foundation model (free tier...)\n",
    "                response = str(gen_text_databricks([escalated_prompt], temperature=0.1, max_new_tokens=5000, use_template=True))\n",
    "                response = response.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                \n",
    "                repaired = repair_json(response)\n",
    "                parsed = ast.literal_eval(json.loads(repaired)[0])\n",
    "                validate_json_structure(parsed)\n",
    "                \n",
    "                print(\"Success:\")\n",
    "                bad_response = False\n",
    "                \n",
    "                # Create mapping from variations to standardized names\n",
    "                mapping = {}\n",
    "                for group in parsed[\"groups\"]:\n",
    "                    try:\n",
    "                        standardized = group[\"standardized_name\"]\n",
    "                    except: # plan for the LLM to hallucinate standardized name \n",
    "                        standardized = group[\"standard_name\"]\n",
    "                    for variation in group[\"variations\"]:\n",
    "                        mapping[variation] = standardized\n",
    "                \n",
    "                return mapping\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nAttempt {attempt + 1} failed.\")\n",
    "                print(\"Error:\", str(e))\n",
    "                print(\"Response:\", response[:500] if 'response' in locals() else \"No response\")\n",
    "                attempt += 1\n",
    "        \n",
    "        # If all attempts failed, raise the last exception\n",
    "        raise Exception(f\"LLM validation failed after {max_attempts} attempts\")\n",
    "\n",
    "    def smart_clustering(self, exercise_names: List[str], max_cluster_size: int = 25) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Perform intelligent clustering with balanced cluster sizes\n",
    "        \"\"\"\n",
    "        print(f\"Starting smart clustering for {len(exercise_names)} exercises...\")\n",
    "        \n",
    "        # Step 1: Pre-cluster by equipment/movement to create balanced groups\n",
    "        equipment_groups = self.feature_based_pre_clustering(exercise_names)\n",
    "        print(f\"Pre-clustering created {len(equipment_groups)} equipment/movement groups\")\n",
    "        \n",
    "        final_mapping = {}\n",
    "        \n",
    "        # Step 2: Process each equipment group separately\n",
    "        for group_name, group_exercises in equipment_groups.items():\n",
    "            print(f\"Processing {group_name} group with {len(group_exercises)} exercises\")\n",
    "            \n",
    "            if len(group_exercises) == 1:\n",
    "                # Single exercise - no clustering needed\n",
    "                final_mapping[group_exercises[0]] = group_exercises[0]\n",
    "                continue\n",
    "            \n",
    "            # Step 3: Balanced clustering within each group\n",
    "            balanced_clusters = self.balanced_clustering_within_group(group_exercises, max_cluster_size)\n",
    "            \n",
    "            # Step 4: LLM validation for each balanced cluster\n",
    "            for cluster in balanced_clusters:\n",
    "                print(f\"  Processing cluster with {len(cluster)} exercises\")\n",
    "                \n",
    "                if len(cluster) == 1:\n",
    "                    final_mapping[cluster[0]] = cluster[0]\n",
    "                else:\n",
    "                    try:\n",
    "                        cluster_mapping = self.llm_validate_cluster(cluster)\n",
    "                        final_mapping.update(cluster_mapping)\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Warning: LLM validation failed for cluster, treating as individual exercises\")\n",
    "                        # Fallback: treat each as individual exercise\n",
    "                        for exercise in cluster:\n",
    "                            final_mapping[exercise] = exercise\n",
    "        \n",
    "        print(f\"Smart clustering complete. Processed {len(final_mapping)} exercises\")\n",
    "        return final_mapping\n",
    "\n",
    "    def standardize_exercise_dataset(self, exercise_names: List[str]) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Complete standardization pipeline\n",
    "        \"\"\"\n",
    "        print(\"Starting exercise standardization pipeline...\")\n",
    "        \n",
    "        # Remove obvious invalid exercises\n",
    "        valid_exercises = [name for name in exercise_names \n",
    "                          if not any(invalid in name.upper() \n",
    "                                   for invalid in [\"REST\", \"JOGGING\", \"WALKING\", \"CYCLING\"])]\n",
    "        \n",
    "        print(f\"Filtered out {len(exercise_names) - len(valid_exercises)} invalid exercises\")\n",
    "        \n",
    "        # Perform smart clustering\n",
    "        mapping = self.smart_clustering(valid_exercises)\n",
    "        \n",
    "        # Generate summary\n",
    "        standardized_names = list(set(mapping.values()))\n",
    "        \n",
    "        for k, v in mapping.items():\n",
    "            tokens = word_tokenize(v.lower())\n",
    "            tagged = pos_tag(tokens)\n",
    "            lemmatized = [\n",
    "                lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for word, pos in tagged\n",
    "                if word.upper() not in STOPWORDS]\n",
    "            mapping[k] = ' '.join([t.upper() for t in lemmatized]).replace('-', ' ') # update mapping with lemmatized version\n",
    "        \n",
    "        # Group by standardized name for review\n",
    "        groups = {}\n",
    "        for original, standardized in mapping.items():\n",
    "            if standardized not in groups:\n",
    "                groups[standardized] = []\n",
    "            groups[standardized].append(original)\n",
    "        \n",
    "        # Create DataFrame for review\n",
    "        review_data = []\n",
    "        for standardized, originals in groups.items():\n",
    "        \n",
    "            review_data.append({\n",
    "                \"standardized_name\": standardized,\n",
    "                \"original_count\": len(originals),\n",
    "                \"original_names\": \" | \".join(originals)})\n",
    "        \n",
    "        review_df = pd.DataFrame(review_data).sort_values(\"original_count\", ascending=False)\n",
    "        \n",
    "        print(f\"Standardization complete:\")\n",
    "        print(f\"  Original exercises: {len(exercise_names)}\")\n",
    "        print(f\"  Valid exercises: {len(valid_exercises)}\")\n",
    "        print(f\"  Standardized exercises: {len(standardized_names)}\")\n",
    "        print(f\"  Reduction: {((len(valid_exercises) - len(standardized_names)) / len(valid_exercises) * 100):.1f}%\")\n",
    "        \n",
    "        return mapping, review_df\n",
    "\n",
    "# implementation\n",
    "def apply_standardization(df, exercise_column=\"name\"):\n",
    "    \"\"\"\n",
    "    Apply standardization to your exercise DataFrame using broadcast join\n",
    "    \"\"\"\n",
    "    \n",
    "    standardizer = AdvancedExerciseStandardizer()\n",
    "    unique_exercises = [row.name for row in df.select(\"name\").distinct().collect() if row.name is not None]\n",
    "    mapping, review_df = standardizer.standardize_exercise_dataset(unique_exercises)\n",
    "    \n",
    "    print(\"Type of mapping:\", type(mapping))\n",
    "    print(\"First few items:\", list(mapping.items())[:5] if hasattr(mapping, \"items\") else mapping.head())\n",
    "    \n",
    "    # mapping df containing original name and standardized name\n",
    "    if isinstance(mapping, dict):\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        mapping_data = [(k, v) for k, v in mapping.items()]\n",
    "        mapping_df = spark.createDataFrame(mapping_data, [\"name\", \"standardized_name\"])\n",
    "        \n",
    "        # can safely use a broadcast join here, mapping_df is quite small (< 500 records)\n",
    "        df = df.join(broadcast(mapping_df), on=\"name\", how=\"left\")\n",
    "    else:\n",
    "        df = df.join(broadcast(mapping), on=\"name\", how=\"left\")\n",
    "    \n",
    "    # drop records where a standardized name was not successfully generated\n",
    "    df = df.dropna(subset=[\"standardized_name\"])\n",
    "    \n",
    "    return df, mapping, review_df\n",
    "\n",
    "# apply gen AI standardization of exercise names\n",
    "standardized_df, name_mapping, review_report = apply_standardization(exercise_df)\n",
    "display(standardized_df.select(\"name\", \"standardized_name\").sort(\"standardized_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43721c21-db1c-478c-8ab3-f4974fc02875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wger_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
