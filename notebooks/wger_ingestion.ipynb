{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04ffa21-dccf-4cea-ae1c-f34fa5c13e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from langdetect import detect\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import pandas as pd\n",
    "import ast\n",
    "from json_repair import repair_json\n",
    "\n",
    "def get_exercises(api_url):\n",
    "    '''Retrieves complete list of exercises from the wger public API'''\n",
    "    all_exercises = []\n",
    "    current_url = api_url\n",
    "\n",
    "    try:\n",
    "        while current_url:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            all_exercises.extend(data['results'])\n",
    "            current_url = data.get('next')\n",
    "        return all_exercises\n",
    "    except Exception as e:\n",
    "        print('error fetching exercise data from wger API:', e)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def is_english_content(description):\n",
    "    \"\"\"check if description is actually in English\"\"\"\n",
    "    try:\n",
    "        # check description language, sampling first 100 chars \n",
    "        if description and len(description.strip()) > 0:\n",
    "            desc_sample = description[:100]\n",
    "            desc_lang = detect(desc_sample)\n",
    "            if desc_lang != 'en':\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # if detection fails, assume not english\n",
    "        return False\n",
    "    \n",
    "def normalize_record(record):\n",
    "    \"\"\"Normalize a record dictionary by extracting and formatting specific fields.\"\"\"\n",
    "    # extract name and description from translations with language == 2 (english)\n",
    "    translations = record.get('translations', [])\n",
    "    name = None\n",
    "    description = None\n",
    "    \n",
    "    for translation in translations:\n",
    "        if translation.get(\"language\") == 2:\n",
    "            candidate_name = translation.get(\"name\")\n",
    "            candidate_description = translation.get(\"description\")\n",
    "            \n",
    "            # verify the content is english\n",
    "            if is_english_content(candidate_description):\n",
    "                name = candidate_name\n",
    "                description = candidate_description\n",
    "                break\n",
    "    \n",
    "    # return None if no English content found\n",
    "    # this is a data quality fix;\n",
    "    # there is a known issue in wger API that results in non-english records despite language = 2 filtering\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    # data cleaning step: remove HTML tags from name and description\n",
    "    if name:\n",
    "        name = re.sub(r'<[^>]*>', '', name).strip()\n",
    "    if description:\n",
    "        description = re.sub(r'<[^>]*>', '', description).strip()\n",
    "    \n",
    "    # parse datetime fields with proper error handling\n",
    "    created = None\n",
    "    if record.get(\"created\"):\n",
    "        created = datetime.fromisoformat(record[\"created\"].replace('Z', '+00:00'))\n",
    "    \n",
    "    last_update = None\n",
    "    if record.get(\"last_update\"):\n",
    "        last_update = datetime.fromisoformat(\n",
    "            record[\"last_update\"].replace('Z', '+00:00'))\n",
    "    \n",
    "    # generate normalized dict structure\n",
    "    normalized = {\n",
    "        \"id\": record.get(\"id\"),\n",
    "        \"uuid\": record.get(\"uuid\"),\n",
    "        \"name\": name.upper(),\n",
    "        \"description\": description,\n",
    "        \"created\": created,\n",
    "        \"last_update\": last_update,\n",
    "        \"category\": record.get(\"category\")[\"name\"],\n",
    "        \"muscles\": [muscle[\"name\"] for muscle in record.get(\"muscles\", [])],\n",
    "        \"muscles_secondary\": [muscle[\"name\"] for muscle in record.get(\"muscles_secondary\", [])],\n",
    "        \"equipment\": [equip[\"name\"] for equip in record.get(\"equipment\", [])],\n",
    "        \"variations\": record.get(\"variations\") or [],\n",
    "        \"license_author\": record.get(\"license_author\")}\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def query_databricks_foundation_model(prompt, model_name=\"databricks-llama-4-maverick\", max_tokens=2000, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Query a Databricks foundation model through the serving endpoint\n",
    "    \"\"\"\n",
    "    # construct the API endpoint URL\n",
    "    api_url = f\"https://{DATABRICKS_INSTANCE}/serving-endpoints/{model_name}/invocations\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # chat payload\n",
    "    payload_messages = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # prompt payload\n",
    "    payload_prompt = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # input payload\n",
    "    payload_input = {\n",
    "        \"input\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # attempt each format above\n",
    "    for payload_name, payload in [(\"messages\", payload_messages), (\"prompt\", payload_prompt), (\"input\", payload_input)]:\n",
    "        try:\n",
    "            print(f\"Trying payload format: {payload_name}\")\n",
    "            response = requests.post(api_url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Success with {payload_name} format!\")\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed with {payload_name} format: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"all payload formats failed\")\n",
    "    return None\n",
    "\n",
    "# Alternative approach using the newer Databricks SDK\n",
    "def query_with_databricks_sdk(prompt, model_name=\"databricks-meta-llama-3-1-70b-instruct\", max_tokens=2000, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Alternative approach using Databricks SDK (if available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        \n",
    "        w = WorkspaceClient()\n",
    "        \n",
    "        # Format the request properly for the SDK\n",
    "        request_data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        response = w.serving_endpoints.query(\n",
    "            name=model_name,\n",
    "            **request_data\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except ImportError:\n",
    "        print(\"Databricks SDK not available, use the API approach instead\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error with SDK approach: {e}\")\n",
    "        return None\n",
    "\n",
    "def format_prompt(instruction, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Format prompt similar to your original template\n",
    "    Based on documentation here:\n",
    "    https://github.com/databricks/databricks-ml-examples/blob/master/llm-models/mistral/mistral-7b/01_load_inference.py\n",
    "    \"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"You are a JSON-only exercise standardization assistant. \n",
    "        ALWAYS respond with valid JSON in this exact format:\n",
    "        {\"groups\":[{\"standardized_name\":\"NAME\",\"variations\":[\"var1\",\"var2\"]}]}\n",
    "        Never include explanations, markdown, misspellings, or extra text.\"\"\"\n",
    "    \n",
    "    formatted_prompt = f\"\"\"<s>[INST]<<SYS>>{system_prompt}<</SYS>>{instruction}[/INST]\"\"\"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "# Example usage\n",
    "def gen_text_databricks(prompts, use_template=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate text using Databricks foundation model\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        if use_template:\n",
    "            formatted_prompt = format_prompt(prompt)\n",
    "        else:\n",
    "            formatted_prompt = prompt\n",
    "            \n",
    "        # First try the API approach\n",
    "        response = query_databricks_foundation_model(\n",
    "            formatted_prompt, \n",
    "            max_tokens=kwargs.get('max_new_tokens', 2000),\n",
    "            temperature=kwargs.get('temperature', 0.5)\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            # Handle different response formats\n",
    "            if 'choices' in response and response['choices']:\n",
    "                # OpenAI-style response\n",
    "                content = response['choices'][0].get('message', {}).get('content', '') or response['choices'][0].get('text', '')\n",
    "                results.append(content)\n",
    "            elif 'predictions' in response:\n",
    "                # MLflow-style response\n",
    "                results.append(response['predictions'][0].get('generated_text', ''))\n",
    "            elif 'candidates' in response:\n",
    "                # Gemini-style response\n",
    "                results.append(response['candidates'][0].get('content', {}).get('parts', [{}])[0].get('text', ''))\n",
    "            else:\n",
    "                # Try to extract any text from the response\n",
    "                results.append(str(response))\n",
    "        else:\n",
    "            # Try SDK approach as fallback\n",
    "            sdk_response = query_with_databricks_sdk(\n",
    "                formatted_prompt,\n",
    "                max_tokens=kwargs.get('max_new_tokens', 2000),\n",
    "                temperature=kwargs.get('temperature', 0.5)\n",
    "            )\n",
    "            if sdk_response:\n",
    "                results.append(str(sdk_response))\n",
    "            else:\n",
    "                results.append(\"Error: Could not get response from model\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "DATABRICKS_INSTANCE = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "# documentation link: https://exercise.hellogym.io/nl/software/api\n",
    "# the exerciseinfo contains denormalized data with nested\n",
    "wger_api_url = \"https://wger.de/api/v2/exerciseinfo/?status=2&language=2\"\n",
    "exercises = get_exercises(wger_api_url)\n",
    "\n",
    "ex_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"uuid\", StringType()), \n",
    "    StructField(\"name\", StringType()), \n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"created\", TimestampType()),\n",
    "    StructField(\"last_update\", TimestampType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"muscles\",  StringType()),\n",
    "    StructField(\"muscles_secondary\",  StringType()),\n",
    "    StructField(\"equipment\",  StringType()),\n",
    "    StructField(\"variations\", StringType()),\n",
    "    StructField(\"license_author\", StringType())])\n",
    "    \n",
    "# filter out None values for data quality\n",
    "exercise_abbv = [normalize_record(record) for record in exercises]\n",
    "exercise_abbv = [record for record in exercise_abbv if record is not None]  # Remove non-English records\n",
    "\n",
    "try:\n",
    "    exercise_df = spark.createDataFrame(exercise_abbv, schema=ex_schema)\n",
    "    exercise_df = exercise_df.drop(\"uuid\", \"created\", \"last_update\", \"license_author\")\n",
    "    display(exercise_df.sort(\"name\"))\n",
    "except Exception as e:\n",
    "    print(\"error creating dataframe:\", e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475fcdbb-0651-4ee4-87fa-209ce1655b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Set, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class ExerciseFeatures:\n",
    "    \"\"\"Structured representation of exercise features\"\"\"\n",
    "    equipment: Set[str] = field(default_factory=set)\n",
    "    movement_pattern: Optional[str] = None\n",
    "    muscle_groups: Set[str] = field(default_factory=set)\n",
    "    modifiers: Set[str] = field(default_factory=set)\n",
    "    angle_position: Optional[str] = None\n",
    "    grip_stance: Optional[str] = None\n",
    "    raw_name: str = \"\"\n",
    "    cleaned_name: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class StandardizationConfig:\n",
    "    \"\"\"Configuration for the standardization pipeline\"\"\"\n",
    "    similarity_threshold: float = 0.85\n",
    "    feature_weight: float = 0.3\n",
    "    semantic_weight: float = 0.7\n",
    "    min_cluster_size: int = 2\n",
    "    \n",
    "    # Equipment hierarchies - more specific equipment takes precedence\n",
    "    equipment_patterns: Dict[str, Set[str]] = field(default_factory=lambda: {\n",
    "        'BARBELL': {'BARBELL', 'BB', 'OLYMPIC BAR'},\n",
    "        'DUMBBELL': {'DUMBBELL', 'DB', 'DUMBBELLS'},\n",
    "        'CABLE': {'CABLE', 'CABLES', 'PULLEY'},\n",
    "        'MACHINE': {'MACHINE', 'SMITH MACHINE', 'HACK SQUAT'},\n",
    "        'KETTLEBELL': {'KETTLEBELL', 'KB'},\n",
    "        'RESISTANCE_BAND': {'RESISTANCE BAND', 'BAND', 'ELASTIC'},\n",
    "        'BODYWEIGHT': {'BODYWEIGHT', 'BW', 'CALISTHENICS'},\n",
    "        'EZ_BAR': {'EZ BAR', 'EZ-BAR', 'EZBAR', 'CURL BAR'},\n",
    "        'T_BAR': {'T-BAR', 'T BAR', 'TBAR'}\n",
    "    })\n",
    "    \n",
    "    movement_patterns: Dict[str, Set[str]] = field(default_factory=lambda: {\n",
    "        'PRESS': {'PRESS', 'PRESSING'},\n",
    "        'CURL': {'CURL', 'CURLING'},\n",
    "        'ROW': {'ROW', 'ROWING'},\n",
    "        'RAISE': {'RAISE', 'LATERAL RAISE', 'FRONT RAISE'},\n",
    "        'EXTENSION': {'EXTENSION', 'EXTEND'},\n",
    "        'FLEXION': {'FLEXION', 'FLEX'},\n",
    "        'SQUAT': {'SQUAT', 'SQUATTING'},\n",
    "        'DEADLIFT': {'DEADLIFT', 'DEAD LIFT'},\n",
    "        'PULL': {'PULL', 'PULLDOWN', 'PULL-DOWN'},\n",
    "        'PUSH': {'PUSH', 'PUSHUP', 'PUSH-UP'},\n",
    "        'FLY': {'FLY', 'FLYE'},\n",
    "        'DIP': {'DIP', 'DIPS'},\n",
    "        'LUNGE': {'LUNGE', 'LUNGES'},\n",
    "        'CRUNCH': {'CRUNCH', 'CRUNCHES'},\n",
    "        'PLANK': {'PLANK', 'PLANKS'}\n",
    "    })\n",
    "    \n",
    "    muscle_groups: Dict[str, Set[str]] = field(default_factory=lambda: {\n",
    "        'CHEST': {'CHEST', 'PECTORAL', 'PECS'},\n",
    "        'BACK': {'BACK', 'LATISSIMUS', 'LATS', 'RHOMBOIDS'},\n",
    "        'SHOULDERS': {'SHOULDER', 'SHOULDERS', 'DELTOID', 'DELTS'},\n",
    "        'BICEPS': {'BICEP', 'BICEPS'},\n",
    "        'TRICEPS': {'TRICEP', 'TRICEPS'},\n",
    "        'LEGS': {'LEG', 'LEGS', 'QUADRICEPS', 'QUADS', 'HAMSTRING', 'HAMSTRINGS'},\n",
    "        'GLUTES': {'GLUTE', 'GLUTES', 'GLUTEUS'},\n",
    "        'CALVES': {'CALF', 'CALVES', 'GASTROCNEMIUS'},\n",
    "        'CORE': {'ABS', 'ABDOMINAL', 'CORE', 'OBLIQUES'}\n",
    "    })\n",
    "    \n",
    "    modifiers: Dict[str, Set[str]] = field(default_factory=lambda: {\n",
    "        'INCLINE': {'INCLINE', 'INCLINED'},\n",
    "        'DECLINE': {'DECLINE', 'DECLINED'},\n",
    "        'FLAT': {'FLAT', 'HORIZONTAL'},\n",
    "        'SEATED': {'SEATED', 'SITTING'},\n",
    "        'STANDING': {'STANDING', 'UPRIGHT'},\n",
    "        'LYING': {'LYING', 'SUPINE', 'PRONE'},\n",
    "        'SINGLE_ARM': {'SINGLE ARM', 'ONE ARM', 'UNILATERAL'},\n",
    "        'ALTERNATING': {'ALTERNATING', 'ALTERNATE'},\n",
    "        'HAMMER': {'HAMMER', 'NEUTRAL GRIP'},\n",
    "        'REVERSE': {'REVERSE', 'PRONATED'},\n",
    "        'WIDE': {'WIDE', 'WIDE GRIP'},\n",
    "        'NARROW': {'NARROW', 'CLOSE GRIP', 'NARROW GRIP'},\n",
    "        'OVERHEAD': {'OVERHEAD', 'MILITARY'}\n",
    "    })\n",
    "\n",
    "class SmartExerciseStandardizer:\n",
    "    \"\"\"Advanced exercise standardization using hybrid ML approach\"\"\"\n",
    "    \n",
    "    def __init__(self, config: StandardizationConfig = None):\n",
    "        self.config = config or StandardizationConfig()\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "    def extract_features(self, exercise_name: str) -> ExerciseFeatures:\n",
    "        \"\"\"Extract structured features from exercise name\"\"\"\n",
    "        name_upper = exercise_name.upper()\n",
    "        features = ExerciseFeatures(raw_name=exercise_name)\n",
    "        \n",
    "        # Extract equipment (most specific wins)\n",
    "        equipment_scores = {}\n",
    "        for equip_type, patterns in self.config.equipment_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern in name_upper:\n",
    "                    equipment_scores[equip_type] = len(pattern)  # Longer match = more specific\n",
    "        \n",
    "        if equipment_scores:\n",
    "            best_equipment = max(equipment_scores, key=equipment_scores.get)\n",
    "            features.equipment.add(best_equipment)\n",
    "        \n",
    "        # Extract movement pattern\n",
    "        for movement, patterns in self.config.movement_patterns.items():\n",
    "            if any(pattern in name_upper for pattern in patterns):\n",
    "                features.movement_pattern = movement\n",
    "                break\n",
    "        \n",
    "        # Extract muscle groups\n",
    "        for muscle, patterns in self.config.muscle_groups.items():\n",
    "            if any(pattern in name_upper for pattern in patterns):\n",
    "                features.muscle_groups.add(muscle)\n",
    "        \n",
    "        # Extract modifiers\n",
    "        for modifier, patterns in self.config.modifiers.items():\n",
    "            if any(pattern in name_upper for pattern in patterns):\n",
    "                features.modifiers.add(modifier)\n",
    "        \n",
    "        # Extract angle/position\n",
    "        if any(word in name_upper for word in ['INCLINE', 'DECLINE', 'FLAT']):\n",
    "            if 'INCLINE' in name_upper:\n",
    "                features.angle_position = 'INCLINE'\n",
    "            elif 'DECLINE' in name_upper:\n",
    "                features.angle_position = 'DECLINE'\n",
    "            else:\n",
    "                features.angle_position = 'FLAT'\n",
    "        \n",
    "        # Extract grip/stance\n",
    "        if any(word in name_upper for word in ['WIDE', 'NARROW', 'CLOSE', 'HAMMER']):\n",
    "            if 'WIDE' in name_upper:\n",
    "                features.grip_stance = 'WIDE'\n",
    "            elif any(word in name_upper for word in ['NARROW', 'CLOSE']):\n",
    "                features.grip_stance = 'NARROW'\n",
    "            elif 'HAMMER' in name_upper:\n",
    "                features.grip_stance = 'HAMMER'\n",
    "        \n",
    "        # Clean name\n",
    "        features.cleaned_name = self._clean_exercise_name(exercise_name)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _clean_exercise_name(self, name: str) -> str:\n",
    "        \"\"\"Clean exercise name while preserving important distinctions\"\"\"\n",
    "        # Remove parenthetical information\n",
    "        name = re.sub(r'\\s*\\([^)]*\\)', '', name)\n",
    "        \n",
    "        # Standardize common variations\n",
    "        replacements = {\n",
    "            r'DUMBBELLS?': 'DUMBBELL',\n",
    "            r'BARBELLS?': 'BARBELL',\n",
    "            r'PULL[-\\s]?UPS?': 'PULL UP',\n",
    "            r'PUSH[-\\s]?UPS?': 'PUSH UP',\n",
    "            r'SIT[-\\s]?UPS?': 'SIT UP',\n",
    "            r'BICEPS?': 'BICEP',\n",
    "            r'TRICEPS?': 'TRICEP'\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Clean whitespace\n",
    "        name = re.sub(r'[-_]+', ' ', name)\n",
    "        name = ' '.join(name.split())\n",
    "        \n",
    "        return name.strip()\n",
    "    \n",
    "    def create_feature_vector(self, features: ExerciseFeatures) -> np.ndarray:\n",
    "        \"\"\"Convert structured features to numerical vector\"\"\"\n",
    "        vector = []\n",
    "        \n",
    "        # Equipment features (one-hot encoded)\n",
    "        for equip in ['BARBELL', 'DUMBBELL', 'CABLE', 'MACHINE', 'KETTLEBELL', \n",
    "                     'RESISTANCE_BAND', 'BODYWEIGHT', 'EZ_BAR', 'T_BAR']:\n",
    "            vector.append(1 if equip in features.equipment else 0)\n",
    "        \n",
    "        # Movement pattern features\n",
    "        for movement in ['PRESS', 'CURL', 'ROW', 'RAISE', 'EXTENSION', 'SQUAT', \n",
    "                        'DEADLIFT', 'PULL', 'PUSH', 'FLY', 'DIP', 'LUNGE', 'CRUNCH']:\n",
    "            vector.append(1 if features.movement_pattern == movement else 0)\n",
    "        \n",
    "        # Muscle group features\n",
    "        for muscle in ['CHEST', 'BACK', 'SHOULDERS', 'BICEPS', 'TRICEPS', 'LEGS', 'GLUTES', 'CORE']:\n",
    "            vector.append(1 if muscle in features.muscle_groups else 0)\n",
    "        \n",
    "        # Modifier features\n",
    "        for modifier in ['INCLINE', 'DECLINE', 'SEATED', 'STANDING', 'HAMMER', 'REVERSE', 'WIDE', 'NARROW']:\n",
    "            vector.append(1 if modifier in features.modifiers else 0)\n",
    "        \n",
    "        return np.array(vector)\n",
    "    \n",
    "    def compute_hybrid_similarity(self, exercises: List[str]) -> np.ndarray:\n",
    "        \"\"\"Compute hybrid similarity matrix combining semantic and structural features\"\"\"\n",
    "        # Extract features for all exercises\n",
    "        all_features = [self.extract_features(ex) for ex in exercises]\n",
    "        \n",
    "        # Create semantic embeddings\n",
    "        semantic_embeddings = self.model.encode(exercises)\n",
    "        \n",
    "        # Create feature vectors\n",
    "        feature_vectors = np.array([self.create_feature_vector(f) for f in all_features])\n",
    "        \n",
    "        # Compute similarity matrices\n",
    "        semantic_sim = cosine_similarity(semantic_embeddings)\n",
    "        feature_sim = cosine_similarity(feature_vectors)\n",
    "        \n",
    "        # Combine similarities\n",
    "        hybrid_sim = (self.config.semantic_weight * semantic_sim + \n",
    "                     self.config.feature_weight * feature_sim)\n",
    "        \n",
    "        return hybrid_sim\n",
    "    \n",
    "    def smart_clustering(self, exercises: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Perform intelligent clustering using hybrid similarity\"\"\"\n",
    "        print(f\"Clustering {len(exercises)} exercises...\")\n",
    "        \n",
    "        # Compute hybrid similarity\n",
    "        similarity_matrix = self.compute_hybrid_similarity(exercises)\n",
    "        \n",
    "        # Convert similarity to distance\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "        \n",
    "        # Use Agglomerative Clustering for better control\n",
    "        n_clusters = None\n",
    "        linkage = 'average'\n",
    "        \n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters,\n",
    "            distance_threshold=1 - self.config.similarity_threshold,\n",
    "            linkage=linkage,\n",
    "            metric='precomputed'\n",
    "        )\n",
    "        \n",
    "        labels = clustering.fit_predict(distance_matrix)\n",
    "        \n",
    "        # Group exercises by cluster\n",
    "        clusters = defaultdict(list)\n",
    "        for i, label in enumerate(labels):\n",
    "            clusters[label].append(exercises[i])\n",
    "        \n",
    "        print(f\"Found {len(clusters)} clusters\")\n",
    "        return dict(clusters)\n",
    "    \n",
    "    def select_canonical_name(self, exercise_group: List[str]) -> str:\n",
    "        \"\"\"Select the most representative name from a group\"\"\"\n",
    "        if len(exercise_group) == 1:\n",
    "            return self._clean_exercise_name(exercise_group[0])\n",
    "        \n",
    "        # Extract features for all exercises in group\n",
    "        features_list = [self.extract_features(ex) for ex in exercise_group]\n",
    "        \n",
    "        # Score each exercise based on completeness and clarity\n",
    "        scores = []\n",
    "        for i, (exercise, features) in enumerate(zip(exercise_group, features_list)):\n",
    "            score = 0\n",
    "            \n",
    "            # Prefer exercises with equipment specified\n",
    "            if features.equipment:\n",
    "                score += 2\n",
    "            \n",
    "            # Prefer exercises with clear movement pattern\n",
    "            if features.movement_pattern:\n",
    "                score += 2\n",
    "            \n",
    "            # Prefer exercises with modifiers (more specific)\n",
    "            score += len(features.modifiers)\n",
    "            \n",
    "            # Prefer shorter names (less likely to have extra info)\n",
    "            score += max(0, 10 - len(exercise.split()))\n",
    "            \n",
    "            # Prefer common naming patterns\n",
    "            if any(pattern in exercise.upper() for pattern in ['BARBELL', 'DUMBBELL']):\n",
    "                score += 1\n",
    "            \n",
    "            scores.append((score, exercise))\n",
    "        \n",
    "        # Select highest scoring exercise\n",
    "        best_exercise = max(scores, key=lambda x: x[0])[1]\n",
    "        return self._clean_exercise_name(best_exercise)\n",
    "    \n",
    "    def validate_cluster_quality(self, cluster: List[str]) -> bool:\n",
    "        \"\"\"Validate if exercises in cluster should really be grouped\"\"\"\n",
    "        if len(cluster) <= 1:\n",
    "            return True\n",
    "        \n",
    "        features_list = [self.extract_features(ex) for ex in cluster]\n",
    "        \n",
    "        # Check if all exercises have same core characteristics\n",
    "        equipments = [f.equipment for f in features_list if f.equipment]\n",
    "        movements = [f.movement_pattern for f in features_list if f.movement_pattern]\n",
    "        \n",
    "        # If equipment differs significantly, likely different exercises\n",
    "        if len(set().union(*equipments)) > 1 and len(equipments) > 1:\n",
    "            return False\n",
    "        \n",
    "        # If movement patterns differ, likely different exercises\n",
    "        if len(set(movements)) > 1:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def standardize_exercises(self, exercises: List[str]) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "        \"\"\"Main standardization pipeline\"\"\"\n",
    "        print(\"Starting exercise standardization...\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_exercises = list(dict.fromkeys(exercises))\n",
    "        print(f\"Processing {len(unique_exercises)} unique exercises...\")\n",
    "        \n",
    "        # Perform smart clustering\n",
    "        clusters = self.smart_clustering(unique_exercises)\n",
    "        \n",
    "        # Create mapping and validate clusters\n",
    "        mapping = {}\n",
    "        validated_clusters = []\n",
    "        \n",
    "        for cluster_id, cluster_exercises in clusters.items():\n",
    "            if self.validate_cluster_quality(cluster_exercises):\n",
    "                canonical_name = self.select_canonical_name(cluster_exercises)\n",
    "                for exercise in cluster_exercises:\n",
    "                    mapping[exercise] = canonical_name\n",
    "                validated_clusters.append({\n",
    "                    'canonical_name': canonical_name,\n",
    "                    'variations': cluster_exercises,\n",
    "                    'count': len(cluster_exercises)\n",
    "                })\n",
    "            else:\n",
    "                # Split invalid clusters\n",
    "                for exercise in cluster_exercises:\n",
    "                    mapping[exercise] = self._clean_exercise_name(exercise)\n",
    "                    validated_clusters.append({\n",
    "                        'canonical_name': self._clean_exercise_name(exercise),\n",
    "                        'variations': [exercise],\n",
    "                        'count': 1\n",
    "                    })\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_df = pd.DataFrame(validated_clusters)\n",
    "        summary_df = summary_df.sort_values('count', ascending=False)\n",
    "        \n",
    "        # Print results\n",
    "        original_count = len(unique_exercises)\n",
    "        standardized_count = len(set(mapping.values()))\n",
    "        reduction = (1 - standardized_count / original_count) * 100\n",
    "        \n",
    "        print(f\"\\nStandardization Results:\")\n",
    "        print(f\"Original exercises: {original_count}\")\n",
    "        print(f\"Standardized exercises: {standardized_count}\")\n",
    "        print(f\"Reduction: {reduction:.1f}%\")\n",
    "        print(f\"Largest groups: {summary_df.head(3)['canonical_name'].tolist()}\")\n",
    "        \n",
    "        return mapping, summary_df\n",
    "\n",
    "# Usage example\n",
    "def apply_standardization(df, exercise_column='name'):\n",
    "    \"\"\"Apply standardization to DataFrame using serverless-compatible operations\"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import broadcast\n",
    "    \n",
    "    standardizer = SmartExerciseStandardizer()\n",
    "    \n",
    "    # Get unique exercise names using DataFrame operations (serverless compatible)\n",
    "    unique_exercises_df = df.select(exercise_column).distinct().filter(f\"{exercise_column} IS NOT NULL\")\n",
    "    unique_exercises = [row[exercise_column] for row in unique_exercises_df.collect()]\n",
    "    \n",
    "    print(f\"Collected {len(unique_exercises)} unique exercises for standardization\")\n",
    "    \n",
    "    # Standardize\n",
    "    mapping, summary = standardizer.standardize_exercises(unique_exercises)\n",
    "    \n",
    "    # Create mapping DataFrame for join\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    mapping_data = [(k, v) for k, v in mapping.items()]\n",
    "    mapping_df = spark.createDataFrame(mapping_data, [exercise_column, \"standardized_name\"])\n",
    "    \n",
    "    # Use broadcast join for efficiency (mapping table is small)\n",
    "    result_df = df.join(broadcast(mapping_df), on=exercise_column, how=\"left\")\n",
    "    \n",
    "    return result_df, mapping, summary\n",
    "\n",
    "# Alternative approach using pandas UDF if you need to process in batches\n",
    "def apply_standardization_batched(df, exercise_column='name', batch_size=1000):\n",
    "    \"\"\"Apply standardization using pandas UDFs for large datasets\"\"\"\n",
    "    from pyspark.sql.functions import pandas_udf, col\n",
    "    from pyspark.sql.types import StringType\n",
    "    import pandas as pd\n",
    "    \n",
    "    # First, get all unique exercises and create mapping\n",
    "    unique_exercises_df = df.select(exercise_column).distinct().filter(f\"{exercise_column} IS NOT NULL\")\n",
    "    unique_exercises = [row[exercise_column] for row in unique_exercises_df.collect()]\n",
    "    \n",
    "    standardizer = SmartExerciseStandardizer()\n",
    "    mapping, summary = standardizer.standardize_exercises(unique_exercises)\n",
    "    \n",
    "    # Create pandas UDF for standardization\n",
    "    @pandas_udf(StringType())\n",
    "    def standardize_exercise_udf(exercise_series: pd.Series) -> pd.Series:\n",
    "        return exercise_series.map(mapping).fillna(exercise_series)\n",
    "    \n",
    "    # Apply standardization\n",
    "    result_df = df.withColumn(\"standardized_name\", standardize_exercise_udf(col(exercise_column)))\n",
    "    \n",
    "    return result_df, mapping, summary\n",
    "\n",
    "# Example usage:\n",
    "standardized_df, exercise_mapping, summary_report = apply_standardization(exercise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6e88f4-28c9-447e-8d2b-c83478bf3c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(standardized_df.select('name', 'standardized_name').sort('standardized_name'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wger_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
