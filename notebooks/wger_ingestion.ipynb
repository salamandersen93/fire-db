{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04ffa21-dccf-4cea-ae1c-f34fa5c13e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from langdetect import detect\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import squareform\n",
    "from typing import List, Dict, Tuple, Set, Union\n",
    "import pandas as pd\n",
    "import ast\n",
    "from json_repair import repair_json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize, punkt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk \n",
    "\n",
    "def get_exercises(api_url):\n",
    "    \"\"\"Retrieves complete list of exercises from the wger public API\"\"\"\n",
    "    all_exercises = []\n",
    "    current_url = api_url\n",
    "\n",
    "    try:\n",
    "        while current_url:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            all_exercises.extend(data[\"results\"])\n",
    "            current_url = data.get(\"next\")\n",
    "        return all_exercises\n",
    "    except Exception as e:\n",
    "        print(\"error fetching exercise data from wger API:\", e)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def is_english_content(description):\n",
    "    \"\"\"check if description is actually in English\"\"\"\n",
    "    try:\n",
    "        # check description language, sampling first 100 chars \n",
    "        if description and len(description.strip()) > 0:\n",
    "            desc_sample = description[:100]\n",
    "            desc_lang = detect(desc_sample)\n",
    "            if desc_lang != \"en\":\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # if detection fails, assume not english\n",
    "        return False\n",
    "    \n",
    "def normalize_record(record):\n",
    "    \"\"\"Normalize a record dictionary by extracting and formatting specific fields.\"\"\"\n",
    "    # extract name and description from translations with language == 2 (english)\n",
    "    translations = record.get(\"translations\", [])\n",
    "    name = None\n",
    "    description = None\n",
    "    \n",
    "    for translation in translations:\n",
    "        if translation.get(\"language\") == 2:\n",
    "            candidate_name = translation.get(\"name\")\n",
    "            candidate_description = translation.get(\"description\")\n",
    "            \n",
    "            # verify the content is english\n",
    "            if is_english_content(candidate_description):\n",
    "                name = candidate_name\n",
    "                description = candidate_description\n",
    "                break\n",
    "    \n",
    "    # return None if no English content found\n",
    "    # this is a data quality fix;\n",
    "    # there is a known issue in wger API that results in non-english records despite language = 2 filtering\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    # data cleaning step: remove HTML tags from name and description\n",
    "    if name:\n",
    "        name = re.sub(r\"<[^>]*>\", \"\", name).strip()\n",
    "    if description:\n",
    "        description = re.sub(r\"<[^>]*>\", \"\", description).strip()\n",
    "    \n",
    "    # parse datetime fields with proper error handling\n",
    "    created = None\n",
    "    if record.get(\"created\"):\n",
    "        created = datetime.fromisoformat(record[\"created\"].replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "    last_update = None\n",
    "    if record.get(\"last_update\"):\n",
    "        last_update = datetime.fromisoformat(\n",
    "            record[\"last_update\"].replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "    # generate normalized dict structure\n",
    "    normalized = {\n",
    "        \"id\": record.get(\"id\"),\n",
    "        \"uuid\": record.get(\"uuid\"),\n",
    "        \"name\": name.upper(),\n",
    "        \"description\": description,\n",
    "        \"created\": created,\n",
    "        \"last_update\": last_update,\n",
    "        \"category\": record.get(\"category\")[\"name\"],\n",
    "        \"muscles\": [muscle[\"name\"] for muscle in record.get(\"muscles\", [])],\n",
    "        \"muscles_secondary\": [muscle[\"name\"] for muscle in record.get(\"muscles_secondary\", [])],\n",
    "        \"equipment\": [equip[\"name\"] for equip in record.get(\"equipment\", [])],\n",
    "        \"variations\": record.get(\"variations\") or [],\n",
    "        \"license_author\": record.get(\"license_author\")}\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def query_databricks_foundation_model(prompt, model_name=\"databricks-llama-4-maverick\", max_tokens=512, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Query a Databricks foundation model through the serving endpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    # construct the API endpoint URL\n",
    "    api_url = f\"https://{DATABRICKS_INSTANCE}/serving-endpoints/{model_name}/invocations\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # chat payload\n",
    "    payload_messages = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # prompt payload\n",
    "    payload_prompt = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # input payload\n",
    "    payload_input = {\n",
    "        \"input\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    # attempt each format above\n",
    "    for payload_name, payload in [(\"messages\", payload_messages), (\"prompt\", payload_prompt), (\"input\", payload_input)]:\n",
    "        try:\n",
    "            print(f\"Trying payload format: {payload_name}\")\n",
    "            response = requests.post(api_url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Success with {payload_name} format!\")\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed with {payload_name} format: {e}\")\n",
    "            if hasattr(e, \"response\") and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"all payload formats failed\")\n",
    "    return None\n",
    "\n",
    "# Alternative approach using the newer Databricks SDK\n",
    "def query_with_databricks_sdk(prompt, model_name=\"databricks-llama-4-maverick\", max_tokens=5000, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Alternative approach using Databricks SDK (if available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        \n",
    "        w = WorkspaceClient()\n",
    "        \n",
    "        # Format the request properly for the SDK\n",
    "        request_data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        response = w.serving_endpoints.query(\n",
    "            name=model_name,\n",
    "            **request_data\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except ImportError:\n",
    "        print(\"Databricks SDK not available, use the API approach instead\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error with SDK approach: {e}\")\n",
    "        return None\n",
    "\n",
    "def format_prompt(instruction, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Format prompt similar to your original template\n",
    "    Based on documentation here:\n",
    "    https://github.com/databricks/databricks-ml-examples/blob/master/llm-models/mistral/mistral-7b/01_load_inference.py\n",
    "    \"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"You are a JSON-only exercise standardization assistant. \n",
    "        ALWAYS respond with valid JSON in this exact format:\n",
    "        {\"groups\":[{\"standardized_name\":\"NAME\",\"variations\":[\"var1\",\"var2\"]}]}\n",
    "        Never include explanations, markdown, misspellings, or extra text.\"\"\"\n",
    "    \n",
    "    formatted_prompt = f\"\"\"[INST]<>{system_prompt}<>{instruction}[/INST]\"\"\"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "# Example usage\n",
    "def gen_text_databricks(prompts, use_template=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate text using Databricks foundation model\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        if use_template:\n",
    "            formatted_prompt = format_prompt(prompt)\n",
    "        else:\n",
    "            formatted_prompt = prompt\n",
    "            \n",
    "        # First try the API approach\n",
    "        response = query_databricks_foundation_model(\n",
    "            formatted_prompt, \n",
    "            max_tokens=kwargs.get(\"max_new_tokens\", 5000),\n",
    "            temperature=kwargs.get(\"temperature\", 0.5)\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            # Handle different response formats\n",
    "            if \"choices\" in response and response[\"choices\"]:\n",
    "                # OpenAI-style response\n",
    "                content = response[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\") or response[\"choices\"][0].get(\"text\", \"\")\n",
    "                results.append(content)\n",
    "            elif \"predictions\" in response:\n",
    "                # MLflow-style response\n",
    "                results.append(response[\"predictions\"][0].get(\"generated_text\", \"\"))\n",
    "            elif \"candidates\" in response:\n",
    "                # Gemini-style response\n",
    "                results.append(response[\"candidates\"][0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"\"))\n",
    "            else:\n",
    "                # Try to extract any text from the response\n",
    "                results.append(str(response))\n",
    "        else:\n",
    "            # Try SDK approach as fallback\n",
    "            sdk_response = query_with_databricks_sdk(\n",
    "                formatted_prompt,\n",
    "                max_tokens=kwargs.get(\"max_new_tokens\", 5000),\n",
    "                temperature=kwargs.get(\"temperature\", 0.5)\n",
    "            )\n",
    "            if sdk_response:\n",
    "                results.append(str(sdk_response))\n",
    "            else:\n",
    "                results.append(\"Error: Could not get response from model\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "DATABRICKS_INSTANCE = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "# documentation link: https://exercise.hellogym.io/nl/software/api\n",
    "# the exerciseinfo contains denormalized data with nested\n",
    "wger_api_url = \"https://wger.de/api/v2/exerciseinfo/?status=2&language=2\"\n",
    "exercises = get_exercises(wger_api_url)\n",
    "\n",
    "ex_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"uuid\", StringType()), \n",
    "    StructField(\"name\", StringType()), \n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"created\", TimestampType()),\n",
    "    StructField(\"last_update\", TimestampType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"muscles\",  StringType()),\n",
    "    StructField(\"muscles_secondary\",  StringType()),\n",
    "    StructField(\"equipment\",  StringType()),\n",
    "    StructField(\"variations\", StringType()),\n",
    "    StructField(\"license_author\", StringType())])\n",
    "    \n",
    "# filter out None values for data quality\n",
    "exercise_abbv = [normalize_record(record) for record in exercises]\n",
    "exercise_abbv = [record for record in exercise_abbv if record is not None]  # Remove non-English records\n",
    "\n",
    "try:\n",
    "    exercise_df = spark.createDataFrame(exercise_abbv, schema=ex_schema)\n",
    "    exercise_df = exercise_df.drop(\"uuid\", \"created\", \"last_update\", \"license_author\")\n",
    "    display(exercise_df.sort(\"name\"))\n",
    "except Exception as e:\n",
    "    print(\"error creating dataframe:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6e88f4-28c9-447e-8d2b-c83478bf3c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# The approach below leverages clustering prior for scoping of exercises into pre-defined areas prior to LLM prompting.\n",
    "# This is to reduce ambiguity and provide a tightened group of similar exercise names to improve chances \n",
    "#   of converging on the correct atomic standardized naming convention.\n",
    "# The pre-clustering approach reduces tokens and cost by minimizing the number of tokens sent to the LLM.\n",
    "# Additionally, the code below integrates subject matter knowledge related to equipment, movement patterns,\n",
    "#   and muscle groups to improve the clustering and provide context to the LLM\n",
    "#Smaller, semantically meaningful groups make it easier to manually validate or improve LLM results if needed.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS = {'WITH', 'AND', 'ON', 'INTO', 'IN', 'AT', 'BY', 'OF', 'FOR', 'TO',\n",
    "    'FROM', 'USING', 'THE', 'A', 'AN', 'ONTO'}\n",
    "\n",
    "def validate_json_structure(result):\n",
    "    \"\"\"Validate JSON structure by checking for required keys and data structures/types\"\"\"\n",
    "    if not isinstance(result, dict):\n",
    "        raise ValueError(\"Result is not a dictionary\")\n",
    "    \n",
    "    if \"groups\" not in result:\n",
    "        raise ValueError(\"Missing 'groups' key\")\n",
    "    \n",
    "    if not isinstance(result[\"groups\"], list):\n",
    "        raise ValueError(\"'groups' is not a list\")\n",
    "    \n",
    "    for i, group in enumerate(result[\"groups\"]):\n",
    "        if not isinstance(group, dict):\n",
    "            raise ValueError(f\"Group {i} is not a dictionary\")\n",
    "        \n",
    "        # Check for standardized name with multiple possible keys\n",
    "        if not any(key in group for key in [\"standardized_name\", \"standard_name\"]):\n",
    "            raise ValueError(f\"Group {i} missing standardized name key\")\n",
    "        \n",
    "        if \"variations\" not in group:\n",
    "            raise ValueError(f\"Group {i} missing 'variations' key\")\n",
    "        \n",
    "        if not isinstance(group[\"variations\"], list):\n",
    "            raise ValueError(f\"Group {i} 'variations' is not a list\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def escalate_prompt(base_prompt, level, feedback=None):\n",
    "    \"\"\"Logic to modify prompts that fail validation logic.\n",
    "    Response augmentation varies by failure type, attempt number, and feedback.\"\"\"\n",
    "    escalated = base_prompt\n",
    "    if feedback:\n",
    "        escalated += f\"\\nNOTE: Your previous output failed validation because: {feedback}\"\n",
    "        escalated += \"\\nCarefully reprocess the input and correct this mistake.\"\n",
    "    if level == 1:\n",
    "        escalated += \"\\nSTRICTLY enforce JSON compliance. Only output a parsable JSON object.\"\n",
    "    elif level == 2:\n",
    "        escalated += \"\\nYour previous response was invalid. Ensure the response is a clean JSON object with no extra formatting or text.\"\n",
    "    elif level >= 3:\n",
    "        escalated += \"\\nFinal warning: Return ONLY a valid JSON object. No natural language, no markdown, no surrounding explanations.\"\n",
    "    return escalated\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Uses nltk wordnet to convert nltk pos tags to wordnet pos tags\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "class AdvancedExerciseStandardizer:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize with a sentence transformer model optimized for semantic similarity\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def preprocess_exercise_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess exercise name with lemmatization and stopword removal.\n",
    "        Fulfills requirement to eliminate inconsistencies, standardize format, and reduce noise prior to clustering.\n",
    "        \"\"\"\n",
    "        name = name.upper()\n",
    "        name = re.sub(r'[-_/\\\\|]+', ' ', name)\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        tokens = word_tokenize(name.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        lemmatized = [\n",
    "            lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "            for word, pos in tagged\n",
    "            if word.upper() not in STOPWORDS]\n",
    "\n",
    "        return ' '.join([t.upper() for t in lemmatized])\n",
    "\n",
    "    def extract_exercise_features(self, exercise_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract key features that distinguish exercises.\n",
    "        The purpose of this function is to provide context to the clustering algorithm by \n",
    "            capturing domain knowledge and facilitating more accurate clustering.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        name_upper = exercise_name.upper()\n",
    "        \n",
    "        # Find equipment in the name\n",
    "        equipment_found = set()\n",
    "        equipment_list = [\n",
    "            \"BARBELL\", \"DUMBBELL\", \"KETTLEBELL\", \n",
    "            \"EZ BAR\", \"EZ-BAR\", \"EZBAR\", \"T-BAR\", \"T BAR\", \"HEX\", \"HEX BAR\",\n",
    "            \"CABLE\",  \"ROPE\", \"V-BAR\",\n",
    "            \"HANDLE\", \"SINGLE HANDLE\", \"SMITH\",\n",
    "            \"PREACHER\", \"HACK\",\"TRX\", \"BAG\",\n",
    "            \"BAND\", \"MINI BAND\", \"THERABAND\",\n",
    "            \"BOX\", \"BALL\"]\n",
    "        \n",
    "        for equip in equipment_list:\n",
    "            if equip in name_upper:\n",
    "                equipment_found.add(equip)\n",
    "\n",
    "        # Find movement patterns\n",
    "        movement_found = set()\n",
    "        movement_list = [\n",
    "            \"SQUAT\", \"DEADLIFT\", \"LUNGE\", \"STEP-UP\",\n",
    "            \"PUSH\", \"PRESS\", \"ROW\",\n",
    "            \"DIP\", \"CURL\", \"EXTENSION\", \"RAISE\", \"FLY\", \"FLYE\",\n",
    "            \"PULLOVER\", \"SHRUG\",\n",
    "            \"CRUNCH\", \"PLANK\", \"HOLD\", \"BRIDGE\",  \"BEND\",\n",
    "            \"SPRINT\", \"KICKBACK\",\n",
    "            \"THRUST\", \"TAP\", \"THROWS\",\n",
    "            \"CLIMB\", \"CARRY\", \"WALK\", \"THROW\"]\n",
    "        \n",
    "        for movement in movement_list:\n",
    "            if movement in name_upper:\n",
    "                movement_found.add(movement)\n",
    "        \n",
    "        # Find body parts\n",
    "        body_parts_found = set()\n",
    "        body_parts_list = [\n",
    "            \"NECK\", \"TRAP\", \"SHOULDER\", \"DELTOID\", \"DELT\",\n",
    "            \"BICEP\", \"BI\", \"TRICEP\", \"TRI\",\n",
    "            \"FOREARM\", \"CHEST\", \"PECTORAL\", \"PEC\",\n",
    "            \"BACK\", \"LAT\", \"UPPER BACK\", \"LOWER BACK\",\n",
    "            \"ABS\", \"ABDOMINALS\", \"AB\", \"OBLIQUE\", \"CORE\",\n",
    "            \"GLUTE\", \"BUTT\", \"HIP\",  \"HIP FLEXOR\",\n",
    "            \"ADDUCTOR\", \"ABDUCTOR\", \"QUAD\", \"QUADRICEP\", \"HAMSTRING\", \"HAM\",\n",
    "            \"CALF\", \"LEG\", \"ARM\", \"CHIN\"]\n",
    "        \n",
    "        for body_part in body_parts_list:\n",
    "            if body_part in name_upper:\n",
    "                body_parts_found.add(body_part)\n",
    "\n",
    "        # Find modifiers\n",
    "        modifiers_found = set()\n",
    "        modifiers_list = [\n",
    "            \"WIDE\", \"WIDE-GRIP\", \"NARROW\", \"NARROW-GRIP\", \"CLOSE\", \"CLOSE-GRIP\", \"CROSSBODY\", \"CROSS-BODY\",\n",
    "            \"HAMMER\", \"NEUTRAL-GRIP\", \"OVERHAND\", \"UNDERHAND\", \"SUPINATED\", \"PRONATED\", \n",
    "            \"REVERSE\", \"MIXED-GRIP\", \"SINGLE-ARM\", \"ONE-ARM\", \"ONE-ARMED\",\"SINGLE-LEG\", \"ONE-LEG\", \"ONE-LEGGED\",\n",
    "            \"UNILATERAL\", \"ALTERNATING\", \"ALTERNATE\",\"ISOLATED\", \"ISOLATION\",\"CROSSOVER\", \"CROSS-BODY\",\"BILATERAL\", \n",
    "            \"DOUBLE-ARM\", \"DOUBLE-LEG\", \"SUMO\", \"WIDE-STANCE\", \"NARROW-STANCE\", \"CLOSE-STANCE\", \"SPLIT\", \"BULGARIAN\",\n",
    "            \"STAGGERED\",\"STEP-BACK\", \"STEP-UP\",\"LATERAL\", \"SIDE\", \"SIDE-STEP\",\"CROSSOVER\", \"REVERSE\", \"INCLINE\", \n",
    "            \"DECLINE\", \"FLAT\",\"VERTICAL\", \"HORIZONTAL\",\"DIAGONAL\", \"ROTATIONAL\", \"TWISTING\", \"STANDING\", \"SEATED\", \n",
    "            \"SITTING\", \"LYING\", \"PRONE\", \"SUPINE\", \"SIDE-LYING\", \"KNEELING\", \"HALF-KNEELING\", \"LEANING\", \n",
    "            \"FORWARD-LEANING\", \"PARTIAL\", \"FULL\", \"HALF\", \"HOLD\", \"ISOMETRIC\", \"ISO\", \"STATIC-HOLD\",\"LATERAL\"]\n",
    "        \n",
    "        for modifier in modifiers_list:\n",
    "            if modifier in name_upper:\n",
    "                modifiers_found.add(modifier)\n",
    "        \n",
    "        return {\n",
    "            \"equipment\": equipment_found,\n",
    "            \"movement\": movement_found,\n",
    "            \"body_parts\": body_parts_found,\n",
    "            \"modifiers\": modifiers_found,\n",
    "            \"raw_name\": exercise_name}\n",
    "\n",
    "    def create_feature_aware_embeddings(self, exercise_names: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embeddings that emphasize critical distinguishing features (based on equipment,\n",
    "            movement pattern, body part, and key modifiers). The function adds weights by \n",
    "            importance for the clustering algorithm, emphasizing equipment and movement patterns.\n",
    "            For example, Barbell Press and Barbell Bench Press are more similar than Barbell Press \n",
    "            and Barbell Curl. Similarly, Wide Grip Pull Up and Wide Grip Pullup are more similar than \n",
    "            Wide Grip Pull Up and Narrow Grip Pull Up. This function is used to create embeddings for the exercise names.\n",
    "        \"\"\"\n",
    "        enhanced_texts = []\n",
    "        \n",
    "        for name in exercise_names:\n",
    "            # First, preprocess the name (lemmatize, remove stopwords)\n",
    "            processed_name = self.preprocess_exercise_name(name)\n",
    "            features = self.extract_exercise_features(name)\n",
    "            \n",
    "            # Create enhanced text that emphasizes critical features\n",
    "            enhanced_parts = [processed_name]  # Use processed name\n",
    "            \n",
    "            # Equipment gets highest weight (most distinguishing) - 4x\n",
    "            for equip in features[\"equipment\"]:\n",
    "                enhanced_parts.extend([equip] * 4)\n",
    "            \n",
    "            # Movement gets medium weight - 3x  \n",
    "            for movement in features[\"movement\"]:\n",
    "                enhanced_parts.extend([movement] * 3)\n",
    "            \n",
    "            # Modifiers get medium weight - 2x\n",
    "            for modifier in features[\"modifiers\"]:\n",
    "                enhanced_parts.extend([modifier] * 2)\n",
    "            \n",
    "            # Body parts get lower weight - 1x\n",
    "            for body_part in features[\"body_parts\"]:\n",
    "                enhanced_parts.append(body_part)\n",
    "            \n",
    "            enhanced_texts.append(\" \".join(enhanced_parts))\n",
    "        \n",
    "        return self.model.encode(enhanced_texts)\n",
    "\n",
    "    def feature_based_pre_clustering(self, exercise_names: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Pre-cluster exercises by equipment type to create more balanced groups. \n",
    "        \"\"\"\n",
    "        equipment_groups = {}\n",
    "        \n",
    "        for name in exercise_names:\n",
    "            features = self.extract_exercise_features(name)\n",
    "            \n",
    "            # Create equipment signature (primary equipment)\n",
    "            if features[\"equipment\"]:\n",
    "                # Prioritize more specific equipment\n",
    "                equipment_priority = [\n",
    "                    \"BARBELL\", \"DUMBBELL\", \"KETTLEBELL\", \"EZ BAR\", \"EZ-BAR\", \"EZBAR\",\n",
    "                    \"T-BAR\", \"T BAR\", \"HEX BAR\", \"CABLE\", \"MACHINE\", \"SMITH MACHINE\"]\n",
    "                \n",
    "                primary_equipment = None\n",
    "                for equip in equipment_priority:\n",
    "                    if equip in features[\"equipment\"]:\n",
    "                        primary_equipment = equip\n",
    "                        break\n",
    "                \n",
    "                if not primary_equipment:\n",
    "                    primary_equipment = next(iter(features[\"equipment\"]))\n",
    "            else:\n",
    "                primary_equipment = \"BODYWEIGHT\"\n",
    "            \n",
    "            # Add movement pattern for finer grouping\n",
    "            if features[\"movement\"]:\n",
    "                primary_movement = next(iter(features[\"movement\"]))\n",
    "                group_key = f\"{primary_equipment}_{primary_movement}\"\n",
    "            else:\n",
    "                group_key = primary_equipment\n",
    "            \n",
    "            if group_key not in equipment_groups:\n",
    "                equipment_groups[group_key] = []\n",
    "            equipment_groups[group_key].append(name)\n",
    "        \n",
    "        return equipment_groups\n",
    "\n",
    "    def hdbscan_clustering_within_group(self, exercise_names: List[str], \n",
    "                                        min_cluster_size: int = 2, min_samples: int = None) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Perform clustering using HDBSCAN with cosine distance.\n",
    "        Automatically determines number of clusters and filters noise.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(exercise_names) <= min_cluster_size:\n",
    "            return [exercise_names]\n",
    "\n",
    "        # Create normalized embeddings\n",
    "        embeddings = self.create_feature_aware_embeddings(exercise_names)\n",
    "        normalized_embeddings = normalize(embeddings, norm='l2')\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            metric='euclidean',  \n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples or min_cluster_size,\n",
    "            cluster_selection_method='eom'\n",
    "        )\n",
    "        labels = clusterer.fit_predict(normalized_embeddings)\n",
    "\n",
    "        # Group items by cluster label, skip noise (-1)\n",
    "        label_to_names = defaultdict(list)\n",
    "        for name, label in zip(exercise_names, labels):\n",
    "            if label != -1:\n",
    "                label_to_names[label].append(name)\n",
    "\n",
    "        return list(label_to_names.values())\n",
    "\n",
    "    def get_base_prompt(self, exercise_names):\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a highly accurate exercise classification system.\n",
    "\n",
    "        Your task is to analyze a list of exercise names and group all equivalent variations under a single standardized name, following strict disambiguation rules. If two exercises differ in function, form, or equipment, they must remain separate.\n",
    "\n",
    "        OBJECTIVE:\n",
    "        - Group only truly equivalent exercises together by movement pattern, equipment, grip, and body position.\n",
    "        - Assign each group a clean, atomic, and standardized name.\n",
    "        - List all original input names that belong to each group.\n",
    "\n",
    "        DISAMBIGUATION RULES (strictly enforce):\n",
    "        1. Different equipment = DIFFERENT exercises (e.g., Barbell ≠ Dumbbell ≠ EZ Bar).\n",
    "        2. Different grips or hand positions (e.g., Wide, Narrow, Neutral, Close, Reverse) = DIFFERENT.\n",
    "        3. Different body positions or angles (e.g., Incline, Decline, Overhead, Seated, Standing, Lying) = DIFFERENT.\n",
    "        4. Different movement patterns = DIFFERENT (e.g., “Clean” ≠ “Clean and Jerk”).\n",
    "        5. Differences in spelling, punctuation, plurality, or casing = SAME.\n",
    "        6. Left/right or unilateral/bilateral indicators = SAME.\n",
    "        7. DO NOT use hyphens in standardized names (e.g., use \"Pull Up\", not \"Pull-Up\").\n",
    "        8. NEVER use acronyms or abbreviations (e.g., DB, BB, OHP) in standardized names.\n",
    "        9. NEVER use commas or parentheses in standardized names.\n",
    "        10. Expand abbreviations in variations, but use full terms in standardized names.\n",
    "        11. Do NOT generalize specific names into broader categories (e.g., “Bird Dog” ≠ “Arabesque”).\n",
    "        12. Do NOT drop disambiguating words (e.g., “Close-Grip Lat Pull Down” ≠ “Lat Pull Down”; “Reverse Preacher Curl” ≠ “Preacher Curl”).\n",
    "        13. IF equipment name is included, ALWAYS included it at the beginning of the name (e.g., use \"Barbell Curl\", NOT \"Curl - Barbell\")\n",
    "        14. NEVER add an equipment name to an exercise without one. For example, NEVER standardize \"Pullover\" to \"Dumbbell Pullover\".\n",
    "\n",
    "        ADDITIONAL RULES:\n",
    "        - When unsure, **keep exercises separate**. It is better to under-group than over-group.\n",
    "        - Variations with minor typos should be grouped (e.g., “Low Pulley Cable Ffly” = “Low Pulley Cable Fly”).\n",
    "\n",
    "        OUTPUT RULES:\n",
    "        - Output must be valid **JSON only**. No extra text.\n",
    "        - Each standardized name must be unique, readable, and not overly generic.\n",
    "        - Do not merge distinct exercises into one name.\n",
    "        - Ensure casing and spacing are consistent (Title Case).\n",
    "        - Avoid merged tokens (e.g., no “BENTOVERROWTOEXTERNALROTATION”).\n",
    "        - ALWAYS adhere to the disambiguation rules.\n",
    "\n",
    "        INPUT:\n",
    "        Exercise names: {exercise_names}\n",
    "\n",
    "        OUTPUT FORMAT:\n",
    "        {{\n",
    "        \"groups\": [\n",
    "            {{\n",
    "            \"standardized_name\": \"STANDARDIZED EXERCISE NAME\",\n",
    "            \"variations\": [\"Original Name 1\", \"Original Name 2\", ...]\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "        }}\n",
    "\n",
    "        EXAMPLE:\n",
    "        {{\n",
    "        \"groups\": [\n",
    "            {{\n",
    "            \"standardized_name\": \"BARBELL BICEP CURL\",\n",
    "            \"variations\": [\"Barbell Biceps Curl\", \"Bar Bell Bicep Curls\"]\n",
    "            }},\n",
    "            {{\n",
    "            \"standardized_name\": \"EZ BAR BICEP CURL\",\n",
    "            \"variations\": [\"EZ Bar Biceps Curl\", \"Ez-Bar Bicep Curl\"]\n",
    "            }}\n",
    "        ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def enforce_disambiguation_rules(self, parsed_json) -> Union[bool, str]:\n",
    "        \"\"\"Validate that the parsed JSON adheres to disambiguation rules.\n",
    "        For example, check that different equipment or movements are not referenced in the matched group.\n",
    "        Specifically, Dumbbell Bench Press and Barbell Bench Press should not be grouped.\n",
    "        The result of a rule violation is early termination of the LLM cluster validation and\n",
    "            default to the individual exercise name.\"\"\"\n",
    "        for group in parsed_json[\"groups\"]:\n",
    "            variations = group[\"variations\"]\n",
    "            standard_name = group.get(\"standardized_name\") or group.get(\"standard_name\")\n",
    "\n",
    "            # Collect feature sets\n",
    "            equipment_signatures = []\n",
    "            grip_signatures = []\n",
    "            position_signatures = []\n",
    "            movement_signatures = []\n",
    "\n",
    "            for v in variations:\n",
    "                features = self.extract_exercise_features(v)\n",
    "                equipment_signatures.extend(features.get(\"equipment\", []))\n",
    "                grip_signatures.extend(features.get(\"grip\", []))\n",
    "                position_signatures.extend(features.get(\"position\", []))\n",
    "                movement_signatures.extend(features.get(\"movement\", []))\n",
    "\n",
    "            # Equipment rule\n",
    "            if len(set(equipment_signatures)) > 1:\n",
    "                return f\"Equipment mismatch in group '{standard_name}': {set(equipment_signatures)}\"\n",
    "\n",
    "            # Grip rule\n",
    "            if len(set(grip_signatures)) > 1:\n",
    "                return f\"Grip mismatch in group '{standard_name}': {set(grip_signatures)}\"\n",
    "\n",
    "            # Position rule\n",
    "            if len(set(position_signatures)) > 1:\n",
    "                return f\"Body position mismatch in group '{standard_name}': {set(position_signatures)}\"\n",
    "\n",
    "            # Movement pattern rule\n",
    "            if len(set(movement_signatures)) > 1:\n",
    "                return f\"Movement pattern mismatch in group '{standard_name}': {set(movement_signatures)}\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    def llm_validate_cluster(self, exercise_names: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        use LLM to validate and standardize a cluster of exercise names\n",
    "        \"\"\"\n",
    "        prompt = self.get_base_prompt(exercise_names).replace(\"'\", \"''\")\n",
    "        \n",
    "        bad_response = True\n",
    "        attempt = 0\n",
    "        max_attempts = 5\n",
    "        feedback=None\n",
    "\n",
    "        while bad_response and attempt < max_attempts:\n",
    "            try:\n",
    "                escalated_prompt = escalate_prompt(self.get_base_prompt(exercise_names), attempt)\n",
    "                escalated_prompt = escalate_prompt(self.get_base_prompt(exercise_names), attempt, feedback=feedback)\n",
    "                \n",
    "                # leveraging Databricks foundation model (free tier...)\n",
    "                response = str(gen_text_databricks([escalated_prompt], temperature=0.1, max_new_tokens=5000, use_template=True))\n",
    "                response = response.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                \n",
    "                repaired = repair_json(response)\n",
    "                parsed = ast.literal_eval(json.loads(repaired)[0])\n",
    "                validate_json_structure(parsed)\n",
    "                \n",
    "                validation_result = self.enforce_disambiguation_rules(parsed)\n",
    "                if validation_result is not True:\n",
    "                    feedback = validation_result\n",
    "                    print(\"Invalid group detected:\", feedback)\n",
    "                    print('invalid PARSED:', parsed)\n",
    "                    attempt += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    bad_response = False\n",
    "                                \n",
    "                print(\"Success:\")\n",
    "                bad_response = False\n",
    "                \n",
    "                # Create mapping from variations to standardized names\n",
    "                mapping = {}\n",
    "                for group in parsed[\"groups\"]:\n",
    "                    try:\n",
    "                        standardized = group[\"standardized_name\"]\n",
    "                    except: # plan for the LLM to hallucinate standardized name \n",
    "                        standardized = group[\"standard_name\"]\n",
    "                    for variation in group[\"variations\"]:\n",
    "                        mapping[variation] = standardized\n",
    "                \n",
    "                return mapping\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nAttempt {attempt + 1} failed.\")\n",
    "                print(\"Error:\", str(e))\n",
    "                print(\"Response:\", response[:500] if 'response' in locals() else \"No response\")\n",
    "                attempt += 1\n",
    "        \n",
    "        # If all attempts failed, raise the last exception\n",
    "        raise Exception(f\"LLM validation failed after {max_attempts} attempts\")\n",
    "\n",
    "    def smart_clustering(self, exercise_names: List[str], min_cluster_size: int = 2) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Perform intelligent clustering with balanced cluster sizes\n",
    "        \"\"\"\n",
    "        print(f\"Starting smart clustering for {len(exercise_names)} exercises...\")\n",
    "        \n",
    "        # Step 1: Pre-cluster by equipment/movement to create balanced groups\n",
    "        equipment_groups = self.feature_based_pre_clustering(exercise_names)\n",
    "        print(f\"Pre-clustering created {len(equipment_groups)} equipment/movement groups\")\n",
    "        \n",
    "        final_mapping = {}\n",
    "        \n",
    "        # Step 2: Process each equipment group separately\n",
    "        for group_name, group_exercises in equipment_groups.items():\n",
    "            print(f\"Processing {group_name} group with {len(group_exercises)} exercises\")\n",
    "            \n",
    "            if len(group_exercises) == 1:\n",
    "                # Single exercise - no clustering needed\n",
    "                final_mapping[group_exercises[0]] = group_exercises[0]\n",
    "                continue\n",
    "            \n",
    "            # Step 3: Balanced clustering within each group\n",
    "            #balanced_clusters = self.balanced_clustering_within_group(group_exercises, max_cluster_size)\n",
    "            balanced_clusters = self.hdbscan_clustering_within_group(group_exercises, min_cluster_size)\n",
    "            # Step 4: LLM validation for each balanced cluster\n",
    "            for cluster in balanced_clusters:\n",
    "                print(f\"  Processing cluster with {len(cluster)} exercises\")\n",
    "                \n",
    "                if len(cluster) == 1:\n",
    "                    final_mapping[cluster[0]] = cluster[0]\n",
    "                else:\n",
    "                    try:\n",
    "                        cluster_mapping = self.llm_validate_cluster(cluster)\n",
    "                        final_mapping.update(cluster_mapping)\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Warning: LLM validation failed for cluster, treating as individual exercises\")\n",
    "                        # Fallback: treat each as individual exercise\n",
    "                        for exercise in cluster:\n",
    "                            final_mapping[exercise] = exercise\n",
    "        \n",
    "        print(f\"Smart clustering complete. Processed {len(final_mapping)} exercises\")\n",
    "        return final_mapping\n",
    "\n",
    "    def standardize_exercise_dataset(self, exercise_names: List[str]) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Complete standardization pipeline\n",
    "        \"\"\"\n",
    "        print(\"Starting exercise standardization pipeline...\")\n",
    "        \n",
    "        # Remove obvious invalid exercises\n",
    "        valid_exercises = [name for name in exercise_names \n",
    "                          if not any(invalid in name.upper() \n",
    "                                   for invalid in [\"REST\", \"JOGGING\", \"WALKING\", \"CYCLING\"])]\n",
    "\n",
    "        # cleaning known undesired acronyms\n",
    "        valid_exercises = [name.replace('NP', '') for name in valid_exercises]\n",
    "        \n",
    "        print(f\"Filtered out {len(exercise_names) - len(valid_exercises)} invalid exercises\")\n",
    "        \n",
    "        # Perform smart clustering\n",
    "        mapping = self.smart_clustering(valid_exercises)\n",
    "        \n",
    "        # Generate summary\n",
    "        standardized_names = list(set(mapping.values()))\n",
    "        \n",
    "        for k, v in mapping.items():\n",
    "            tokens = word_tokenize(v.lower())\n",
    "            tagged = pos_tag(tokens)\n",
    "            lemmatized = [\n",
    "                lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for word, pos in tagged\n",
    "                if word.upper() not in STOPWORDS]\n",
    "            mapping[k] = ' '.join([t.upper() for t in lemmatized]).replace('-', ' ') # update mapping with lemmatized version\n",
    "        \n",
    "        # Group by standardized name for review\n",
    "        groups = {}\n",
    "        for original, standardized in mapping.items():\n",
    "            if standardized not in groups:\n",
    "                groups[standardized] = []\n",
    "            groups[standardized].append(original)\n",
    "        \n",
    "        # Create DataFrame for review\n",
    "        review_data = []\n",
    "        for standardized, originals in groups.items():\n",
    "        \n",
    "            review_data.append({\n",
    "                \"standardized_name\": standardized,\n",
    "                \"original_count\": len(originals),\n",
    "                \"original_names\": \" | \".join(originals)})\n",
    "        \n",
    "        review_df = pd.DataFrame(review_data).sort_values(\"original_count\", ascending=False)\n",
    "        \n",
    "        print(f\"Standardization complete:\")\n",
    "        print(f\"  Original exercises: {len(exercise_names)}\")\n",
    "        print(f\"  Valid exercises: {len(valid_exercises)}\")\n",
    "        print(f\"  Standardized exercises: {len(standardized_names)}\")\n",
    "        print(f\"  Reduction: {((len(valid_exercises) - len(standardized_names)) / len(valid_exercises) * 100):.1f}%\")\n",
    "        \n",
    "        return mapping, review_df\n",
    "\n",
    "# implementation\n",
    "def apply_standardization(df, exercise_column=\"name\"):\n",
    "    \"\"\"\n",
    "    Apply standardization to your exercise DataFrame using broadcast join\n",
    "    \"\"\"\n",
    "    \n",
    "    standardizer = AdvancedExerciseStandardizer()\n",
    "    unique_exercises = [row.name for row in df.select(\"name\").distinct().collect() if row.name is not None]\n",
    "    mapping, review_df = standardizer.standardize_exercise_dataset(unique_exercises)\n",
    "    \n",
    "    print(\"Type of mapping:\", type(mapping))\n",
    "    print(\"First few items:\", list(mapping.items())[:5] if hasattr(mapping, \"items\") else mapping.head())\n",
    "    \n",
    "    # mapping df containing original name and standardized name\n",
    "    if isinstance(mapping, dict):\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        mapping_data = [(k, v) for k, v in mapping.items()]\n",
    "        mapping_df = spark.createDataFrame(mapping_data, [\"name\", \"standardized_name\"])\n",
    "        \n",
    "        # can safely use a broadcast join here, mapping_df is quite small (< 500 records)\n",
    "        df = df.join(broadcast(mapping_df), on=\"name\", how=\"left\")\n",
    "    else:\n",
    "        df = df.join(broadcast(mapping), on=\"name\", how=\"left\")\n",
    "    \n",
    "    # drop records where a standardized name was not successfully generated\n",
    "    df = df.dropna(subset=[\"standardized_name\"])\n",
    "    \n",
    "    return df, mapping, review_df\n",
    "\n",
    "def apply_standardization_with_category_validation(df, exercise_column=\"name\", category_column=\"category\"):\n",
    "    \"\"\"\n",
    "    Apply standardization using category information to prevent erroneous cross-category matches\n",
    "    while still allowing legitimate matches across inconsistent category assignments.\n",
    "    \"\"\"\n",
    "    \n",
    "    standardizer = AdvancedExerciseStandardizer()\n",
    "    \n",
    "    # Get unique exercises with their category assignments\n",
    "    unique_exercises_with_categories = (df.select(exercise_column, category_column)\n",
    "                                       .distinct()\n",
    "                                       .filter(col(exercise_column).isNotNull() & \n",
    "                                              col(category_column).isNotNull())\n",
    "                                       .collect())\n",
    "    \n",
    "    # Create exercise-to-categories mapping (some exercises may appear in multiple categories)\n",
    "    exercise_to_categories = {}\n",
    "    for row in unique_exercises_with_categories:\n",
    "        exercise_name = row[exercise_column]\n",
    "        category = row[category_column]\n",
    "        \n",
    "        if exercise_name not in exercise_to_categories:\n",
    "            exercise_to_categories[exercise_name] = set()\n",
    "        exercise_to_categories[exercise_name].add(category)\n",
    "    \n",
    "    # Identify exercises with inconsistent categorization\n",
    "    inconsistent_exercises = {name: cats for name, cats in exercise_to_categories.items() if len(cats) > 1}\n",
    "    \n",
    "    print(f\"Found {len(inconsistent_exercises)} exercises with inconsistent categorization:\")\n",
    "    for exercise, categories in list(inconsistent_exercises.items())[:10]:  # Show first 10\n",
    "        print(f\"  '{exercise}': {categories}\")\n",
    "    if len(inconsistent_exercises) > 10:\n",
    "        print(f\"  ... and {len(inconsistent_exercises) - 10} more\")\n",
    "    \n",
    "    # Get all unique exercise names for clustering\n",
    "    all_exercises = list(exercise_to_categories.keys())\n",
    "    \n",
    "    # Apply standardization to all exercises\n",
    "    all_mappings, review_df = standardizer.standardize_exercise_dataset(all_exercises)\n",
    "    \n",
    "    # Post-process: validate that grouped exercises don't span incompatible categories\n",
    "    validated_mappings = validate_category_consistency(all_mappings, exercise_to_categories)\n",
    "    \n",
    "    # Create mapping DataFrame for join\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    mapping_data = [(k, v) for k, v in validated_mappings.items()]\n",
    "    mapping_df = spark.createDataFrame(mapping_data, [exercise_column, \"standardized_name\"])\n",
    "    \n",
    "    # Join with original DataFrame using broadcast join\n",
    "    df_standardized = df.join(broadcast(mapping_df), on=exercise_column, how=\"left\")\n",
    "    \n",
    "    # Drop records where standardized name was not generated\n",
    "    df_standardized = df_standardized.dropna(subset=[\"standardized_name\"])\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nOverall Summary:\")\n",
    "    print(f\"  Total original exercises: {len(validated_mappings)}\")\n",
    "    print(f\"  Total standardized exercises: {len(set(validated_mappings.values()))}\")\n",
    "    print(f\"  Overall reduction: {((len(validated_mappings) - len(set(validated_mappings.values()))) / len(validated_mappings) * 100):.1f}%\")\n",
    "    \n",
    "    return df_standardized, validated_mappings, review_df\n",
    "\n",
    "\n",
    "def validate_category_consistency(mappings, exercise_to_categories):\n",
    "    \"\"\"\n",
    "    Validate that exercises grouped together don't span incompatible categories.\n",
    "    Split groups if they contain exercises from incompatible categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define compatible category groups (exercises can match across these)\n",
    "    compatible_groups = [\n",
    "        {\"ARMS\", \"CHEST\"},\n",
    "        {\"LEGS\", \"CALVES\", \"CARDIO\"},\n",
    "        {\"CARDIO\", \"ABS\"},\n",
    "        {\"ARMS\", \"BACK\"},\n",
    "        {\"ARMS\", \"SHOULDERS\"},\n",
    "        {\"LEGS\", \"BACK\"},\n",
    "        {\"BACK\", \"ABS\"}]\n",
    "    \n",
    "    def categories_are_compatible(cat_set1, cat_set2):\n",
    "        \"\"\"Check if two sets of categories are compatible for grouping\"\"\"\n",
    "        # If either exercise appears in multiple categories, check for overlap\n",
    "        if len(cat_set1) > 1 or len(cat_set2) > 1:\n",
    "            # If there's any overlap, they're compatible\n",
    "            if cat_set1 & cat_set2:\n",
    "                return True\n",
    "        \n",
    "        # Check if they belong to the same compatible group\n",
    "        for group in compatible_groups:\n",
    "            if (cat_set1 & group) and (cat_set2 & group):\n",
    "                return True\n",
    "        \n",
    "        # Special case: if categories are exactly the same\n",
    "        if cat_set1 == cat_set2:\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    # Group exercises by their standardized names\n",
    "    standardized_groups = {}\n",
    "    for original, standardized in mappings.items():\n",
    "        if standardized not in standardized_groups:\n",
    "            standardized_groups[standardized] = []\n",
    "        standardized_groups[standardized].append(original)\n",
    "    \n",
    "    validated_mappings = {}\n",
    "    split_count = 0\n",
    "    \n",
    "    for standardized_name, original_exercises in standardized_groups.items():\n",
    "        if len(original_exercises) == 1:\n",
    "            # Single exercise, no validation needed\n",
    "            validated_mappings[original_exercises[0]] = standardized_name\n",
    "            continue\n",
    "        \n",
    "        # Check category compatibility within the group\n",
    "        exercise_categories = [(ex, exercise_to_categories[ex]) for ex in original_exercises]\n",
    "        \n",
    "        # Find compatible subgroups\n",
    "        compatible_subgroups = []\n",
    "        \n",
    "        for exercise, categories in exercise_categories:\n",
    "            placed = False\n",
    "            for subgroup in compatible_subgroups:\n",
    "                # Check if this exercise is compatible with all exercises in the subgroup\n",
    "                if all(categories_are_compatible(categories, exercise_to_categories[ex]) \n",
    "                       for ex in subgroup):\n",
    "                    subgroup.append(exercise)\n",
    "                    placed = True\n",
    "                    break\n",
    "            \n",
    "            if not placed:\n",
    "                compatible_subgroups.append([exercise])\n",
    "        \n",
    "        # Create mappings for each compatible subgroup\n",
    "        if len(compatible_subgroups) == 1:\n",
    "            # All exercises are compatible\n",
    "            for exercise in original_exercises:\n",
    "                validated_mappings[exercise] = standardized_name\n",
    "        else:\n",
    "            # Split into multiple groups\n",
    "            split_count += 1\n",
    "            print(f\"Splitting group '{standardized_name}' into {len(compatible_subgroups)} subgroups due to category incompatibility\")\n",
    "            \n",
    "            for i, subgroup in enumerate(compatible_subgroups):\n",
    "                if len(subgroup) == 1:\n",
    "                    # Single exercise gets its own name\n",
    "                    validated_mappings[subgroup[0]] = subgroup[0]\n",
    "                else:\n",
    "                    # Multiple exercises get modified standardized name\n",
    "                    subgroup_name = f\"{standardized_name}\"\n",
    "                    if i > 0:  # Add suffix for disambiguation\n",
    "                        subgroup_name += f\" V{i+1}\"\n",
    "                    \n",
    "                    for exercise in subgroup:\n",
    "                        validated_mappings[exercise] = subgroup_name\n",
    "    \n",
    "    if split_count > 0:\n",
    "        print(f\"Split {split_count} groups due to category incompatibility\")\n",
    "    \n",
    "    return validated_mappings\n",
    "\n",
    "# apply gen AI standardization of exercise names\n",
    "standardized_df, name_mapping, review_report = apply_standardization(exercise_df)\n",
    "display(standardized_df.select(\"name\", \"standardized_name\").sort(\"standardized_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43721c21-db1c-478c-8ab3-f4974fc02875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wger_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
