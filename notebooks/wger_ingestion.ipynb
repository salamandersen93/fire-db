{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04ffa21-dccf-4cea-ae1c-f34fa5c13e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "def get_exercises(api_url):\n",
    "    '''Retrieves complete list of exercises from the wger public API'''\n",
    "    all_exercises = []\n",
    "    current_url = api_url\n",
    "\n",
    "    try:\n",
    "        while current_url:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            all_exercises.extend(data['results'])\n",
    "            current_url = data.get('next')\n",
    "        return all_exercises\n",
    "    except Exception as e:\n",
    "        print('error fetching exercise data from wger API:', e)\n",
    "\n",
    "# documentation link: https://exercise.hellogym.io/nl/software/api\n",
    "# the exerciseinfo contains denormalized data with nested\n",
    "wger_api_url = \"https://wger.de/api/v2/exerciseinfo/?status=2&language=2\"\n",
    "exercises = get_exercises(wger_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77909cd1-f867-49ab-a609-63bfed782aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the schema\n",
    "ex_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"uuid\", StringType()), \n",
    "    StructField(\"name\", StringType()), \n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"created\", TimestampType()),\n",
    "    StructField(\"last_update\", TimestampType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"muscles\",  StringType()),\n",
    "    StructField(\"muscles_secondary\",  StringType()),\n",
    "    StructField(\"equipment\",  StringType()),\n",
    "    StructField(\"variations\", StringType()),\n",
    "    StructField(\"license_author\", StringType())])\n",
    "\n",
    "# Normalize the data\n",
    "def normalize_record(record):\n",
    "    normalized = {\n",
    "        \"id\": record.get(\"id\"),\n",
    "        \"uuid\": record.get(\"uuid\"),\n",
    "        \"name\": [record['translations'][i].get(\"name\") for i in range(len(record['translations'])) if record['translations'][i].get(\"language\") == 2][0],\n",
    "        \"description\": [record['translations'][i].get(\"description\") for i in range(len(record['translations'])) if record['translations'][i].get(\"language\") == 2][0],\n",
    "        \"created\": datetime.fromisoformat(record[\"created\"].replace('Z', '+00:00')) if record.get(\"created\") else None,\n",
    "        \"last_update\": datetime.fromisoformat(record[\"last_update\"].replace('Z', '+00:00')) if record.get(\"last_update\") else None,\n",
    "        \"category\": record.get(\"category\"),\n",
    "        \"muscles\": [muscle[\"name\"] for muscle in record.get(\"muscles\", [])],\n",
    "        \"muscles_secondary\": [muscle[\"name\"] for muscle in record.get(\"muscles_secondary\", [])],\n",
    "        \"equipment\": [equip[\"name\"] for equip in record.get(\"equipment\", [])],\n",
    "        \"variations\": record.get(\"variations\", []) if record.get(\"variations\") is not None else [],\n",
    "        \"license_author\": record.get(\"license_author\")\n",
    "    }\n",
    "    return normalized\n",
    "\n",
    "exercise_abbv = [normalize_record(record) for record in exercises]\n",
    "\n",
    "try:\n",
    "    exercise_df = spark.createDataFrame(exercise_abbv, schema=ex_schema)\n",
    "    exercise_df = exercise_df.drop(\"uuid\", \"created\", \"last_update\", \"license_author\")\n",
    "    display(exercise_df)\n",
    "except Exception as e:\n",
    "    print(\"error creating dataframe:\", e)\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wger_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
