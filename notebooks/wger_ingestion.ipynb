{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04ffa21-dccf-4cea-ae1c-f34fa5c13e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "def get_exercises(api_url):\n",
    "    '''Retrieves complete list of exercises from the wger public API'''\n",
    "    all_exercises = []\n",
    "    current_url = api_url\n",
    "\n",
    "    while current_url:\n",
    "        response = requests.get(current_url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        data = response.json()\n",
    "        all_exercises.extend(data['results'])  # Assuming exercises are in 'results' key\n",
    "        current_url = data.get('next') # Example: data['next'] if available\n",
    "\n",
    "    return all_exercises\n",
    "\n",
    "wger_api_url = \"https://wger.de/api/v2/exerciseinfo/\"  # Replace with the actual endpoint\n",
    "exercises = get_exercises(wger_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77909cd1-f867-49ab-a609-63bfed782aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ex_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"uuid\", StringType()), \n",
    "    StructField(\"created\", TimestampType()),\n",
    "    StructField(\"last_update\", TimestampType()),\n",
    "    StructField(\"category\", ArrayType(StructType())),\n",
    "    StructField(\"muscles\", ArrayType(StructType())),\n",
    "    StructField(\"muscles_secondary\", ArrayType(StructType())),\n",
    "    StructField(\"equipment\", ArrayType(StructType())),\n",
    "    StructField(\"variations\", ArrayType(StructType())),\n",
    "    StructField(\"license_author\", StringType())])\n",
    "\n",
    "fields = [\"id\", \"uuid\", \"created\", \"last_update\", \"category\", \"muscles\", \"muscles_secondary\", \"equipment\", \"variations\", \"license_author\"]\n",
    "exercise_abbv = [{key: record[key] for key in fields if key in record} for record in exercises]\n",
    "print(exercise_abbv[0])\n",
    "exercise_df = spark.createDataFrame(exercise_abbv)\n",
    "exercise_df = exercise_df.drop('uuid', 'created', 'ast_update', 'license_author')\n",
    "exercise_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wger_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
