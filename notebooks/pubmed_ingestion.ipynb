{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64151c2-66a6-4f4f-9174-3d228cec2530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import configparser\n",
    "from Bio import Entrez, Medline\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, to_date, when, length, udf, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "\n",
    "@udf(StringType())\n",
    "def fallback_dateparser(date_str):\n",
    "    \"\"\"\n",
    "    Converts dates that can't be parsed by the default parser.\n",
    "    Returns a dataparser parsed datetime value.\n",
    "    \"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    parsed = dateparser.parse(date_str)\n",
    "    return parsed.strftime(\"%Y-%m-%d\") if parsed else None\n",
    "\n",
    "def extract_doi(rec):\n",
    "    \"\"\"\n",
    "    Extracts the document identifier doi from the record and returns it as a string. The DOI can be parsed from the AID.\n",
    "    \"\"\"\n",
    "    if \"AID\" in rec:\n",
    "        for aid in rec[\"AID\"]:\n",
    "            if \"doi\" in aid.lower():\n",
    "                return aid.replace(\" [doi]\", \"\")\n",
    "    return None\n",
    "\n",
    "def get_pubmed_date_range_past_year():\n",
    "    \"\"\"\n",
    "    Calculates timedelta of one month for pubmed query.\n",
    "    Returns the start and end dates as strings in the format \"YYYY/MM/DD\".\n",
    "    \"\"\"\n",
    "    today = datetime.today()\n",
    "    one_year_ago = today - relativedelta(months=12)\n",
    "    \n",
    "    # Format dates as YYYY/MM/DD (PubMed compatible)\n",
    "    today_str = today.strftime(\"%Y/%m/%d\")\n",
    "    one_year_ago_str = one_year_ago.strftime(\"%Y/%m/%d\")\n",
    "    \n",
    "    return one_year_ago_str, today_str\n",
    "\n",
    "# spark config\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# job metadata metrics\n",
    "run_start = datetime.now()\n",
    "run_status = \"SUCCESS\"\n",
    "new_records = 0\n",
    "\n",
    "# Code to connect to AWS S3 (commented out for Databricks Free Edition compatibility)\n",
    "# In production, S3 access is granted via IAM role attached to the cluster,\n",
    "# or through Databricks Secrets configured for storage credentials\n",
    "# spark.conf.set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.InstanceProfileCredentialsProvider\")\n",
    "\n",
    "try:\n",
    "# Entrez configuration + set max retry configuration to gracefully handle issues\n",
    "    Entrez.email = \"mikeandersen622@gmail.com\"\n",
    "    Entrez.max_tries = 5 \n",
    "    Entrez.sleep_between_tries = 20 \n",
    "\n",
    "    # set batch vs incremental \n",
    "    incremental = False\n",
    "    print(f\"Incremental mode: {incremental}\")\n",
    "\n",
    "    # incremental determines date range of search conditions\n",
    "    if incremental == True:\n",
    "        last_run_df = spark.sql(\"\"\"\n",
    "                SELECT last_run_timestamp\n",
    "                FROM script_run_metadata\n",
    "                WHERE script_name = 'pubmed_ingestion'\n",
    "                ORDER BY last_run_timestamp DESC\n",
    "                LIMIT 1\"\"\")\n",
    "            \n",
    "        last_run_row = last_run_df.head(1)\n",
    "        if last_run_row:\n",
    "            start_date = last_run_row[0]['last_run_timestamp'].strftime('%Y/%m/%d')\n",
    "            end_date = datetime.max()\n",
    "        else:\n",
    "            start_date = datetime.now()\n",
    "    else:\n",
    "        print(\"No previous run found â€” defaulting to past month.\")\n",
    "        start_date, end_date = get_pubmed_date_range_past_year()\n",
    "\n",
    "    displayHTML(f\"start date: {start_date}\")\n",
    "    displayHTML(f\"end date: {end_date}\")\n",
    "\n",
    "    # pubmed search condition definition\n",
    "    # pulls documents tagged with MeSH terms for physical conditioning\n",
    "    mesh_query = (\n",
    "        '\"Exercise\"[MeSH Terms] OR '\n",
    "        '\"Physical Conditioning, Human\"[MeSH Terms] OR '\n",
    "        '\"Resistance Training\"[MeSH Terms] OR '\n",
    "        '\"Aerobic Exercise\"[MeSH Terms] OR '\n",
    "        '\"High-Intensity Interval Training\"[MeSH Terms] OR '\n",
    "        '\"Plyometric Exercise\"[MeSH Terms] OR '\n",
    "        '\"Endurance Training\"[MeSH Terms] OR '\n",
    "        '\"Muscle Stretching Exercises\"[MeSH Terms] OR '\n",
    "        '\"Physical Fitness\"[MeSH Terms] OR '\n",
    "        '\"Cardiorespiratory Fitness\"[MeSH Terms] OR '\n",
    "        '\"Exercise Therapy\"[MeSH Terms] OR '\n",
    "        '\"Walking\"[MeSH Terms] OR '\n",
    "        '\"Swimming\"[MeSH Terms] OR '\n",
    "        '\"Gymnastics\"[MeSH Terms]')\n",
    "\n",
    "    # generate search query for API\n",
    "    search_term = f'({mesh_query}) AND (\"{start_date}\"[Date - Publication] : \"{end_date}\"[Date - Publication])'\n",
    "    #search_term = f'({mesh_query}))'\n",
    "\n",
    "    # batch in sets of 100 (temporary max of 1000)\n",
    "    batch_size = 1000\n",
    "    target_count = 30000 # only grab up to 30000\n",
    "    pmid_list = []\n",
    "\n",
    "    # get total counts of pubs\n",
    "    # with error handling for Entrez API\n",
    "    retries = 0\n",
    "    while retries < Entrez.max_tries:\n",
    "        try:\n",
    "            handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=0)\n",
    "            total_count = int(Entrez.read(handle)[\"Count\"])\n",
    "            displayHTML(f\"<b>total available {total_count}</b>\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if \"HTTP Error 429\" in str(e):\n",
    "                print(\"Rate limit exceeded. Retrying after a delay...\")\n",
    "                time.sleep(Entrez.sleep_between_tries)\n",
    "                retries += 1\n",
    "            else:\n",
    "                displayHTML(f\"Error fetching PMIDs: {e}\")\n",
    "                total_count = 0\n",
    "                break\n",
    "           \n",
    "    # fetch results matching search criteria in batches\n",
    "    # gets pmids based on defined query and batch size\n",
    "    if total_count > 0:\n",
    "        for start in range(0, min(target_count, total_count), batch_size):\n",
    "            # error handling for Entrez API\n",
    "            retries = 0\n",
    "            while retries < Entrez.max_tries:\n",
    "                try:\n",
    "                    handle = Entrez.esearch(\n",
    "                        db=\"pubmed\",\n",
    "                        term=search_term,\n",
    "                        retmax=batch_size,\n",
    "                        retstart=start)\n",
    "                    batch_result = Entrez.read(handle)\n",
    "                    pmid_list.extend(batch_result[\"IdList\"])\n",
    "                    displayHTML(f\"<b>Fetched {len(pmid_list)} PMIDs so far...</b>\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if \"HTTP Error 429\" in str(e):\n",
    "                        print(\"Rate limit exceeded. Retrying after a delay...\")\n",
    "                        time.sleep(Entrez.sleep_between_tries)\n",
    "                        retries += 1\n",
    "                    else:\n",
    "                        displayHTML(f\"Error fetching PMIDs: {e}\")\n",
    "                        break\n",
    "\n",
    "        # parse pubmed udf, then iterate over pmid results and fetch record details\n",
    "        parse_pubmed_date_udf = udf(fallback_dateparser, StringType())\n",
    "        all_records = []\n",
    "        for i in range(0, len(pmid_list), batch_size):\n",
    "            try:\n",
    "                batch_pmids = pmid_list[i:i+batch_size]\n",
    "                handle = Entrez.efetch(db=\"pubmed\", id=batch_pmids, rettype=\"medline\", retmode=\"text\")\n",
    "                records = list(Medline.parse(handle))\n",
    "                for rec in records:\n",
    "                    all_records.append({\n",
    "                        \"pmid\": rec.get(\"PMID\"),\n",
    "                        \"title\": rec.get(\"TI\"),\n",
    "                        \"abstract\": rec.get(\"AB\"),\n",
    "                        \"journal\": rec.get(\"JT\"),\n",
    "                        \"date\": rec.get(\"DP\"),\n",
    "                        \"doi\": extract_doi(rec)})\n",
    "            except Entrez.Parser.ValidationError as e:\n",
    "                print(f\"Entrez XML parsing error: {e}.\")\n",
    "            except Exception as e:\n",
    "                displayHTML(f\"Unexpected error extracting record for PMID: {i}\")\n",
    "\n",
    "        # read into pyspark dataframe\n",
    "        rows = []\n",
    "        for rec in all_records:\n",
    "            rows.append(Row(\n",
    "                pmid=rec.get(\"pmid\"),\n",
    "                title=rec.get(\"title\"),\n",
    "                abstract=rec.get(\"abstract\"),\n",
    "                journal=rec.get(\"journal\"),\n",
    "                date=rec.get(\"date\"),\n",
    "                doi=rec.get(\"doi\")))\n",
    "        df = spark.createDataFrame(rows)\n",
    "        displayHTML(f\"Records prior to normalization: {df.count()}\")\n",
    "\n",
    "        # data quality and normalization:\n",
    "        #   strip whitespace and new lines\n",
    "        #   remove records with null doi (required for matching later)\n",
    "        #   remove duplicate records based on doi (considered true duplicate)\n",
    "        normalized_df = (\n",
    "            df.withColumn(\"abstract\", regexp_replace(col(\"abstract\"), \"\\n\", \" \"))\n",
    "            .withColumn(\"title\", regexp_replace(trim(col(\"title\")), \"\\n\", \"\"))\n",
    "            .withColumn(\"pub_date\", fallback_dateparser(col(\"date\"))).drop(\"date\"))\n",
    "\n",
    "        normalized_df = normalized_df.filter(\n",
    "            (col(\"doi\").isNotNull()) & (trim(col(\"doi\")) != \"\") & (col(\"pub_date\").isNotNull()))\n",
    "        normalized_df = normalized_df.dropDuplicates([\"doi\"])\n",
    "        total_count = normalized_df.count()\n",
    "\n",
    "        # if incremental, only append new records\n",
    "        if incremental:\n",
    "            loaded_df = spark.table(\"firedb_pubmed\")\n",
    "            # find records not in existing table\n",
    "            new_records_df = normalized_df.join(loaded_df.select(\"doi\"), on=\"doi\", how=\"left_anti\")\n",
    "            new_records = new_records_df.count()\n",
    "            if new_records > 0:\n",
    "                new_records_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"firedb_pubmed\")\n",
    "            else:\n",
    "                print(\"No new records to append.\")\n",
    "        # full load mode overwrites entire table\n",
    "        else:\n",
    "            normalized_df.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "                \"overwriteSchema\", \"true\").saveAsTable(\"firedb_pubmed\")\n",
    "    else:\n",
    "        print(\"No new records\")\n",
    "        total_count = 0\n",
    "\n",
    "    # main logic complete, job run successful\n",
    "    run_status = 'SUCCESS'\n",
    "\n",
    "except Exception as e:\n",
    "    # unhandled exception\n",
    "    print('error during pubmed ingestion', e)\n",
    "    run_status = 'FAILED'\n",
    "\n",
    "# logging in run metadata\n",
    "run_end = datetime.now()\n",
    "duration = (run_end - run_start).total_seconds()\n",
    "schema = StructType([\n",
    "    StructField(\"script_name\", StringType(), False),\n",
    "    StructField(\"last_run_timestamp\", TimestampType(), False),\n",
    "    StructField(\"incremental_run\", StringType(), False),\n",
    "    StructField(\"total_pubs\", IntegerType(), True),\n",
    "    StructField(\"new_records\", IntegerType(), True),\n",
    "    StructField(\"run_status\", StringType(), True),\n",
    "    StructField(\"run_start\", TimestampType(), True),\n",
    "    StructField(\"run_end\", TimestampType(), True),\n",
    "    StructField(\"duration_seconds\", FloatType(), True)])\n",
    "run_metadata = [('pubmed_ingestion',datetime.now(),str(incremental), \n",
    "    int(total_count),int(new_records), run_status,run_start,\n",
    "    datetime.now(), float(duration))]\n",
    "# write metadata to delta table\n",
    "metadata_df = spark.createDataFrame(run_metadata, schema=schema)\n",
    "metadata_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"script_run_metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3851c1b7-a17f-4fdf-9b32-9fb53bd2c4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pubmed_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
