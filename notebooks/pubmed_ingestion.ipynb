{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64151c2-66a6-4f4f-9174-3d228cec2530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import configparser\n",
    "from Bio import Entrez, Medline\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "# read config info for run to import config module\n",
    "config_parser = configparser.ConfigParser()\n",
    "sys.path.append(\"/Workspace/Repos/firedb/fire-db/config/\")\n",
    "from pubmed_config import PubmedConfig\n",
    "\n",
    "# configuration & data class definitions\n",
    "@dataclass\n",
    "class ETLConfig:\n",
    "    email: str\n",
    "    max_tries: int = 5\n",
    "    sleep_between_tries: int = 20\n",
    "    batch_size: int = 1000\n",
    "    target_count: int = 5000\n",
    "    incremental: bool = False\n",
    "\n",
    "@dataclass\n",
    "class RunMetadata:\n",
    "    script_name: str\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    incremental_run: bool\n",
    "    total_pubs: int\n",
    "    new_records: int\n",
    "    status: str\n",
    "    duration_seconds: float\n",
    "\n",
    "# business logic and function definitions\n",
    "def get_date_range_past_year() -> Tuple[str, str]:\n",
    "    \"\"\"calculate date range for the past year in appropriate format for Entrez query.\"\"\"\n",
    "    today = datetime.today()\n",
    "    one_year_ago = today - relativedelta(months=12)\n",
    "    return (one_year_ago.strftime(\"%Y/%m/%d\"), today.strftime(\"%Y/%m/%d\"))\n",
    "\n",
    "def build_mesh_query() -> str:\n",
    "    \"\"\"build the MeSH terms query string.\"\"\"\n",
    "    mesh_terms = [\n",
    "        '\"Exercise\"[MeSH Terms]',\n",
    "        '\"Physical Conditioning, Human\"[MeSH Terms]',\n",
    "        '\"Resistance Training\"[MeSH Terms]',\n",
    "        '\"Aerobic Exercise\"[MeSH Terms]',\n",
    "        '\"High-Intensity Interval Training\"[MeSH Terms]',\n",
    "        '\"Plyometric Exercise\"[MeSH Terms]',\n",
    "        '\"Endurance Training\"[MeSH Terms]',\n",
    "        '\"Muscle Stretching Exercises\"[MeSH Terms]',\n",
    "        '\"Physical Fitness\"[MeSH Terms]',\n",
    "        '\"Cardiorespiratory Fitness\"[MeSH Terms]',\n",
    "        '\"Exercise Therapy\"[MeSH Terms]',\n",
    "        '\"Walking\"[MeSH Terms]',\n",
    "        '\"Swimming\"[MeSH Terms]',\n",
    "        '\"Gymnastics\"[MeSH Terms]'\n",
    "    ]\n",
    "    return ' OR '.join(mesh_terms)\n",
    "\n",
    "def build_search_term(start_date: str, end_date: str) -> str:\n",
    "    \"\"\"build complete PubMed search term with MeSH terms and date range.\"\"\"\n",
    "    mesh_query = build_mesh_query()\n",
    "    return f'({mesh_query}) AND (\"{start_date}\"[Date - Publication] : \"{end_date}\"[Date - Publication])'\n",
    "\n",
    "def extract_doi(record: Dict) -> Optional[str]:\n",
    "    \"\"\"extract DOI from PubMed record - used in matching ETL\"\"\"\n",
    "    if \"AID\" not in record:\n",
    "        return None\n",
    "    \n",
    "    for aid in record[\"AID\"]:\n",
    "        if \"doi\" in aid.lower():\n",
    "            return aid.replace(\" [doi]\", \"\")\n",
    "    return None\n",
    "\n",
    "def parse_date_fallback(date_str: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"parse date string using dateparser as fallback, only if default parser unable to parse.\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    parsed = dateparser.parse(date_str)\n",
    "    return parsed.strftime(\"%Y-%m-%d\") if parsed else None\n",
    "\n",
    "# data processing and transformation functions\n",
    "def transform_record(record: Dict) -> Dict:\n",
    "    \"\"\"normalize individual PubMed records into dictionary.\"\"\"\n",
    "    return {\n",
    "        \"pmid\": record.get(\"PMID\"),\n",
    "        \"title\": record.get(\"TI\"),\n",
    "        \"abstract\": record.get(\"AB\"),\n",
    "        \"journal\": record.get(\"JT\"),\n",
    "        \"date\": record.get(\"DP\"),\n",
    "        \"doi\": extract_doi(record)}\n",
    "\n",
    "def normalize_dataframe(df):\n",
    "    \"\"\"apply normalization transformations to DataFrame and apply basic data quality filters\"\"\"\n",
    "    # UDF for date parsing\n",
    "    parse_date_udf = udf(parse_date_fallback, StringType())\n",
    "    \n",
    "    return (df\n",
    "            .withColumn(\"abstract\", regexp_replace(col(\"abstract\"), \"\\n\", \" \"))\n",
    "            .withColumn(\"title\", regexp_replace(trim(col(\"title\")), \"\\n\", \"\"))\n",
    "            .withColumn(\"pub_date\", parse_date_udf(col(\"date\")))\n",
    "            .drop(\"date\")\n",
    "            .filter((col(\"doi\").isNotNull()) & (trim(col(\"doi\")) != \"\") & (col(\"pub_date\").isNotNull()))\n",
    "            .dropDuplicates([\"doi\"]))\n",
    "\n",
    "def filter_new_records(normalized_df, existing_table_name: str, spark):\n",
    "    \"\"\"filter out records that already exist in the target table.\"\"\"\n",
    "    try:\n",
    "        existing_df = spark.table(existing_table_name)\n",
    "        return normalized_df.join(existing_df.select(\"doi\"), on=\"doi\", how=\"left_anti\")\n",
    "    except Exception:\n",
    "        # validate table exists...\n",
    "        return normalized_df\n",
    "\n",
    "# functions to manage interactions with Entrez API (biopython)\n",
    "def fetch_with_retry(fetch_func, max_tries: int = 5, sleep_time: int = 20):\n",
    "    \"\"\"retry wrapper for API calls.\"\"\"\n",
    "    for attempt in range(max_tries):\n",
    "        try:\n",
    "            return fetch_func()\n",
    "        except Exception as e:\n",
    "            if \"HTTP Error 429\" in str(e) and attempt < max_tries - 1:\n",
    "                print(f\"Rate limit exceeded. Retrying after {sleep_time}s... (attempt {attempt + 1})\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise e\n",
    "    raise Exception(f\"Failed after {max_tries} attempts\")\n",
    "\n",
    "def get_total_count(search_term: str) -> int:\n",
    "    \"\"\"get total count of records matching search term.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=0)\n",
    "        return int(Entrez.read(handle)[\"Count\"])\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_pmid_batch(search_term: str, start: int, batch_size: int) -> List[str]:\n",
    "    \"\"\"fetch a batch of PMIDs given a batch size and starting index.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=search_term,\n",
    "            retmax=batch_size,\n",
    "            retstart=start\n",
    "        )\n",
    "        return Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_records_batch(pmids: List[str]) -> List[Dict]:\n",
    "    \"\"\"fetch detailed records for a batch of PMIDs.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"text\")\n",
    "        return list(Medline.parse(handle))\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "# orchestration and transformation functions\n",
    "def collect_all_pmids(search_term: str, config: ETLConfig) -> List[str]:\n",
    "    \"\"\"collect all PMIDs matching the search criteria.\"\"\"\n",
    "    total_count = get_total_count(search_term)\n",
    "    print(f\"Total available records: {total_count}\")\n",
    "    \n",
    "    pmid_list = []\n",
    "    target_count = min(config.target_count, total_count)\n",
    "    \n",
    "    for start in range(0, target_count, config.batch_size):\n",
    "        batch_pmids = fetch_pmid_batch(search_term, start, config.batch_size)\n",
    "        pmid_list.extend(batch_pmids)\n",
    "        print(f\"Fetched {len(pmid_list)} PMIDs so far...\")\n",
    "    \n",
    "    return pmid_list\n",
    "\n",
    "def extract_all_records(pmid_list: List[str], config: ETLConfig) -> List[Dict]:\n",
    "    \"\"\"extract detailed records for all PMIDs.\"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    for i in range(0, len(pmid_list), config.batch_size):\n",
    "        batch_pmids = pmid_list[i:i + config.batch_size]\n",
    "        try:\n",
    "            records = fetch_records_batch(batch_pmids)\n",
    "            transformed_records = [transform_record(rec) for rec in records]\n",
    "            all_records.extend(transformed_records)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_records\n",
    "\n",
    "def create_spark_dataframe(records: List[Dict], spark) -> 'DataFrame':\n",
    "    \"\"\"convert records list to pyspark df.\"\"\"\n",
    "    if not records:\n",
    "        schema = StructType([\n",
    "            StructField(\"pmid\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"abstract\", StringType(), True),\n",
    "            StructField(\"journal\", StringType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"doi\", StringType(), True)])\n",
    "        return spark.createDataFrame([], schema)\n",
    "    \n",
    "    rows = [Row(**record) for record in records]\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def save_run_metadata(metadata: RunMetadata, spark):\n",
    "    \"\"\"store run metadata to tracking table.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"script_name\", StringType(), False),\n",
    "        StructField(\"last_run_timestamp\", TimestampType(), False),\n",
    "        StructField(\"incremental_run\", StringType(), False),\n",
    "        StructField(\"total_pubs\", IntegerType(), True),\n",
    "        StructField(\"new_records\", IntegerType(), True),\n",
    "        StructField(\"run_status\", StringType(), True),\n",
    "        StructField(\"run_start\", TimestampType(), True),\n",
    "        StructField(\"run_end\", TimestampType(), True),\n",
    "        StructField(\"duration_seconds\", FloatType(), True)])\n",
    "    \n",
    "    row_data = [(\n",
    "        metadata.script_name,\n",
    "        metadata.end_time,\n",
    "        str(metadata.incremental_run),\n",
    "        metadata.total_pubs,\n",
    "        metadata.new_records,\n",
    "        metadata.status,\n",
    "        metadata.start_time,\n",
    "        metadata.end_time,\n",
    "        metadata.duration_seconds)]\n",
    "    \n",
    "    metadata_df = spark.createDataFrame(row_data, schema=schema)\n",
    "    metadata_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"script_run_metadata\")\n",
    "\n",
    "# main pipeine/ETL function\n",
    "def run_pubmed_etl(config: ETLConfig, spark) -> RunMetadata:\n",
    "    \"\"\"main ETL pipeline function.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Entrez configuration\n",
    "        Entrez.email = config.email\n",
    "        Entrez.max_tries = config.max_tries\n",
    "        Entrez.sleep_between_tries = config.sleep_between_tries\n",
    "        \n",
    "        # date range generation\n",
    "        if config.incremental:\n",
    "            # get last run date from metadata table...\n",
    "            start_date, end_date = get_incremental_date_range(spark)\n",
    "        else:\n",
    "            start_date, end_date = get_date_range_past_year()\n",
    "        print(f\"Date range: {start_date} to {end_date}\")\n",
    "        \n",
    "        # generate search term string for Entrez API query\n",
    "        search_term = build_search_term(start_date, end_date)\n",
    "        # data extraction from API\n",
    "        pmid_list = collect_all_pmids(search_term, config)\n",
    "        # if no new pmids, end the main function and log the run metadata\n",
    "        if not pmid_list:\n",
    "            return create_run_metadata(\"SUCCESS\", start_time, 0, 0)\n",
    "        \n",
    "        records = extract_all_records(pmid_list, config)\n",
    "        \n",
    "        # transformations and normalization\n",
    "        df = create_spark_dataframe(records, spark)\n",
    "        normalized_df = normalize_dataframe(df)\n",
    "        \n",
    "        # depending on whether incremental or bulk, save data to table \n",
    "        # but only save records not already existing in targe table (ensure idempotency)\n",
    "        if config.incremental:\n",
    "            final_df = filter_new_records(normalized_df, \"firedb_pubmed\", spark)\n",
    "            new_records = final_df.count()\n",
    "            if new_records > 0:\n",
    "                save_data(final_df, \"firedb_pubmed\", \"append\")\n",
    "        else:\n",
    "            new_records = normalized_df.count()\n",
    "            save_data(normalized_df, \"firedb_pubmed\", \"overwrite\")\n",
    "        # get new record counts for metadata logging\n",
    "        total_records = normalized_df.count()\n",
    "        \n",
    "        return create_run_metadata(\"SUCCESS\", start_time, total_records, new_records)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ETL failed: {e}\")\n",
    "        # log failure if issue occurs\n",
    "        return create_run_metadata(\"FAILED\", start_time, 0, 0)\n",
    "\n",
    "def create_run_metadata(status: str, start_time: datetime, total: int, new: int) -> RunMetadata:\n",
    "    \"\"\"helper function to create RunMetadata object for logging table.\"\"\"\n",
    "    end_time = datetime.now()\n",
    "    return RunMetadata(\n",
    "        script_name=\"pubmed_ingestion\",\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        incremental_run=False,\n",
    "        total_pubs=total,\n",
    "        new_records=new,\n",
    "        status=status,\n",
    "        duration_seconds=(end_time - start_time).total_seconds())\n",
    "\n",
    "def get_incremental_date_range(spark) -> Tuple[str, str]:\n",
    "    \"\"\"Get date range for incremental run based on last execution.\"\"\"\n",
    "    try:\n",
    "        last_run_df = spark.sql(\"\"\"\n",
    "            SELECT last_run_timestamp\n",
    "            FROM script_run_metadata\n",
    "            WHERE script_name = 'pubmed_ingestion'\n",
    "            ORDER BY last_run_timestamp DESC\n",
    "            LIMIT 1\"\"\")\n",
    "        \n",
    "        last_run_row = last_run_df.collect()\n",
    "        if last_run_row:\n",
    "            start_date = last_run_row[0]['last_run_timestamp'].strftime('%Y/%m/%d')\n",
    "            end_date = datetime.now().strftime('%Y/%m/%d')\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            return get_date_range_past_year()\n",
    "    except Exception:\n",
    "        return get_date_range_past_year()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # spark initialization\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    config_parser.read('.pubmedcfg')\n",
    "    config = ETLConfig(\n",
    "        email=PubmedConfig.EMAIL,\n",
    "        incremental=PubmedConfig.INCREMENTAL,\n",
    "        batch_size=PubmedConfig.BATCH_SIZE,\n",
    "        target_count=PubmedConfig.TARGET_COUNT)\n",
    "    \n",
    "    # main ETL\n",
    "    metadata = run_pubmed_etl(config, spark)\n",
    "    # run metadata\n",
    "    save_run_metadata(metadata, spark)\n",
    "    \n",
    "    # logs...\n",
    "    print(f\"ETL completed with status: {metadata.status}\")\n",
    "    print(f\"Total records: {metadata.total_pubs}, New records: {metadata.new_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3851c1b7-a17f-4fdf-9b32-9fb53bd2c4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pubmed_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
