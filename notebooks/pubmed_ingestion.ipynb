{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953bd8c8-afa0-4957-b0ff-c839531f7910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from enum import Enum\n",
    "import time\n",
    "import configparser\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import json\n",
    "from Bio import Entrez, Medline\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (\n",
    "    col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf, explode)\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType, \n",
    "    IntegerType, FloatType, ArrayType)\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# New imports for relationship extraction\n",
    "import spacy\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "import mlflow\n",
    "\n",
    "# import modules\n",
    "PROJECT_ROOT = '/Workspace/Repos/firedb/fire-db'\n",
    "MODULES_PATH = os.path.join(PROJECT_ROOT, 'modules')\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.insert(0, MODULES_PATH)\n",
    "# MeSH map and outcome keywords used for data extraction and mapping\n",
    "from meshmap import mesh_mapping, outcome_keywords \n",
    "# setup \n",
    "sys.path.append(\"/Workspace/Repos/firedb/fire-db/config/\")\n",
    "from pubmed_config import PubmedConfig\n",
    "config_parser = configparser.ConfigParser()\n",
    "\n",
    "'''\n",
    "Note that this ETL job is designed to run on a single-node cluster with the following specifications,\n",
    "    per Databricks Free Edition's limitations:\n",
    "        - Limited to serverless compute only (no custom Spark configurations)\n",
    "        - Small cluster size\n",
    "        - Max of 5 concurrent tasks\n",
    "'''\n",
    "\n",
    "# Load SpaCy model globally for efficiency\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model not found. Installing...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# configuration & data class definitions\n",
    "@dataclass\n",
    "class ETLConfig:\n",
    "    email: str\n",
    "    max_tries: int = 10\n",
    "    sleep_between_tries: int = 20\n",
    "    batch_size: int = 1000\n",
    "    target_count: int = 20000\n",
    "    incremental: bool = False\n",
    "    max_records_per_query: int = 9999  # PubMed's limit\n",
    "\n",
    "@dataclass\n",
    "class RunMetadata:\n",
    "    script_name: str\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    incremental_run: bool\n",
    "    total_pubs: int\n",
    "    new_records: int\n",
    "    status: str\n",
    "    duration_seconds: float\n",
    "\n",
    "@dataclass\n",
    "class RelationshipTriplet:\n",
    "    \"\"\"Represents an exercise-outcome relationship triplet\"\"\"\n",
    "    exercise_modality: str\n",
    "    relationship_type: str  # e.g., \"decreases\", \"increases\", \"improves\"\n",
    "    outcome: str\n",
    "    confidence_score: float\n",
    "    source_sentence: str\n",
    "\n",
    "class ETLStatus(Enum):\n",
    "    SUCCESS = \"SUCCESS\"\n",
    "    FAILED = \"FAILED\"\n",
    "    PARTIAL = \"PARTIAL\"\n",
    "    NO_DATA = \"NO_DATA\"\n",
    "\n",
    "# BUSINESS LOGIC FUNCTION DEFINITIONS\n",
    "def setup_logging():\n",
    "    \"\"\"configure logging for ETL\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler('pubmed_etl.log')])\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def get_date_range_past_year() -> Tuple[str, str]:\n",
    "    \"\"\"calculate date range for the past year in appropriate format for Entrez query.\"\"\"\n",
    "    today = datetime.today()\n",
    "    one_year_ago = today - relativedelta(months=12)\n",
    "    return (one_year_ago.strftime(\"%Y/%m/%d\"), today.strftime(\"%Y/%m/%d\"))\n",
    "\n",
    "def build_mesh_query() -> str:\n",
    "    \"\"\"build the MeSH terms query string for Pubmed API.\"\"\"\n",
    "    mesh_terms = [\n",
    "        '\"Exercise\"[MeSH Terms]',\n",
    "        '\"Physical Conditioning, Human\"[MeSH Terms]',\n",
    "        '\"Resistance Training\"[MeSH Terms]',\n",
    "        '\"Aerobic Exercise\"[MeSH Terms]',\n",
    "        '\"Circuit-Based Exercise\"[MeSH Terms]',\n",
    "        '\"Periodization\"[MeSH Terms]',\n",
    "        '\"Weight Lifting\"[MeSH Terms]',\n",
    "        '\"High-Intensity Interval Training\"[MeSH Terms]',\n",
    "        '\"Plyometric Exercise\"[MeSH Terms]',\n",
    "        '\"Endurance Training\"[MeSH Terms]',\n",
    "        '\"Muscle Stretching Exercises\"[MeSH Terms]',\n",
    "        '\"Physical Fitness\"[MeSH Terms]',\n",
    "        '\"Cardiorespiratory Fitness\"[MeSH Terms]',\n",
    "        '\"Exercise Therapy\"[MeSH Terms]',\n",
    "        '\"Walking\"[MeSH Terms]',\n",
    "        '\"Swimming\"[MeSH Terms]',\n",
    "        '\"Gymnastics\"[MeSH Terms]',\n",
    "        '\"Bicycling\"[MeSH Terms]']\n",
    "    return ' OR '.join(mesh_terms)\n",
    "\n",
    "def build_search_term(start_date: str, end_date: str) -> str:\n",
    "    \"\"\"build complete PubMed search term with MeSH terms and date range.\"\"\"\n",
    "    mesh_query = build_mesh_query()\n",
    "    return f'({mesh_query}) AND (\"{start_date}\"[Date - Publication] : \"{end_date}\"[Date - Publication])'\n",
    "\n",
    "def partition_date_range(start_date_str: str, end_date_str: str, num_partitions: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\"partition date range used to query Pubmed into smaller chunks.\n",
    "    - enables bypassing the default limit of 10k requests.\"\"\"\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y/%m/%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y/%m/%d\")\n",
    "    \n",
    "    total_days = (end_date - start_date).days\n",
    "    days_per_partition = max(1, total_days // num_partitions)\n",
    "    \n",
    "    partitions = []\n",
    "    current_start = start_date\n",
    "    \n",
    "    for i in range(num_partitions):\n",
    "        if i == num_partitions - 1:\n",
    "            # last partition gets any remaining days...\n",
    "            current_end = end_date\n",
    "        else:\n",
    "            current_end = current_start + timedelta(days=days_per_partition)\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "        \n",
    "        partitions.append((\n",
    "            current_start.strftime(\"%Y/%m/%d\"),\n",
    "            current_end.strftime(\"%Y/%m/%d\")))\n",
    "        \n",
    "        current_start = current_end + timedelta(days=1)\n",
    "        if current_start > end_date:\n",
    "            break\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "def extract_doi(record: Dict) -> Optional[str]:\n",
    "    \"\"\"extract DOI from PubMed record - used in matching ETL\"\"\"\n",
    "    if \"AID\" not in record:\n",
    "        return None\n",
    "    \n",
    "    for aid in record[\"AID\"]:\n",
    "        if \"doi\" in aid.lower():\n",
    "            return aid.replace(\" [doi]\", \"\")\n",
    "    return None\n",
    "\n",
    "def parse_date_fallback(date_str: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"parse date string using dateparser as fallback, only if default parser unable to find a date.\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    parsed = dateparser.parse(date_str)\n",
    "    return parsed.strftime(\"%Y-%m-%d\") if parsed else None\n",
    "\n",
    "def extract_sample_size(text: str) -> int:\n",
    "    \"\"\"extract sample size of cohort/study from abstract using regex.\"\"\"\n",
    "    match = re.search(r\"(?:N\\s*=\\s*|sample size of\\s+|total of\\s+)(\\d{2,5})\", text or \"\", re.IGNORECASE)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def build_synonym_lookup(mesh_map: dict) -> Dict[str, str]:\n",
    "    \"\"\"flatten mesh_mapping to a dict {synonym_or_term_lower: canonical_term} for fast lookup.\"\"\"\n",
    "    lookup = {}\n",
    "\n",
    "    def recurse(term, node):\n",
    "        # map the canonical term to itself (mesh)\n",
    "        lookup[term.lower()] = term\n",
    "        # map synonyms to the canonical term then recurse into subterms\n",
    "        for syn in node.get(\"synonyms\", []):\n",
    "            lookup[syn.lower()] = term\n",
    "        for subterm, subnode in node.get(\"subterms\", {}).items():\n",
    "            recurse(subterm, subnode)\n",
    "\n",
    "    for top_term, node in mesh_map.items():\n",
    "        recurse(top_term, node)\n",
    "    return lookup\n",
    "\n",
    "def extract_outcomes(text: str) -> List[str]:\n",
    "    \"\"\"match known clinical outcomes from abstract\"\"\"\n",
    "    return [term for term in outcome_keywords if term.lower() in (text or \"\").lower()]\n",
    "\n",
    "def extract_modalities(text):\n",
    "    \"\"\"extract known exercise modalities from abstract\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    found_modalities = []\n",
    "    for mesh_term, synonyms in synonym_lookup.items():\n",
    "        for term in synonyms:\n",
    "            if term in text_lower:\n",
    "                found_modalities.append(mesh_term)\n",
    "                break \n",
    "    return list(set(found_modalities))\n",
    "\n",
    "# RELATIONSHIP EXTRACTION FUNCTIONS\n",
    "def get_llama_client():\n",
    "    \"\"\"Initialize LLaMA client for Databricks\"\"\"\n",
    "    try:\n",
    "        # Databricks Foundation Models API\n",
    "        import mlflow.deployments\n",
    "        client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not initialize LLaMA client: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_relationships_spacy(text: str, modalities: List[str], outcomes: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract exercise-outcome relationships using SpaCy NLP\n",
    "    \"\"\"\n",
    "    if not text or not modalities or not outcomes:\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    \n",
    "    # Define relationship verbs that indicate exercise-outcome connections\n",
    "    relationship_verbs = [\n",
    "        \"increase\", \"increases\", \"increased\", \"improve\", \"improves\", \"improved\",\n",
    "        \"decrease\", \"decreases\", \"decreased\", \"reduce\", \"reduces\", \"reduced\", \n",
    "        \"enhance\", \"enhances\", \"enhanced\", \"boost\", \"boosts\", \"boosted\",\n",
    "        \"lower\", \"lowers\", \"lowered\", \"raise\", \"raises\", \"raised\",\n",
    "        \"affect\", \"affects\", \"affected\", \"influence\", \"influences\", \"influenced\"\n",
    "        \"lift\", \"lifts\", \"lifted\", \"elevate\", \"elevates\", \"elevated\", \"lift\", \"lifts\", \"support\", \"supported\", \n",
    "        \"supports\", \"encourage\", \"encourages\", \"encouraged\", \"extend\", \"extends\", \"extended\",\n",
    "        \"multiply\", \"multiplies\", \"multiplied\", \"surpress\", \"surpresses\", \"surpressed\",\n",
    "        \"upregulate\", \"upregulates\", \"upregulated\", \"abate\", \"abates\", \"abated\",\n",
    "        \"curve\", \"curves\", \"curved\", \"weaken\", \"weakened\", \"weakens\", \"strengthen\", \"strengthened\",\n",
    "        \"strengthens\", \"lessened\", \"lessened\", \"lessens\", \"heighten\", \"heightened\", \"heightens\",\n",
    "        \"diminish\", \"diminishes\", \"diminished\"]\n",
    "    \n",
    "    # Look for sentences containing both modalities and outcomes\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.lower()\n",
    "        \n",
    "        # Find modalities and outcomes in this sentence\n",
    "        sent_modalities = [mod for mod in modalities if mod.lower() in sent_text]\n",
    "        sent_outcomes = [out for out in outcomes if out.lower() in sent_text]\n",
    "        \n",
    "        if sent_modalities and sent_outcomes:\n",
    "            # Look for relationship verbs\n",
    "            for token in sent:\n",
    "                if token.lemma_ in [v.rstrip('sd') for v in relationship_verbs]:\n",
    "                    for modality in sent_modalities:\n",
    "                        for outcome in sent_outcomes:\n",
    "                            relationships.append({\n",
    "                                'exercise_modality': modality,\n",
    "                                'relationship_type': token.lemma_,\n",
    "                                'outcome': outcome,\n",
    "                                'confidence_score': 0.7,  # Base confidence for SpaCy extraction\n",
    "                                'source_sentence': sent.text.strip(),\n",
    "                                'method': 'spacy'\n",
    "                            })\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "def extract_relationships_llama(text: str, modalities: List[str], outcomes: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract exercise-outcome relationships using LLaMA model\n",
    "    \"\"\"\n",
    "    if not text or not modalities or not outcomes:\n",
    "        return []\n",
    "    \n",
    "    client = get_llama_client()\n",
    "    if not client:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Prepare prompt for LLaMA\n",
    "        prompt = f\"\"\"\n",
    "        Extract exercise-outcome relationships from the following research abstract.\n",
    "        \n",
    "        Available exercise modalities: {', '.join(modalities)}\n",
    "        Available outcomes: {', '.join(outcomes)}\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        For each relationship found, return a JSON object with:\n",
    "        - exercise_modality: the specific exercise type\n",
    "        - relationship_type: how the exercise affects the outcome (increases, decreases, improves, etc.)\n",
    "        - outcome: the health/fitness outcome\n",
    "        - confidence_score: confidence level (0.0-1.0)\n",
    "        - source_sentence: the sentence containing the relationship\n",
    "        \n",
    "        Return only valid JSON objects, one per line. If no relationships found, return empty response.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.predict(\n",
    "            endpoint=\"databricks-meta-llama-3-3-70b-instruct\", \n",
    "            inputs={\"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n",
    "        )\n",
    "        \n",
    "        # Parse LLaMA response\n",
    "        relationships = []\n",
    "        if response and 'predictions' in response:\n",
    "            for line in response['predictions'][0].split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and line.startswith('{'):\n",
    "                    try:\n",
    "                        rel = json.loads(line)\n",
    "                        rel['method'] = 'llama'\n",
    "                        relationships.append(rel)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        \n",
    "        return relationships\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"LLaMA extraction failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def combine_relationship_extractions(spacy_rels: List[Dict], llama_rels: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combine and deduplicate relationships from SpaCy and LLaMA\n",
    "    \"\"\"\n",
    "    all_relationships = []\n",
    "    seen = set()\n",
    "    \n",
    "    # Add SpaCy relationships\n",
    "    for rel in spacy_rels:\n",
    "        key = (rel['exercise_modality'].lower(), rel['outcome'].lower(), rel['relationship_type'].lower())\n",
    "        if key not in seen:\n",
    "            all_relationships.append(rel)\n",
    "            seen.add(key)\n",
    "    \n",
    "    # Add LLaMA relationships, boosting confidence if they match SpaCy findings\n",
    "    for rel in llama_rels:\n",
    "        key = (rel['exercise_modality'].lower(), rel['outcome'].lower(), rel['relationship_type'].lower())\n",
    "        if key in seen:\n",
    "            # Find matching SpaCy relationship and boost confidence\n",
    "            for existing_rel in all_relationships:\n",
    "                existing_key = (existing_rel['exercise_modality'].lower(), \n",
    "                              existing_rel['outcome'].lower(), \n",
    "                              existing_rel['relationship_type'].lower())\n",
    "                if existing_key == key:\n",
    "                    existing_rel['confidence_score'] = min(1.0, existing_rel['confidence_score'] + 0.2)\n",
    "                    existing_rel['method'] = 'spacy+llama'\n",
    "                    break\n",
    "        else:\n",
    "            all_relationships.append(rel)\n",
    "            seen.add(key)\n",
    "    \n",
    "    return all_relationships\n",
    "\n",
    "def extract_all_relationships(text: str, modalities: List[str], outcomes: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Master function to extract relationships using both SpaCy and LLaMA\n",
    "    \"\"\"\n",
    "    spacy_relationships = extract_relationships_spacy(text, modalities, outcomes)\n",
    "    llama_relationships = extract_relationships_llama(text, modalities, outcomes)\n",
    "    \n",
    "    return combine_relationship_extractions(spacy_relationships, llama_relationships)\n",
    "\n",
    "# DATA PROCESSING/TRANSFORMATION FUNCTIONS\n",
    "def transform_record(record: Dict) -> Dict:\n",
    "    \"\"\"normalize individual PubMed records into dictionary with enrichment.\"\"\"\n",
    "    abstract = record.get(\"AB\", \"\")\n",
    "    title = record.get(\"TI\", \"\")\n",
    "    full_text = f\"{title} {abstract}\"\n",
    "    \n",
    "    # Extract basic features\n",
    "    modalities = extract_modalities(full_text)\n",
    "    outcomes = extract_outcomes(full_text)\n",
    "    \n",
    "    # Extract relationships\n",
    "    relationships = extract_all_relationships(full_text, modalities, outcomes)\n",
    "    \n",
    "    return {\n",
    "        \"pmid\": record.get(\"PMID\"),\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"journal\": record.get(\"JT\"),\n",
    "        \"date\": record.get(\"DP\"),\n",
    "        \"doi\": extract_doi(record),\n",
    "        \"mesh_terms\": record.get(\"MH\", []),\n",
    "        \"publication_types\": record.get(\"PT\", []),\n",
    "        \"keywords\": record.get(\"OT\", []),\n",
    "        \"n_size\": extract_sample_size(abstract),\n",
    "        \"outcomes\": outcomes,\n",
    "        \"modalities\": modalities,\n",
    "        \"relationships\": relationships  # New field for relationship triplets\n",
    "    }\n",
    "\n",
    "def normalize_dataframe(df):\n",
    "    \"\"\"apply normalization transformations to df and apply basic data quality filters\"\"\"\n",
    "    parse_date_udf = udf(parse_date_fallback, StringType())\n",
    "    \n",
    "    return (df\n",
    "            .withColumn(\"abstract\", regexp_replace(col(\"abstract\"), \"\\n\", \" \"))\n",
    "            .withColumn(\"title\", regexp_replace(trim(col(\"title\")), \"\\n\", \"\"))\n",
    "            .withColumn(\"pub_date\", parse_date_udf(col(\"date\")))\n",
    "            .drop(\"date\")\n",
    "            .filter((col(\"doi\").isNotNull()) & (trim(col(\"doi\")) != \"\") & (col(\"pub_date\").isNotNull()))\n",
    "            .dropDuplicates([\"doi\"]))\n",
    "\n",
    "def filter_new_records(normalized_df, existing_table_name: str, spark):\n",
    "    \"\"\"filter out records that already exist in the target table -- enables incremental loading.\"\"\"\n",
    "    try:\n",
    "        existing_df = spark.table(existing_table_name)\n",
    "        return normalized_df.join(existing_df.select(\"doi\"), on=\"doi\", how=\"left_anti\")\n",
    "    except Exception:\n",
    "        # validate table exists...\n",
    "        return normalized_df\n",
    "\n",
    "# ENTREZ API/BIOPYTHON EXTRACTION FUNCTIONS\n",
    "def fetch_with_retry(fetch_func, max_tries: int = 5, sleep_time: int = 20):\n",
    "    \"\"\" function to fetch data from Pubmed w/ retry wrapper and error reporting\"\"\"\n",
    "    last_exception = None\n",
    "    \n",
    "    for attempt in range(max_tries):\n",
    "        try:\n",
    "            return fetch_func()\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            if \"HTTP Error 429\" in error_msg and attempt < max_tries - 1:\n",
    "                wait_time = sleep_time * (2 ** attempt)  # Exponential backoff\n",
    "                logger.warning(f\"Rate limit hit! Retrying in {wait_time}s (attempt {attempt + 1}/{max_tries})\")\n",
    "                time.sleep(wait_time)\n",
    "            elif attempt < max_tries - 1:\n",
    "                logger.warning(f\"Request failed! {error_msg}. Retrying... (attempt {attempt + 1}/{max_tries})\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                logger.error(f\"All retry attempts failed! Last error: {error_msg}\")\n",
    "                raise last_exception\n",
    "    \n",
    "    raise last_exception\n",
    "\n",
    "def get_total_count(search_term: str) -> int:\n",
    "    \"\"\"get total count of records matching search term.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=0)\n",
    "        return int(Entrez.read(handle)[\"Count\"])\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_pmid_batch(search_term: str, start: int, batch_size: int) -> List[str]:\n",
    "    \"\"\"fetch a batch of PMIDs given a batch size and starting index.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=search_term,\n",
    "            retmax=batch_size,\n",
    "            retstart=start)\n",
    "        return Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_records_batch(pmids: List[str]) -> List[Dict]:\n",
    "    \"\"\"fetch detailed records for a batch of PMIDs.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"text\")\n",
    "        return list(Medline.parse(handle))\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "# ORCHESTRATION FUNCTIONS\n",
    "def collect_pmids_for_date_range(start_date: str, end_date: str, config: ETLConfig, remaining_target: int = None) -> List[str]:\n",
    "    \"\"\"collect PMIDs for a specific date range, handling the 9999 limit.\"\"\"\n",
    "    search_term = build_search_term(start_date, end_date)\n",
    "    total_count = get_total_count(search_term)\n",
    "    \n",
    "    print(f\"Date range {start_date} to {end_date}: {total_count} records\")\n",
    "    effective_target = remaining_target if remaining_target is not None else config.target_count\n",
    "    \n",
    "    if total_count > config.max_records_per_query: # check if limit hit\n",
    "        print(f\"Records ({total_count}) exceed limit ({config.max_records_per_query}). Partitioning date range...\")\n",
    "        \n",
    "        # calculate number of partitions needed\n",
    "        num_partitions = (total_count // config.max_records_per_query) + 1\n",
    "        partitions = partition_date_range(start_date, end_date, num_partitions)\n",
    "        \n",
    "        all_pmids = []\n",
    "        for partition_start, partition_end in partitions:\n",
    "            remaining_needed = effective_target - len(all_pmids)\n",
    "            if remaining_needed <= 0:\n",
    "                break\n",
    "                \n",
    "            partition_pmids = collect_pmids_for_date_range(partition_start, partition_end, config, remaining_needed)\n",
    "            all_pmids.extend(partition_pmids)\n",
    "        \n",
    "        return all_pmids\n",
    "    \n",
    "    # if under the limit, proceed with normal collection\n",
    "    target_count = min(effective_target, total_count)\n",
    "    pmid_list = []\n",
    "    \n",
    "    for start in range(0, target_count, config.batch_size):\n",
    "        remaining_in_batch = min(config.batch_size, target_count - len(pmid_list))\n",
    "        batch_pmids = fetch_pmid_batch(search_term, start, remaining_in_batch)\n",
    "        pmid_list.extend(batch_pmids)\n",
    "        print(f\"Fetched {len(pmid_list)} PMIDs so far for range {start_date} to {end_date}...\")\n",
    "        \n",
    "        if len(pmid_list) >= target_count:\n",
    "            break\n",
    "    \n",
    "    return pmid_list\n",
    "\n",
    "def collect_all_pmids(search_term: str, config: ETLConfig, start_date: str, end_date: str) -> List[str]:\n",
    "    \"\"\"collect all PMIDs matching the search criteria with automatic date partitioning.\"\"\"\n",
    "    # first, get the total count to make informed decisions\n",
    "    total_available = get_total_count(search_term)\n",
    "    print(f\"Total records available: {total_available}\")\n",
    "    print(f\"Target count configured: {config.target_count}\")\n",
    "    \n",
    "    # then, determine how many to actually collect\n",
    "    effective_target = min(config.target_count, total_available)\n",
    "    print(f\"Will collect: {effective_target} records\")\n",
    "    \n",
    "    return collect_pmids_for_date_range(start_date, end_date, config, effective_target)\n",
    "\n",
    "def extract_all_records(pmid_list: List[str], config: ETLConfig) -> List[Dict]:\n",
    "    \"\"\"extract detailed records for all PMIDs.\"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    for i in range(0, len(pmid_list), config.batch_size):\n",
    "        batch_pmids = pmid_list[i:i + config.batch_size]\n",
    "        try:\n",
    "            records = fetch_records_batch(batch_pmids)\n",
    "            transformed_records = [transform_record(rec) for rec in records]\n",
    "            all_records.extend(transformed_records)\n",
    "            print(f\"Processed {len(all_records)} records so far...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_records\n",
    "\n",
    "def create_spark_dataframe(records: List[Dict], spark) -> 'DataFrame':\n",
    "    \"\"\"Convert records list to PySpark df with complete schema including relationships.\"\"\"\n",
    "    # Define relationship schema\n",
    "    relationship_schema = StructType([\n",
    "        StructField(\"exercise_modality\", StringType(), True),\n",
    "        StructField(\"relationship_type\", StringType(), True),\n",
    "        StructField(\"outcome\", StringType(), True),\n",
    "        StructField(\"confidence_score\", FloatType(), True),\n",
    "        StructField(\"source_sentence\", StringType(), True),\n",
    "        StructField(\"method\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    if not records:\n",
    "        schema = StructType([\n",
    "            StructField(\"pmid\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"abstract\", StringType(), True),\n",
    "            StructField(\"journal\", StringType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"doi\", StringType(), True),\n",
    "            StructField(\"mesh_terms\", ArrayType(StringType()), True),\n",
    "            StructField(\"publication_types\", ArrayType(StringType()), True),\n",
    "            StructField(\"keywords\", ArrayType(StringType()), True),\n",
    "            StructField(\"n_size\", IntegerType(), True),\n",
    "            StructField(\"outcomes\", ArrayType(StringType()), True),\n",
    "            StructField(\"modalities\", ArrayType(StringType()), True),\n",
    "            StructField(\"relationships\", ArrayType(relationship_schema), True)\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema)\n",
    "    \n",
    "    rows = [Row(**record) for record in records]\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def create_relationships_table(df, spark):\n",
    "    \"\"\"Create a separate table for exercise-outcome relationships\"\"\"\n",
    "    relationships_df = (df\n",
    "                       .select(\"pmid\", \"doi\", explode(\"relationships\").alias(\"relationship\"))\n",
    "                       .select(\"pmid\", \"doi\",\n",
    "                              col(\"relationship.exercise_modality\").alias(\"exercise_modality\"),\n",
    "                              col(\"relationship.relationship_type\").alias(\"relationship_type\"), \n",
    "                              col(\"relationship.outcome\").alias(\"outcome\"),\n",
    "                              col(\"relationship.confidence_score\").alias(\"confidence_score\"),\n",
    "                              col(\"relationship.source_sentence\").alias(\"source_sentence\"),\n",
    "                              col(\"relationship.method\").alias(\"extraction_method\")))\n",
    "    \n",
    "    # Save relationships table\n",
    "    save_data(relationships_df, \"firedb_exercise_outcome_relationships\", \"overwrite\")\n",
    "    return relationships_df\n",
    "\n",
    "def save_run_metadata(metadata: RunMetadata, spark):\n",
    "    \"\"\"store run metadata to tracking table.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"script_name\", StringType(), False),\n",
    "        StructField(\"last_run_timestamp\", TimestampType(), False),\n",
    "        StructField(\"incremental_run\", StringType(), False),\n",
    "        StructField(\"total_pubs\", IntegerType(), True),\n",
    "        StructField(\"new_records\", IntegerType(), True),\n",
    "        StructField(\"run_status\", StringType(), True),\n",
    "        StructField(\"run_start\", TimestampType(), True),\n",
    "        StructField(\"run_end\", TimestampType(), True),\n",
    "        StructField(\"duration_seconds\", FloatType(), True)])\n",
    "    \n",
    "    row_data = [(\n",
    "        metadata.script_name,\n",
    "        metadata.end_time,\n",
    "        str(metadata.incremental_run),\n",
    "        metadata.total_pubs,\n",
    "        metadata.new_records,\n",
    "        metadata.status,\n",
    "        metadata.start_time,\n",
    "        metadata.end_time,\n",
    "        metadata.duration_seconds)]\n",
    "    \n",
    "    metadata_df = spark.createDataFrame(row_data, schema=schema)\n",
    "    metadata_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"script_run_metadata\")\n",
    "\n",
    "def create_run_metadata(status: str, start_time: datetime, total: int, new: int) -> RunMetadata:\n",
    "    \"\"\"helper function to create RunMetadata object for logging table.\"\"\"\n",
    "    end_time = datetime.now()\n",
    "    return RunMetadata(\n",
    "        script_name=\"pubmed_ingestion\",\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        incremental_run=False,\n",
    "        total_pubs=total,\n",
    "        new_records=new,\n",
    "        status=status,\n",
    "        duration_seconds=(end_time - start_time).total_seconds())\n",
    "\n",
    "def get_incremental_date_range(spark) -> Tuple[str, str]:\n",
    "    \"\"\"Get date range for incremental run based on last execution.\"\"\"\n",
    "    try:\n",
    "        last_run_df = spark.sql(\"\"\"\n",
    "            SELECT last_run_timestamp\n",
    "            FROM script_run_metadata\n",
    "            WHERE script_name = 'pubmed_ingestion'\n",
    "            ORDER BY last_run_timestamp DESC\n",
    "            LIMIT 1\"\"\")\n",
    "        \n",
    "        last_run_row = last_run_df.collect()\n",
    "        if last_run_row:\n",
    "            start_date = last_run_row[0]['last_run_timestamp'].strftime('%Y/%m/%d')\n",
    "            end_date = datetime.now().strftime('%Y/%m/%d')\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            return get_date_range_past_year()\n",
    "    except Exception:\n",
    "        return get_date_range_past_year()\n",
    "\n",
    "def generate_relationship_summary(relationships_df, spark):\n",
    "    \"\"\"Generate summary statistics for extracted relationships\"\"\"\n",
    "    summary_stats = relationships_df.agg(\n",
    "        {\"confidence_score\": \"avg\", \"*\": \"count\"}\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Relationship Extraction Summary:\")\n",
    "    print(f\"Total relationships extracted: {summary_stats[1]}\")\n",
    "    print(f\"Average confidence score: {summary_stats[0]:.3f}\")\n",
    "    \n",
    "    # Top modalities and outcomes\n",
    "    print(\"\\nTop Exercise Modalities:\")\n",
    "    top_modalities = relationships_df.groupBy(\"exercise_modality\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "    top_modalities.show()\n",
    "    \n",
    "    print(\"\\nTop Outcomes:\")\n",
    "    top_outcomes = relationships_df.groupBy(\"outcome\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "    top_outcomes.show()\n",
    "    \n",
    "    print(\"\\nTop Relationship Types:\")\n",
    "    top_relationships = relationships_df.groupBy(\"relationship_type\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "    top_relationships.show()\n",
    "\n",
    "# MAIN EXTRACTION FUNCTION\n",
    "def run_pubmed_etl(config: ETLConfig, spark) -> RunMetadata:\n",
    "    \"\"\"main ETL pipeline with relationship extraction\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(f\"Starting PubMed ETL with relationship extraction. Config: {config}\")\n",
    "    \n",
    "    try:\n",
    "        Entrez.email = config.email\n",
    "        Entrez.max_tries = config.max_tries\n",
    "        Entrez.sleep_between_tries = config.sleep_between_tries\n",
    "        \n",
    "        # generate date range based on incremental vs. bulk load configuration\n",
    "        if config.incremental:\n",
    "            start_date, end_date = get_incremental_date_range(spark)\n",
    "            logger.info(f\"Incremental run: {start_date} to {end_date}\")\n",
    "        else:\n",
    "            start_date, end_date = get_date_range_past_year()\n",
    "            logger.info(f\"Full run: {start_date} to {end_date}\")\n",
    "        \n",
    "        # build search + collect relevant pmids \n",
    "        search_term = build_search_term(start_date, end_date)\n",
    "        logger.info(f\"Search term: {search_term}\")\n",
    "        pmid_list = collect_all_pmids(search_term, config, start_date, end_date)\n",
    "        \n",
    "        if not pmid_list:\n",
    "            logger.info(\"No new PMIDs found\")\n",
    "            return create_run_metadata(ETLStatus.NO_DATA.value, start_time, 0, 0)\n",
    "        logger.info(f\"Collected {len(pmid_list)} PMIDs\") \n",
    "        \n",
    "        # extract + transform with relationship extraction\n",
    "        logger.info(\"Starting record extraction and relationship mining...\")\n",
    "        records = extract_all_records(pmid_list, config)\n",
    "        logger.info(f\"Extracted {len(records)} records with relationships\")\n",
    "\n",
    "        # process w/ spark function then normalize\n",
    "        df = create_spark_dataframe(records, spark)\n",
    "        normalized_df = normalize_dataframe(df)\n",
    "        \n",
    "        # Create separate relationships table\n",
    "        logger.info(\"Creating relationships table...\")\n",
    "        relationships_df = create_relationships_table(normalized_df, spark)\n",
    "        \n",
    "        # Generate relationship summary\n",
    "        generate_relationship_summary(relationships_df, spark)\n",
    "        \n",
    "        # handle incremental vs full loading logic\n",
    "        if config.incremental:\n",
    "            final_df = filter_new_records(normalized_df, \"firedb_pubmed\", spark)\n",
    "            new_records = final_df.count()\n",
    "            if new_records > 0:\n",
    "                save_data(final_df, \"firedb_pubmed\", \"append\")\n",
    "                logger.info(f\"Appended {new_records} new records\")\n",
    "        else:\n",
    "            new_records = normalized_df.count()\n",
    "            save_data(normalized_df, \"firedb_pubmed\", \"overwrite\")\n",
    "            logger.info(f\"Saved {new_records} records (full refresh)\")\n",
    "        \n",
    "        total_records = normalized_df.count()\n",
    "        total_relationships = relationships_df.count()\n",
    "        \n",
    "        logger.info(f\"ETL completed successfully:\")\n",
    "        logger.info(f\"  - Total publications: {total_records}\")\n",
    "        logger.info(f\"  - Total relationships extracted: {total_relationships}\")\n",
    "        logger.info(f\"  - Average relationships per publication: {total_relationships/total_records:.2f}\")\n",
    "        \n",
    "        metadata = create_run_metadata(ETLStatus.SUCCESS.value, start_time, total_records, new_records)\n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ETL failed with error: {str(e)}\", exc_info=True)\n",
    "        return create_run_metadata(ETLStatus.FAILED.value, start_time, 0, 0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # spark initialization\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    logger = setup_logging()\n",
    "\n",
    "    config = ETLConfig(\n",
    "        email=PubmedConfig.EMAIL,\n",
    "        incremental=PubmedConfig.INCREMENTAL,\n",
    "        batch_size=PubmedConfig.BATCH_SIZE,\n",
    "        target_count=PubmedConfig.TARGET_COUNT)\n",
    "    \n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"Email: {PubmedConfig.EMAIL}\")\n",
    "    print(f\"Incremental: {PubmedConfig.INCREMENTAL}\")\n",
    "    print(f\"Batch Size: {PubmedConfig.BATCH_SIZE}\")\n",
    "    print(f\"Target Count: {PubmedConfig.TARGET_COUNT}\")\n",
    "    \n",
    "    # Build synonym lookup for modality extraction\n",
    "    synonym_lookup = build_synonym_lookup(mesh_mapping)\n",
    "    \n",
    "    print(f\"Loaded {len(synonym_lookup)} exercise modality synonyms\")\n",
    "    print(f\"Loaded {len(outcome_keywords)} outcome keywords\")\n",
    "\n",
    "    # main ETL with relationship extraction\n",
    "    metadata = run_pubmed_etl(config, spark)\n",
    "    \n",
    "    # save run metadata\n",
    "    save_run_metadata(metadata, spark)\n",
    "    \n",
    "    # final logs\n",
    "    print(f\"\\n=== ETL COMPLETED ===\")\n",
    "    print(f\"Status: {metadata.status}\")\n",
    "    print(f\"Total records: {metadata.total_pubs}\")\n",
    "    print(f\"New records: {metadata.new_records}\")\n",
    "    print(f\"Duration: {metadata.duration_seconds:.1f} seconds\")\n",
    "    \n",
    "    sql_query = \"\"\"\n",
    "        SELECT exercise_modality, relationship_type, outcome, confidence_score, extraction_method\n",
    "        FROM firedb_exercise_outcome_relationships \n",
    "        WHERE confidence_score > 0.8 \n",
    "        ORDER BY confidence_score DESC \n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "    try:\n",
    "        sample_relationships = spark.sql(sql_query)\n",
    "        print(f\"\\n=== TOP CONFIDENCE RELATIONSHIPS ===\")\n",
    "        sample_relationships.show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display sample relationships: {e}\")\n",
    "        \n",
    "    # Display relationship statistics by method\n",
    "    sql_query = \"\"\"\n",
    "            SELECT extraction_method,\n",
    "                   COUNT(*) as total_relationships,\n",
    "                   AVG(confidence_score) as avg_confidence,\n",
    "                   COUNT(DISTINCT exercise_modality) as unique_modalities,\n",
    "                   COUNT(DISTINCT outcome) as unique_outcomes\n",
    "            FROM firedb_exercise_outcome_relationships \n",
    "            GROUP BY extraction_method\n",
    "            ORDER BY total_relationships DESC\n",
    "        \"\"\"\n",
    "    try:\n",
    "        method_stats = spark.sql(sql_query)\n",
    "        print(f\"\\n=== EXTRACTION METHOD STATISTICS ===\")\n",
    "        method_stats.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display method statistics: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pubmed_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
