{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64151c2-66a6-4f4f-9174-3d228cec2530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import configparser\n",
    "from Bio import Entrez, Medline\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# read config info for run to import config module\n",
    "config_parser = configparser.ConfigParser()\n",
    "sys.path.append(\"/Workspace/Repos/firedb/fire-db/config/\")\n",
    "from pubmed_config import PubmedConfig\n",
    "\n",
    "# global variables\n",
    "OUTCOME_KEYWORDS = [\n",
    "    \"blood pressure\", \"LDL\", \"HDL\", \"total cholesterol\", \"triglycerides\",\n",
    "    \"insulin sensitivity\", \"glucose\", \"HbA1c\", \"fasting glucose\",\n",
    "    \"BMI\", \"waist circumference\", \"body fat percentage\", \"bone density\",\n",
    "    \"resting heart rate\", \"heart rate variability\",\n",
    "    \"muscle strength\", \"maximal strength\", \"grip strength\",\n",
    "    \"1RM\", \"2RM\", \"3RM\", \"4RM\", \"5RM\", \"6RM\", \"7RM\", \"8RM\", \"9RM\", \"10RM\", \"11RM\", \"12RM\",\n",
    "    \"isometric strength\", \"explosive strength\", \"power output\", \"vertical jump\",\n",
    "    \"VO2 max\", \"aerobic capacity\", \"oxygen consumption\", \"endurance performance\",\n",
    "    \"time to exhaustion\", \"lactate threshold\", \"anaerobic threshold\",\n",
    "    \"cycling time trial\", \"treadmill test\", \"distance run\", \"step test\",\n",
    "    \"speed\", \"reaction time\", \"agility\", \"change of direction\", \"balance\",\n",
    "    \"postural stability\", \"range of motion\", \"flexibility\", \"mobility\",\n",
    "    \"lean mass\", \"fat-free mass\", \"skeletal muscle mass\", \"fat mass\",\n",
    "    \"fatigue\", \"mood\", \"wellbeing\", \"mental health\", \"quality of life\",\n",
    "    \"stress\", \"cognitive function\", \"depression\", \"anxiety\", \"sleep quality\",\n",
    "    \"self-efficacy\", \"motivation\", \"perceived exertion\", \"body image\",\n",
    "    \"appetite\", \"satiety\", \"ghrelin\", \"leptin\", \"cortisol\",\n",
    "    \"testosterone\", \"growth hormone\", \"inflammatory markers\", \"C-reactive protein\",\n",
    "    \"functional capacity\", \"walking speed\", \"gait speed\", \"fall risk\",\n",
    "    \"pain\", \"DOMS\", \"recovery\", \"soreness\", \"inflammation\", \"muscle damage\"]\n",
    "\n",
    "# configuration & data class definitions\n",
    "@dataclass\n",
    "class ETLConfig:\n",
    "    email: str\n",
    "    max_tries: int = 10\n",
    "    sleep_between_tries: int = 20\n",
    "    batch_size: int = 1000\n",
    "    target_count: int = 20000\n",
    "    incremental: bool = False\n",
    "    max_records_per_query: int = 9999  # PubMed's limit\n",
    "\n",
    "@dataclass\n",
    "class RunMetadata:\n",
    "    script_name: str\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    incremental_run: bool\n",
    "    total_pubs: int\n",
    "    new_records: int\n",
    "    status: str\n",
    "    duration_seconds: float\n",
    "\n",
    "# business logic and function definitions\n",
    "def get_date_range_past_year() -> Tuple[str, str]:\n",
    "    \"\"\"calculate date range for the past year in appropriate format for Entrez query.\"\"\"\n",
    "    today = datetime.today()\n",
    "    one_year_ago = today - relativedelta(months=12)\n",
    "    return (one_year_ago.strftime(\"%Y/%m/%d\"), today.strftime(\"%Y/%m/%d\"))\n",
    "\n",
    "def build_mesh_query() -> str:\n",
    "    \"\"\"build the MeSH terms query string.\"\"\"\n",
    "    mesh_terms = [\n",
    "        '\"Exercise\"[MeSH Terms]',\n",
    "        '\"Physical Conditioning, Human\"[MeSH Terms]',\n",
    "        '\"Resistance Training\"[MeSH Terms]',\n",
    "        '\"Aerobic Exercise\"[MeSH Terms]',\n",
    "        '\"High-Intensity Interval Training\"[MeSH Terms]',\n",
    "        '\"Plyometric Exercise\"[MeSH Terms]',\n",
    "        '\"Endurance Training\"[MeSH Terms]',\n",
    "        '\"Muscle Stretching Exercises\"[MeSH Terms]',\n",
    "        '\"Physical Fitness\"[MeSH Terms]',\n",
    "        '\"Cardiorespiratory Fitness\"[MeSH Terms]',\n",
    "        '\"Exercise Therapy\"[MeSH Terms]',\n",
    "        '\"Walking\"[MeSH Terms]',\n",
    "        '\"Swimming\"[MeSH Terms]',\n",
    "        '\"Gymnastics\"[MeSH Terms]'\n",
    "    ]\n",
    "    return ' OR '.join(mesh_terms)\n",
    "\n",
    "def build_search_term(start_date: str, end_date: str) -> str:\n",
    "    \"\"\"build complete PubMed search term with MeSH terms and date range.\"\"\"\n",
    "    mesh_query = build_mesh_query()\n",
    "    return f'({mesh_query}) AND (\"{start_date}\"[Date - Publication] : \"{end_date}\"[Date - Publication])'\n",
    "\n",
    "def partition_date_range(start_date_str: str, end_date_str: str, num_partitions: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Partition a date range into smaller chunks.\"\"\"\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y/%m/%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y/%m/%d\")\n",
    "    \n",
    "    total_days = (end_date - start_date).days\n",
    "    days_per_partition = max(1, total_days // num_partitions)\n",
    "    \n",
    "    partitions = []\n",
    "    current_start = start_date\n",
    "    \n",
    "    for i in range(num_partitions):\n",
    "        if i == num_partitions - 1:\n",
    "            # Last partition gets any remaining days\n",
    "            current_end = end_date\n",
    "        else:\n",
    "            current_end = current_start + timedelta(days=days_per_partition)\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "        \n",
    "        partitions.append((\n",
    "            current_start.strftime(\"%Y/%m/%d\"),\n",
    "            current_end.strftime(\"%Y/%m/%d\")\n",
    "        ))\n",
    "        \n",
    "        current_start = current_end + timedelta(days=1)\n",
    "        if current_start > end_date:\n",
    "            break\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "def extract_doi(record: Dict) -> Optional[str]:\n",
    "    \"\"\"extract DOI from PubMed record - used in matching ETL\"\"\"\n",
    "    if \"AID\" not in record:\n",
    "        return None\n",
    "    \n",
    "    for aid in record[\"AID\"]:\n",
    "        if \"doi\" in aid.lower():\n",
    "            return aid.replace(\" [doi]\", \"\")\n",
    "    return None\n",
    "\n",
    "def parse_date_fallback(date_str: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"parse date string using dateparser as fallback, only if default parser unable to parse.\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    parsed = dateparser.parse(date_str)\n",
    "    return parsed.strftime(\"%Y-%m-%d\") if parsed else None\n",
    "\n",
    "def extract_sample_size(text: str) -> int:\n",
    "    \"\"\"Extract sample size from abstract using regex.\"\"\"\n",
    "    match = re.search(r\"(?:N\\s*=\\s*|sample size of\\s+|total of\\s+)(\\d{2,5})\", text or \"\", re.IGNORECASE)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_outcomes(text: str) -> List[str]:\n",
    "    \"\"\"Match known clinical outcomes.\"\"\"\n",
    "    return [term for term in OUTCOME_KEYWORDS if term.lower() in (text or \"\").lower()]\n",
    "\n",
    "# data processing and transformation functions\n",
    "def transform_record(record: Dict) -> Dict:\n",
    "    \"\"\"Normalize individual PubMed records into dictionary with enrichment.\"\"\"\n",
    "    abstract = record.get(\"AB\", \"\")\n",
    "    return {\n",
    "        \"pmid\": record.get(\"PMID\"),\n",
    "        \"title\": record.get(\"TI\"),\n",
    "        \"abstract\": abstract,\n",
    "        \"journal\": record.get(\"JT\"),\n",
    "        \"date\": record.get(\"DP\"),\n",
    "        \"doi\": extract_doi(record),\n",
    "        \"mesh_terms\": record.get(\"MH\", []),\n",
    "        \"publication_types\": record.get(\"PT\", []),\n",
    "        \"keywords\": record.get(\"OT\", []),\n",
    "        \"n_size\": extract_sample_size(abstract),\n",
    "        \"outcomes\": extract_outcomes(abstract)}\n",
    "\n",
    "def normalize_dataframe(df):\n",
    "    \"\"\"apply normalization transformations to DataFrame and apply basic data quality filters\"\"\"\n",
    "    # UDF for date parsing\n",
    "    parse_date_udf = udf(parse_date_fallback, StringType())\n",
    "    \n",
    "    return (df\n",
    "            .withColumn(\"abstract\", regexp_replace(col(\"abstract\"), \"\\n\", \" \"))\n",
    "            .withColumn(\"title\", regexp_replace(trim(col(\"title\")), \"\\n\", \"\"))\n",
    "            .withColumn(\"pub_date\", parse_date_udf(col(\"date\")))\n",
    "            .drop(\"date\")\n",
    "            .filter((col(\"doi\").isNotNull()) & (trim(col(\"doi\")) != \"\") & (col(\"pub_date\").isNotNull()))\n",
    "            .dropDuplicates([\"doi\"]))\n",
    "\n",
    "def filter_new_records(normalized_df, existing_table_name: str, spark):\n",
    "    \"\"\"filter out records that already exist in the target table.\"\"\"\n",
    "    try:\n",
    "        existing_df = spark.table(existing_table_name)\n",
    "        return normalized_df.join(existing_df.select(\"doi\"), on=\"doi\", how=\"left_anti\")\n",
    "    except Exception:\n",
    "        # validate table exists...\n",
    "        return normalized_df\n",
    "\n",
    "# functions to manage interactions with Entrez API (biopython)\n",
    "def fetch_with_retry(fetch_func, max_tries: int = 5, sleep_time: int = 20):\n",
    "    \"\"\"retry wrapper for API calls.\"\"\"\n",
    "    for attempt in range(max_tries):\n",
    "        try:\n",
    "            return fetch_func()\n",
    "        except Exception as e:\n",
    "            if \"HTTP Error 429\" in str(e) and attempt < max_tries - 1:\n",
    "                print(f\"Rate limit exceeded. Retrying after {sleep_time}s... (attempt {attempt + 1})\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise e\n",
    "    raise Exception(f\"Failed after {max_tries} attempts\")\n",
    "\n",
    "def get_total_count(search_term: str) -> int:\n",
    "    \"\"\"get total count of records matching search term.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=0)\n",
    "        return int(Entrez.read(handle)[\"Count\"])\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_pmid_batch(search_term: str, start: int, batch_size: int) -> List[str]:\n",
    "    \"\"\"fetch a batch of PMIDs given a batch size and starting index.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=search_term,\n",
    "            retmax=batch_size,\n",
    "            retstart=start\n",
    "        )\n",
    "        return Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_records_batch(pmids: List[str]) -> List[Dict]:\n",
    "    \"\"\"fetch detailed records for a batch of PMIDs.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"text\")\n",
    "        return list(Medline.parse(handle))\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "# NEW: Enhanced orchestration functions with date partitioning\n",
    "def collect_pmids_for_date_range(start_date: str, end_date: str, config: ETLConfig, remaining_target: int = None) -> List[str]:\n",
    "    \"\"\"collect PMIDs for a specific date range, handling the 9999 limit.\"\"\"\n",
    "    search_term = build_search_term(start_date, end_date)\n",
    "    total_count = get_total_count(search_term)\n",
    "    \n",
    "    print(f\"Date range {start_date} to {end_date}: {total_count} records\")\n",
    "    \n",
    "    # Use remaining target if provided, otherwise use config target\n",
    "    effective_target = remaining_target if remaining_target is not None else config.target_count\n",
    "    \n",
    "    # If we're over the limit, partition the date range\n",
    "    if total_count > config.max_records_per_query:\n",
    "        print(f\"Records ({total_count}) exceed limit ({config.max_records_per_query}). Partitioning date range...\")\n",
    "        \n",
    "        # Calculate number of partitions needed\n",
    "        num_partitions = (total_count // config.max_records_per_query) + 1\n",
    "        partitions = partition_date_range(start_date, end_date, num_partitions)\n",
    "        \n",
    "        all_pmids = []\n",
    "        for partition_start, partition_end in partitions:\n",
    "            remaining_needed = effective_target - len(all_pmids)\n",
    "            if remaining_needed <= 0:\n",
    "                break\n",
    "                \n",
    "            partition_pmids = collect_pmids_for_date_range(partition_start, partition_end, config, remaining_needed)\n",
    "            all_pmids.extend(partition_pmids)\n",
    "        \n",
    "        return all_pmids\n",
    "    \n",
    "    # If under the limit, proceed with normal collection\n",
    "    # Don't exceed what's actually available or what we need\n",
    "    target_count = min(effective_target, total_count)\n",
    "    pmid_list = []\n",
    "    \n",
    "    for start in range(0, target_count, config.batch_size):\n",
    "        remaining_in_batch = min(config.batch_size, target_count - len(pmid_list))\n",
    "        batch_pmids = fetch_pmid_batch(search_term, start, remaining_in_batch)\n",
    "        pmid_list.extend(batch_pmids)\n",
    "        print(f\"Fetched {len(pmid_list)} PMIDs so far for range {start_date} to {end_date}...\")\n",
    "        \n",
    "        # Stop if we've collected what we need\n",
    "        if len(pmid_list) >= target_count:\n",
    "            break\n",
    "    \n",
    "    return pmid_list\n",
    "\n",
    "def collect_all_pmids(search_term: str, config: ETLConfig, start_date: str, end_date: str) -> List[str]:\n",
    "    \"\"\"collect all PMIDs matching the search criteria with automatic date partitioning.\"\"\"\n",
    "    # First, get the total count to make informed decisions\n",
    "    total_available = get_total_count(search_term)\n",
    "    print(f\"Total records available: {total_available}\")\n",
    "    print(f\"Target count configured: {config.target_count}\")\n",
    "    \n",
    "    # Determine how many we should actually collect\n",
    "    effective_target = min(config.target_count, total_available)\n",
    "    print(f\"Will collect: {effective_target} records\")\n",
    "    \n",
    "    return collect_pmids_for_date_range(start_date, end_date, config, effective_target)\n",
    "\n",
    "def extract_all_records(pmid_list: List[str], config: ETLConfig) -> List[Dict]:\n",
    "    \"\"\"extract detailed records for all PMIDs.\"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    for i in range(0, len(pmid_list), config.batch_size):\n",
    "        batch_pmids = pmid_list[i:i + config.batch_size]\n",
    "        try:\n",
    "            records = fetch_records_batch(batch_pmids)\n",
    "            transformed_records = [transform_record(rec) for rec in records]\n",
    "            all_records.extend(transformed_records)\n",
    "            print(f\"Processed {len(all_records)} records so far...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_records\n",
    "\n",
    "def create_spark_dataframe(records: List[Dict], spark) -> 'DataFrame':\n",
    "    \"\"\"Convert records list to PySpark DataFrame with enriched schema.\"\"\"\n",
    "    if not records:\n",
    "        schema = StructType([\n",
    "            StructField(\"pmid\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"abstract\", StringType(), True),\n",
    "            StructField(\"journal\", StringType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"doi\", StringType(), True),\n",
    "            StructField(\"mesh_terms\", ArrayType(StringType()), True),\n",
    "            StructField(\"publication_types\", ArrayType(StringType()), True),\n",
    "            StructField(\"keywords\", ArrayType(StringType()), True),\n",
    "            StructField(\"n_size\", IntegerType(), True),\n",
    "            StructField(\"outcomes\", ArrayType(StringType()), True)\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema)\n",
    "    \n",
    "    rows = [Row(**record) for record in records]\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def save_run_metadata(metadata: RunMetadata, spark):\n",
    "    \"\"\"store run metadata to tracking table.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"script_name\", StringType(), False),\n",
    "        StructField(\"last_run_timestamp\", TimestampType(), False),\n",
    "        StructField(\"incremental_run\", StringType(), False),\n",
    "        StructField(\"total_pubs\", IntegerType(), True),\n",
    "        StructField(\"new_records\", IntegerType(), True),\n",
    "        StructField(\"run_status\", StringType(), True),\n",
    "        StructField(\"run_start\", TimestampType(), True),\n",
    "        StructField(\"run_end\", TimestampType(), True),\n",
    "        StructField(\"duration_seconds\", FloatType(), True)])\n",
    "    \n",
    "    row_data = [(\n",
    "        metadata.script_name,\n",
    "        metadata.end_time,\n",
    "        str(metadata.incremental_run),\n",
    "        metadata.total_pubs,\n",
    "        metadata.new_records,\n",
    "        metadata.status,\n",
    "        metadata.start_time,\n",
    "        metadata.end_time,\n",
    "        metadata.duration_seconds)]\n",
    "    \n",
    "    metadata_df = spark.createDataFrame(row_data, schema=schema)\n",
    "    metadata_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"script_run_metadata\")\n",
    "\n",
    "# UPDATED: main pipeline/ETL function\n",
    "def run_pubmed_etl(config: ETLConfig, spark) -> RunMetadata:\n",
    "    \"\"\"main ETL pipeline function with date partitioning support.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Entrez configuration\n",
    "        Entrez.email = config.email\n",
    "        Entrez.max_tries = config.max_tries\n",
    "        Entrez.sleep_between_tries = config.sleep_between_tries\n",
    "        \n",
    "        # date range generation\n",
    "        if config.incremental:\n",
    "            # get last run date from metadata table...\n",
    "            start_date, end_date = get_incremental_date_range(spark)\n",
    "        else:\n",
    "            start_date, end_date = get_date_range_past_year()\n",
    "        print(f\"Date range: {start_date} to {end_date}\")\n",
    "        \n",
    "        # generate search term string for Entrez API query\n",
    "        search_term = build_search_term(start_date, end_date)\n",
    "        \n",
    "        # UPDATED: data extraction from API with date partitioning\n",
    "        pmid_list = collect_all_pmids(search_term, config, start_date, end_date)\n",
    "        \n",
    "        # if no new pmids, end the main function and log the run metadata\n",
    "        if not pmid_list:\n",
    "            return create_run_metadata(\"SUCCESS\", start_time, 0, 0)\n",
    "        \n",
    "        print(f\"Total PMIDs collected: {len(pmid_list)}\")\n",
    "        records = extract_all_records(pmid_list, config)\n",
    "        \n",
    "        # transformations and normalization\n",
    "        df = create_spark_dataframe(records, spark)\n",
    "        normalized_df = normalize_dataframe(df)\n",
    "        \n",
    "        # depending on whether incremental or bulk, save data to table \n",
    "        # but only save records not already existing in target table (ensure idempotency)\n",
    "        if config.incremental:\n",
    "            final_df = filter_new_records(normalized_df, \"firedb_pubmed\", spark)\n",
    "            new_records = final_df.count()\n",
    "            if new_records > 0:\n",
    "                save_data(final_df, \"firedb_pubmed\", \"append\")\n",
    "        else:\n",
    "            new_records = normalized_df.count()\n",
    "            save_data(normalized_df, \"firedb_pubmed\", \"overwrite\")\n",
    "        # get new record counts for metadata logging\n",
    "        total_records = normalized_df.count()\n",
    "        \n",
    "        return create_run_metadata(\"SUCCESS\", start_time, total_records, new_records)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ETL failed: {e}\")\n",
    "        # log failure if issue occurs\n",
    "        return create_run_metadata(\"FAILED\", start_time, 0, 0)\n",
    "\n",
    "def create_run_metadata(status: str, start_time: datetime, total: int, new: int) -> RunMetadata:\n",
    "    \"\"\"helper function to create RunMetadata object for logging table.\"\"\"\n",
    "    end_time = datetime.now()\n",
    "    return RunMetadata(\n",
    "        script_name=\"pubmed_ingestion\",\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        incremental_run=False,\n",
    "        total_pubs=total,\n",
    "        new_records=new,\n",
    "        status=status,\n",
    "        duration_seconds=(end_time - start_time).total_seconds())\n",
    "\n",
    "def get_incremental_date_range(spark) -> Tuple[str, str]:\n",
    "    \"\"\"Get date range for incremental run based on last execution.\"\"\"\n",
    "    try:\n",
    "        last_run_df = spark.sql(\"\"\"\n",
    "            SELECT last_run_timestamp\n",
    "            FROM script_run_metadata\n",
    "            WHERE script_name = 'pubmed_ingestion'\n",
    "            ORDER BY last_run_timestamp DESC\n",
    "            LIMIT 1\"\"\")\n",
    "        \n",
    "        last_run_row = last_run_df.collect()\n",
    "        if last_run_row:\n",
    "            start_date = last_run_row[0]['last_run_timestamp'].strftime('%Y/%m/%d')\n",
    "            end_date = datetime.now().strftime('%Y/%m/%d')\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            return get_date_range_past_year()\n",
    "    except Exception:\n",
    "        return get_date_range_past_year()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # spark initialization\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    config = ETLConfig(\n",
    "        email=PubmedConfig.EMAIL,\n",
    "        incremental=PubmedConfig.INCREMENTAL,\n",
    "        batch_size=PubmedConfig.BATCH_SIZE,\n",
    "        target_count=PubmedConfig.TARGET_COUNT)\n",
    "    print(f\"config:\")\n",
    "    print(PubmedConfig.EMAIL)\n",
    "    print(PubmedConfig.INCREMENTAL)\n",
    "    print(PubmedConfig.BATCH_SIZE)\n",
    "    print(PubmedConfig.TARGET_COUNT)\n",
    "    \n",
    "    # main ETL\n",
    "    metadata = run_pubmed_etl(config, spark)\n",
    "    # run metadata\n",
    "    save_run_metadata(metadata, spark)\n",
    "    \n",
    "    # logs...\n",
    "    print(f\"ETL completed with status: {metadata.status}\")\n",
    "    print(f\"Total records: {metadata.total_pubs}, New records: {metadata.new_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3851c1b7-a17f-4fdf-9b32-9fb53bd2c4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pubmed_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
