{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64151c2-66a6-4f4f-9174-3d228cec2530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import logging\n",
    "from enum import Enum\n",
    "import time\n",
    "import configparser\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "from Bio import Entrez, Medline\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (\n",
    "    col, concat_ws, regexp_replace, to_date, trim, lit, when, length, udf)\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType, \n",
    "    IntegerType, FloatType, ArrayType)\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "sys.path.append(\"/Workspace/Repos/firedb/fire-db/config/\")\n",
    "from pubmed_config import PubmedConfig\n",
    "\n",
    "'''\n",
    "Note that this ETL job is designed to run on a single-node cluster with the following specifications,\n",
    "    per Databricks Free Edition's limitations:\n",
    "        - Limited to serverless compute only (no custom Spark configurations)\n",
    "        - Small cluster size\n",
    "        - Max of 5 concurrent tasks\n",
    "'''\n",
    "\n",
    "# read config info for run to import config module\n",
    "config_parser = configparser.ConfigParser()\n",
    "sys.path.append(\"/Workspace/Repos/firedb/fire-db/config/\")\n",
    "from pubmed_config import PubmedConfig\n",
    "\n",
    "# global variables\n",
    "OUTCOME_KEYWORDS = [\n",
    "    \"blood pressure\", \"LDL\", \"HDL\", \"total cholesterol\", \"triglycerides\",\n",
    "    \"insulin sensitivity\", \"glucose\", \"HbA1c\", \"fasting glucose\",\n",
    "    \"BMI\", \"waist circumference\", \"body fat percentage\", \"bone density\",\n",
    "    \"resting heart rate\", \"heart rate variability\",\n",
    "    \"muscle strength\", \"maximal strength\", \"grip strength\",\n",
    "    \"1RM\", \"2RM\", \"3RM\", \"4RM\", \"5RM\", \"6RM\", \"7RM\", \"8RM\", \"9RM\", \"10RM\", \"11RM\", \"12RM\",\n",
    "    \"isometric strength\", \"explosive strength\", \"power output\", \"vertical jump\",\n",
    "    \"VO2 max\", \"aerobic capacity\", \"oxygen consumption\", \"endurance performance\",\n",
    "    \"time to exhaustion\", \"lactate threshold\", \"anaerobic threshold\",\n",
    "    \"cycling time trial\", \"treadmill test\", \"distance run\", \"step test\",\n",
    "    \"speed\", \"reaction time\", \"agility\", \"change of direction\", \"balance\",\n",
    "    \"postural stability\", \"range of motion\", \"flexibility\", \"mobility\",\n",
    "    \"lean mass\", \"fat-free mass\", \"skeletal muscle mass\", \"fat mass\", \"adiposity\",\n",
    "    \"fatigue\", \"mood\", \"wellbeing\", \"mental health\", \"quality of life\",\n",
    "    \"stress\", \"cognitive function\", \"depression\", \"anxiety\", \"sleep quality\",\n",
    "    \"self-efficacy\", \"motivation\", \"perceived exertion\", \"body image\",\n",
    "    \"appetite\", \"satiety\", \"ghrelin\", \"leptin\", \"cortisol\",\n",
    "    \"testosterone\", \"growth hormone\", \"inflammatory markers\", \"C-reactive protein\",\n",
    "    \"functional capacity\", \"walking speed\", \"gait speed\", \"fall risk\",\n",
    "    \"pain\", \"DOMS\", \"recovery\", \"soreness\", \"inflammation\", \"muscle damage\"\n",
    "    \"cognitive score\", \"cognition\", \"joint density\", \"motor impairment\", \"gait\",\n",
    "    \"body mass index\", \"stroke\", \"artery disease\", \"diabetes\", \"cancer\", \n",
    "    \"behavior rating inventory of executive function\", \"international physical activity questionnaire\",\n",
    "    \"relative exercise dose intensity\", \"functional ambulation category\",\n",
    "    \"fugl-meyer upper extremity\", \"moderate-to-vigorous physical activity\", \"motor coordination\",\n",
    "    \"cardiorespiratory fitness\", \"hypertrophy\", \"atrogene expression\", \"peak aerobic power\",\n",
    "    \"fat oxidation rate\", \"6 minute walk test\", \"6-minute walk distance\",\n",
    "    \"cardiac function\", \"executive function\", \"peak torque\", \"muscle csa\",\n",
    "    \"resting metabolic rate\", \"basal metabolic rate\", \"symptom-limited maximal cycle exercise test\",\n",
    "    \"incremental shuttle walk test\", \"exercise capacity\", \"muscle strength\", \"muscle power\",\n",
    "    \"muscle endurance\", \"muscle hypertrophy\", \"muscle atrophy\", \"joint density\"]\n",
    "\n",
    "MODALITY_KEYWORDS = [\"endurance training\", \"aerobic exercise\", \"zone 1 cardio\", \"zone 2 cardio\",\n",
    "    \"threshold training\", \"moderate-intensity exercise\", \"moderate-intensity training\", \"moderate-intensity continuous exercise\", \"moderate-intensity continuous training\", \"moderate-to-vigorous intensity continuous training\", \"low-intensity exercise\", \"low-intensity training\",\"low-intensity continuous endurance training\", \"high-intensity interval exercise\", \"hiit\", \"high-intensity interval training\", \"high intensity exercise\"\n",
    "    \"speed endurance production\", \"running\", \"cycling\",\n",
    "    \"upright cycling\", \"treadmill\", \"rowing\", \"swimming\", \"ergometer\",\n",
    "    \"resistance exercise\", \"acute resistance exercise\", \"conventional resistance training\",\n",
    "    \"combined exercise training\", \"eccentric resistance training\", \"eccentric exercise\",\n",
    "    \"strength training\", \"bodyweight exercise\", \"bodyweight training\",\n",
    "    \"elastic resistance training\", \"freeweight training\", \"freeweight exercise\"\n",
    "    \"olympic training\", \"olympic weightlifting\", \"olympic lifting\", \"olympic weight training\",\n",
    "    \"weightlifting\", \"flexibility training\", \"flexibility exercise\", \"stability training\", \"stability exercise\",\n",
    "    \"balance training\", \"balance exercise\", \"yoga\", \"pilates\", \"mind-body exercise\", \"circuit training\",\n",
    "    \"circuit-based exercise\", \"plyometric exercise\", \"plyometric training\", \"isometric training\", \n",
    "    \"isometric exercise\"]\n",
    "\n",
    "\n",
    "# configuration & data class definitions\n",
    "@dataclass\n",
    "class ETLConfig:\n",
    "    email: str\n",
    "    max_tries: int = 10\n",
    "    sleep_between_tries: int = 20\n",
    "    batch_size: int = 1000\n",
    "    target_count: int = 20000\n",
    "    incremental: bool = False\n",
    "    max_records_per_query: int = 9999  # PubMed's limit\n",
    "\n",
    "@dataclass\n",
    "class RunMetadata:\n",
    "    script_name: str\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    incremental_run: bool\n",
    "    total_pubs: int\n",
    "    new_records: int\n",
    "    status: str\n",
    "    duration_seconds: float\n",
    "\n",
    "class ETLStatus(Enum):\n",
    "    SUCCESS = \"SUCCESS\"\n",
    "    FAILED = \"FAILED\"\n",
    "    PARTIAL = \"PARTIAL\"\n",
    "    NO_DATA = \"NO_DATA\"\n",
    "\n",
    "# business logic and function definitions\n",
    "def setup_logging():\n",
    "    \"\"\"Configure logging for ETL process\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler('pubmed_etl.log')\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def get_date_range_past_year() -> Tuple[str, str]:\n",
    "    \"\"\"calculate date range for the past year in appropriate format for Entrez query.\"\"\"\n",
    "    today = datetime.today()\n",
    "    one_year_ago = today - relativedelta(months=12)\n",
    "    return (one_year_ago.strftime(\"%Y/%m/%d\"), today.strftime(\"%Y/%m/%d\"))\n",
    "\n",
    "def build_mesh_query() -> str:\n",
    "    \"\"\"build the MeSH terms query string.\"\"\"\n",
    "    mesh_terms = [\n",
    "        '\"Exercise\"[MeSH Terms]',\n",
    "        '\"Physical Conditioning, Human\"[MeSH Terms]',\n",
    "        '\"Resistance Training\"[MeSH Terms]',\n",
    "        '\"Aerobic Exercise\"[MeSH Terms]',\n",
    "        '\"Circuit-Based Exercise\"[MeSH Terms]',\n",
    "        '\"Periodization\"[MeSH Terms]',\n",
    "        '\"Weight Lifting\"[MeSH Terms]',\n",
    "        '\"High-Intensity Interval Training\"[MeSH Terms]',\n",
    "        '\"Plyometric Exercise\"[MeSH Terms]',\n",
    "        '\"Endurance Training\"[MeSH Terms]',\n",
    "        '\"Muscle Stretching Exercises\"[MeSH Terms]',\n",
    "        '\"Physical Fitness\"[MeSH Terms]',\n",
    "        '\"Cardiorespiratory Fitness\"[MeSH Terms]',\n",
    "        '\"Exercise Therapy\"[MeSH Terms]',\n",
    "        '\"Walking\"[MeSH Terms]',\n",
    "        '\"Swimming\"[MeSH Terms]',\n",
    "        '\"Gymnastics\"[MeSH Terms]',\n",
    "        '\"Bicycling\"[MeSH Terms]'\n",
    "    ]\n",
    "    return ' OR '.join(mesh_terms)\n",
    "\n",
    "def build_search_term(start_date: str, end_date: str) -> str:\n",
    "    \"\"\"build complete PubMed search term with MeSH terms and date range.\"\"\"\n",
    "    mesh_query = build_mesh_query()\n",
    "    return f'({mesh_query}) AND (\"{start_date}\"[Date - Publication] : \"{end_date}\"[Date - Publication])'\n",
    "\n",
    "def partition_date_range(start_date_str: str, end_date_str: str, num_partitions: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Partition a date range into smaller chunks.\"\"\"\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y/%m/%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y/%m/%d\")\n",
    "    \n",
    "    total_days = (end_date - start_date).days\n",
    "    days_per_partition = max(1, total_days // num_partitions)\n",
    "    \n",
    "    partitions = []\n",
    "    current_start = start_date\n",
    "    \n",
    "    for i in range(num_partitions):\n",
    "        if i == num_partitions - 1:\n",
    "            # Last partition gets any remaining days\n",
    "            current_end = end_date\n",
    "        else:\n",
    "            current_end = current_start + timedelta(days=days_per_partition)\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "        \n",
    "        partitions.append((\n",
    "            current_start.strftime(\"%Y/%m/%d\"),\n",
    "            current_end.strftime(\"%Y/%m/%d\")\n",
    "        ))\n",
    "        \n",
    "        current_start = current_end + timedelta(days=1)\n",
    "        if current_start > end_date:\n",
    "            break\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "def extract_doi(record: Dict) -> Optional[str]:\n",
    "    \"\"\"extract DOI from PubMed record - used in matching ETL\"\"\"\n",
    "    if \"AID\" not in record:\n",
    "        return None\n",
    "    \n",
    "    for aid in record[\"AID\"]:\n",
    "        if \"doi\" in aid.lower():\n",
    "            return aid.replace(\" [doi]\", \"\")\n",
    "    return None\n",
    "\n",
    "def parse_date_fallback(date_str: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"parse date string using dateparser as fallback, only if default parser unable to parse.\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    parsed = dateparser.parse(date_str)\n",
    "    return parsed.strftime(\"%Y-%m-%d\") if parsed else None\n",
    "\n",
    "def extract_sample_size(text: str) -> int:\n",
    "    \"\"\"Extract sample size from abstract using regex.\"\"\"\n",
    "    match = re.search(r\"(?:N\\s*=\\s*|sample size of\\s+|total of\\s+)(\\d{2,5})\", text or \"\", re.IGNORECASE)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_outcomes(text: str) -> List[str]:\n",
    "    \"\"\"Match known clinical outcomes.\"\"\"\n",
    "    return [term for term in OUTCOME_KEYWORDS if term.lower() in (text or \"\").lower()]\n",
    "\n",
    "def extract_modalities(text: str) -> List[str]:\n",
    "    \"\"\"Match known exercise modality keywords.\"\"\"\n",
    "    return [term for term in MODALITY_KEYWORDS if term.lower() in (text or \"\").lower()]\n",
    "\n",
    "# data processing and transformation functions\n",
    "def transform_record(record: Dict) -> Dict:\n",
    "    \"\"\"Normalize individual PubMed records into dictionary with enrichment.\"\"\"\n",
    "    abstract = record.get(\"AB\", \"\")\n",
    "    return {\n",
    "        \"pmid\": record.get(\"PMID\"),\n",
    "        \"title\": record.get(\"TI\"),\n",
    "        \"abstract\": abstract,\n",
    "        \"journal\": record.get(\"JT\"),\n",
    "        \"date\": record.get(\"DP\"),\n",
    "        \"doi\": extract_doi(record),\n",
    "        \"mesh_terms\": record.get(\"MH\", []),\n",
    "        \"publication_types\": record.get(\"PT\", []),\n",
    "        \"keywords\": record.get(\"OT\", []),\n",
    "        \"n_size\": extract_sample_size(abstract),\n",
    "        \"outcomes\": extract_outcomes(abstract),\n",
    "        \"modalities\": extract_modalities(abstract)}\n",
    "\n",
    "def normalize_dataframe(df):\n",
    "    \"\"\"apply normalization transformations to DataFrame and apply basic data quality filters\"\"\"\n",
    "    # UDF for date parsing\n",
    "    parse_date_udf = udf(parse_date_fallback, StringType())\n",
    "    \n",
    "    return (df\n",
    "            .withColumn(\"abstract\", regexp_replace(col(\"abstract\"), \"\\n\", \" \"))\n",
    "            .withColumn(\"title\", regexp_replace(trim(col(\"title\")), \"\\n\", \"\"))\n",
    "            .withColumn(\"pub_date\", parse_date_udf(col(\"date\")))\n",
    "            .drop(\"date\")\n",
    "            .filter((col(\"doi\").isNotNull()) & (trim(col(\"doi\")) != \"\") & (col(\"pub_date\").isNotNull()))\n",
    "            .dropDuplicates([\"doi\"]))\n",
    "\n",
    "def filter_new_records(normalized_df, existing_table_name: str, spark):\n",
    "    \"\"\"filter out records that already exist in the target table.\"\"\"\n",
    "    try:\n",
    "        existing_df = spark.table(existing_table_name)\n",
    "        return normalized_df.join(existing_df.select(\"doi\"), on=\"doi\", how=\"left_anti\")\n",
    "    except Exception:\n",
    "        # validate table exists...\n",
    "        return normalized_df\n",
    "\n",
    "# functions to manage interactions with Entrez API (biopython)\n",
    "def fetch_with_retry(fetch_func, max_tries: int = 5, sleep_time: int = 20):\n",
    "    \"\"\"Enhanced retry wrapper with better error reporting\"\"\"\n",
    "    last_exception = None\n",
    "    \n",
    "    for attempt in range(max_tries):\n",
    "        try:\n",
    "            return fetch_func()\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            if \"HTTP Error 429\" in error_msg and attempt < max_tries - 1:\n",
    "                wait_time = sleep_time * (2 ** attempt)  # Exponential backoff\n",
    "                logger.warning(f\"Rate limit hit. Retrying in {wait_time}s (attempt {attempt + 1}/{max_tries})\")\n",
    "                time.sleep(wait_time)\n",
    "            elif attempt < max_tries - 1:\n",
    "                logger.warning(f\"Request failed: {error_msg}. Retrying... (attempt {attempt + 1}/{max_tries})\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                logger.error(f\"All retry attempts failed. Last error: {error_msg}\")\n",
    "                raise last_exception\n",
    "    \n",
    "    raise last_exception\n",
    "\n",
    "def get_total_count(search_term: str) -> int:\n",
    "    \"\"\"get total count of records matching search term.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=0)\n",
    "        return int(Entrez.read(handle)[\"Count\"])\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_pmid_batch(search_term: str, start: int, batch_size: int) -> List[str]:\n",
    "    \"\"\"fetch a batch of PMIDs given a batch size and starting index.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=search_term,\n",
    "            retmax=batch_size,\n",
    "            retstart=start\n",
    "        )\n",
    "        return Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "def fetch_records_batch(pmids: List[str]) -> List[Dict]:\n",
    "    \"\"\"fetch detailed records for a batch of PMIDs.\"\"\"\n",
    "    def _fetch():\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"text\")\n",
    "        return list(Medline.parse(handle))\n",
    "    \n",
    "    return fetch_with_retry(_fetch)\n",
    "\n",
    "# NEW: Enhanced orchestration functions with date partitioning\n",
    "def collect_pmids_for_date_range(start_date: str, end_date: str, config: ETLConfig, remaining_target: int = None) -> List[str]:\n",
    "    \"\"\"collect PMIDs for a specific date range, handling the 9999 limit.\"\"\"\n",
    "    search_term = build_search_term(start_date, end_date)\n",
    "    total_count = get_total_count(search_term)\n",
    "    \n",
    "    print(f\"Date range {start_date} to {end_date}: {total_count} records\")\n",
    "    \n",
    "    # Use remaining target if provided, otherwise use config target\n",
    "    effective_target = remaining_target if remaining_target is not None else config.target_count\n",
    "    \n",
    "    # If we're over the limit, partition the date range\n",
    "    if total_count > config.max_records_per_query:\n",
    "        print(f\"Records ({total_count}) exceed limit ({config.max_records_per_query}). Partitioning date range...\")\n",
    "        \n",
    "        # Calculate number of partitions needed\n",
    "        num_partitions = (total_count // config.max_records_per_query) + 1\n",
    "        partitions = partition_date_range(start_date, end_date, num_partitions)\n",
    "        \n",
    "        all_pmids = []\n",
    "        for partition_start, partition_end in partitions:\n",
    "            remaining_needed = effective_target - len(all_pmids)\n",
    "            if remaining_needed <= 0:\n",
    "                break\n",
    "                \n",
    "            partition_pmids = collect_pmids_for_date_range(partition_start, partition_end, config, remaining_needed)\n",
    "            all_pmids.extend(partition_pmids)\n",
    "        \n",
    "        return all_pmids\n",
    "    \n",
    "    # If under the limit, proceed with normal collection\n",
    "    # Don't exceed what's actually available or what we need\n",
    "    target_count = min(effective_target, total_count)\n",
    "    pmid_list = []\n",
    "    \n",
    "    for start in range(0, target_count, config.batch_size):\n",
    "        remaining_in_batch = min(config.batch_size, target_count - len(pmid_list))\n",
    "        batch_pmids = fetch_pmid_batch(search_term, start, remaining_in_batch)\n",
    "        pmid_list.extend(batch_pmids)\n",
    "        print(f\"Fetched {len(pmid_list)} PMIDs so far for range {start_date} to {end_date}...\")\n",
    "        \n",
    "        # Stop if we've collected what we need\n",
    "        if len(pmid_list) >= target_count:\n",
    "            break\n",
    "    \n",
    "    return pmid_list\n",
    "\n",
    "def collect_all_pmids(search_term: str, config: ETLConfig, start_date: str, end_date: str) -> List[str]:\n",
    "    \"\"\"collect all PMIDs matching the search criteria with automatic date partitioning.\"\"\"\n",
    "    # First, get the total count to make informed decisions\n",
    "    total_available = get_total_count(search_term)\n",
    "    print(f\"Total records available: {total_available}\")\n",
    "    print(f\"Target count configured: {config.target_count}\")\n",
    "    \n",
    "    # Determine how many we should actually collect\n",
    "    effective_target = min(config.target_count, total_available)\n",
    "    print(f\"Will collect: {effective_target} records\")\n",
    "    \n",
    "    return collect_pmids_for_date_range(start_date, end_date, config, effective_target)\n",
    "\n",
    "def extract_all_records(pmid_list: List[str], config: ETLConfig) -> List[Dict]:\n",
    "    \"\"\"extract detailed records for all PMIDs.\"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    for i in range(0, len(pmid_list), config.batch_size):\n",
    "        batch_pmids = pmid_list[i:i + config.batch_size]\n",
    "        try:\n",
    "            records = fetch_records_batch(batch_pmids)\n",
    "            transformed_records = [transform_record(rec) for rec in records]\n",
    "            all_records.extend(transformed_records)\n",
    "            print(f\"Processed {len(all_records)} records so far...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_records\n",
    "\n",
    "def create_spark_dataframe(records: List[Dict], spark) -> 'DataFrame':\n",
    "    \"\"\"Convert records list to PySpark DataFrame with enriched schema.\"\"\"\n",
    "    if not records:\n",
    "        schema = StructType([\n",
    "            StructField(\"pmid\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"abstract\", StringType(), True),\n",
    "            StructField(\"journal\", StringType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"doi\", StringType(), True),\n",
    "            StructField(\"mesh_terms\", ArrayType(StringType()), True),\n",
    "            StructField(\"publication_types\", ArrayType(StringType()), True),\n",
    "            StructField(\"keywords\", ArrayType(StringType()), True),\n",
    "            StructField(\"n_size\", IntegerType(), True),\n",
    "            StructField(\"outcomes\", ArrayType(StringType()), True),\n",
    "            StructField(\"modalities\", ArrayType(StringType()), True)\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema)\n",
    "    \n",
    "    rows = [Row(**record) for record in records]\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "def save_data(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"save pyspark df to delta table.\"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    if mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def save_run_metadata(metadata: RunMetadata, spark):\n",
    "    \"\"\"store run metadata to tracking table.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"script_name\", StringType(), False),\n",
    "        StructField(\"last_run_timestamp\", TimestampType(), False),\n",
    "        StructField(\"incremental_run\", StringType(), False),\n",
    "        StructField(\"total_pubs\", IntegerType(), True),\n",
    "        StructField(\"new_records\", IntegerType(), True),\n",
    "        StructField(\"run_status\", StringType(), True),\n",
    "        StructField(\"run_start\", TimestampType(), True),\n",
    "        StructField(\"run_end\", TimestampType(), True),\n",
    "        StructField(\"duration_seconds\", FloatType(), True)])\n",
    "    \n",
    "    row_data = [(\n",
    "        metadata.script_name,\n",
    "        metadata.end_time,\n",
    "        str(metadata.incremental_run),\n",
    "        metadata.total_pubs,\n",
    "        metadata.new_records,\n",
    "        metadata.status,\n",
    "        metadata.start_time,\n",
    "        metadata.end_time,\n",
    "        metadata.duration_seconds)]\n",
    "    \n",
    "    metadata_df = spark.createDataFrame(row_data, schema=schema)\n",
    "    metadata_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"script_run_metadata\")\n",
    "\n",
    "# UPDATED: main pipeline/ETL function\n",
    "def run_pubmed_etl(config: ETLConfig, spark) -> RunMetadata:\n",
    "    \"\"\"Main ETL pipeline with enhanced monitoring and error handling\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(f\"Starting PubMed ETL with config: {config}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Entrez\n",
    "        Entrez.email = config.email\n",
    "        Entrez.max_tries = config.max_tries\n",
    "        Entrez.sleep_between_tries = config.sleep_between_tries\n",
    "        \n",
    "        # Determine date range\n",
    "        if config.incremental:\n",
    "            start_date, end_date = get_incremental_date_range(spark)\n",
    "            logger.info(f\"Incremental run: {start_date} to {end_date}\")\n",
    "        else:\n",
    "            start_date, end_date = get_date_range_past_year()\n",
    "            logger.info(f\"Full run: {start_date} to {end_date}\")\n",
    "        \n",
    "        # Build search and collect PMIDs\n",
    "        search_term = build_search_term(start_date, end_date)\n",
    "        logger.info(f\"Search term: {search_term}\")\n",
    "        \n",
    "        pmid_list = collect_all_pmids(search_term, config, start_date, end_date)\n",
    "        \n",
    "        if not pmid_list:\n",
    "            logger.info(\"No new PMIDs found\")\n",
    "            return create_run_metadata(ETLStatus.NO_DATA, start_time, 0, 0)\n",
    "        \n",
    "        logger.info(f\"Collected {len(pmid_list)} PMIDs\")\n",
    "        \n",
    "        # Extract and transform records\n",
    "        records = extract_all_records(pmid_list, config)\n",
    "        logger.info(f\"Extracted {len(records)} records\")\n",
    "        \n",
    "        # Process with Spark\n",
    "        df = create_spark_dataframe(records, spark)\n",
    "        normalized_df = normalize_dataframe(df)\n",
    "        \n",
    "        # Handle incremental vs full load\n",
    "        if config.incremental:\n",
    "            final_df = filter_new_records(normalized_df, \"firedb_pubmed\", spark)\n",
    "            new_records = final_df.count()\n",
    "            if new_records > 0:\n",
    "                save_data(final_df, \"firedb_pubmed\", \"append\")\n",
    "                logger.info(f\"Appended {new_records} new records\")\n",
    "        else:\n",
    "            new_records = normalized_df.count()\n",
    "            save_data(normalized_df, \"firedb_pubmed\", \"overwrite\")\n",
    "            logger.info(f\"Saved {new_records} records (full refresh)\")\n",
    "        \n",
    "        total_records = normalized_df.count()\n",
    "        \n",
    "        metadata = create_run_metadata(ETLStatus.SUCCESS, start_time, total_records, new_records)\n",
    "        logger.info(f\"ETL completed successfully: {metadata}\")\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ETL failed with error: {str(e)}\", exc_info=True)\n",
    "        return create_run_metadata(ETLStatus.FAILED, start_time, 0, 0)\n",
    "\n",
    "def create_run_metadata(status: str, start_time: datetime, total: int, new: int) -> RunMetadata:\n",
    "    \"\"\"helper function to create RunMetadata object for logging table.\"\"\"\n",
    "    end_time = datetime.now()\n",
    "    return RunMetadata(\n",
    "        script_name=\"pubmed_ingestion\",\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        incremental_run=False,\n",
    "        total_pubs=total,\n",
    "        new_records=new,\n",
    "        status=status,\n",
    "        duration_seconds=(end_time - start_time).total_seconds())\n",
    "\n",
    "def get_incremental_date_range(spark) -> Tuple[str, str]:\n",
    "    \"\"\"Get date range for incremental run based on last execution.\"\"\"\n",
    "    try:\n",
    "        last_run_df = spark.sql(\"\"\"\n",
    "            SELECT last_run_timestamp\n",
    "            FROM script_run_metadata\n",
    "            WHERE script_name = 'pubmed_ingestion'\n",
    "            ORDER BY last_run_timestamp DESC\n",
    "            LIMIT 1\"\"\")\n",
    "        \n",
    "        last_run_row = last_run_df.collect()\n",
    "        if last_run_row:\n",
    "            start_date = last_run_row[0]['last_run_timestamp'].strftime('%Y/%m/%d')\n",
    "            end_date = datetime.now().strftime('%Y/%m/%d')\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            return get_date_range_past_year()\n",
    "    except Exception:\n",
    "        return get_date_range_past_year()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # spark initialization\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    logger = setup_logging()\n",
    "\n",
    "    config = ETLConfig(\n",
    "        email=PubmedConfig.EMAIL,\n",
    "        incremental=PubmedConfig.INCREMENTAL,\n",
    "        batch_size=PubmedConfig.BATCH_SIZE,\n",
    "        target_count=PubmedConfig.TARGET_COUNT)\n",
    "    print(f\"config:\")\n",
    "    print(PubmedConfig.EMAIL)\n",
    "    print(PubmedConfig.INCREMENTAL)\n",
    "    print(PubmedConfig.BATCH_SIZE)\n",
    "    print(PubmedConfig.TARGET_COUNT)\n",
    "    \n",
    "    # main ETL\n",
    "    metadata = run_pubmed_etl(config, spark)\n",
    "    # run metadata\n",
    "    save_run_metadata(metadata, spark)\n",
    "    \n",
    "    # logs...\n",
    "    print(f\"ETL completed with status: {metadata.status}\")\n",
    "    print(f\"Total records: {metadata.total_pubs}, New records: {metadata.new_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3851c1b7-a17f-4fdf-9b32-9fb53bd2c4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pubmed_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
